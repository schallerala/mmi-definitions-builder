\input{../preamble.tex}
\begin{document}

%%%%%%%%%% output/1--1-0-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{0}
\setcounter{dfn}{0}
\label{portion:1}

\chapter{Basic combinatorics}


\end{page}

%%%%%%%%%% output/2--1-1-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:2}

\section{How to count}
All sets in this chapter are finite.

For a finite set $X$, by $|X|$ we denote the number of elements in $X$, also called the \emph{cardinality} of $X$.


\end{page}

%%%%%%%%%% output/3--1-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{0}
\label{portion:3}

\subsection{Sum rule}
\begin{center}
\emph{If $X \cap Y = \emptyset$, then $|X \cup Y| = |X| + |Y|$.}
\end{center}

More generally,
\begin{center}
\parbox{.9\textwidth}{\emph{If the sets $X_1, \ldots, X_n$ are pairwise disjoint
(that is $X_i \cap X_j = \emptyset$ for all $i \ne j$),
then $|X_1 \cup X_2 \cup \cdots \cup X_n| = |X_1| + |X_2| + \cdots + |X_n|$.}}
\end{center}
Formally, this follows by induction on $n$ from the sum rule for two sets.

Later we will learn how to proceed if the sets are not disjoint.


\end{page}

%%%%%%%%%% output/4--1-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{0}
\label{portion:4}

\subsection{Product rule}
First, let us state a special product rule:
\[
|X \times Y| = |X| \cdot |Y|.
\]
Here $X \times Y$, the \emph{Cartesian product} of $X$ and $Y$, denotes the set of ordered pairs $(x,y)$ with $x \in X$, $y \in Y$.

The elements of $X \times Y$ can be written in a table whose rows correspond to the elements of $X$,
and the columns correspond to the elements of $Y$.
This justifies the product rule.

Again, there is an extension to several sets:
\[
|X_1 \times \cdots X_n| = |X_1| \cdots |X_n|.
\]


\end{page}

%%%%%%%%%% output/6--1-1-2-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:6}

\begin{exl}
A restaurant offers a choice of $3$ first courses, $4$ main courses, and $5$ desserts.
How many different full course dinners are there?

A full course dinner is an element of the Cartesian product
\[
\{\text{first courses}\} \times \{\text{main courses}\} \times \{\text{desserts}\}.
\]
Multiplying the cardinalities of these sets we obtain the answer: $3 \cdot 4 \cdot 5 = 60$ different full course dinners are possible.
\end{exl}

\end{page}

%%%%%%%%%% output/7--1-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:7}


Up to now we have considered independent choices,
when the result of one choice does not influence the sets of subsequent choices.
However, it is not the \textbf{set} of choices what matters, but rather the \textbf{number} of choices.
This leads us to the general product rule:
\begin{center}
\parbox{.9\textwidth}{\emph{If two consecutive choices are made, with $m$ possibilities for the first choice and $n$ possibilities for the second choice,
then the number of all possible outcomes is equal to $mn$.}}
\end{center}
Of course, this can be generalized to several consecutive choices
if the number of possibilities for each choice is independent of the results of all previous choices.


\end{page}

%%%%%%%%%% output/9--1-1-2-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:9}

\begin{exl}
A person wants to go to a swimming pool once a week, and play tennis once a week, but not both on the same day.
How many different schedules are there?

Choose a swimming day. There are $7$ possibilities for this.
When the choice is made, there remain $6$ possible tennis days.
So, there are $7 \cdot 6 = 42$ different schedules.
\end{exl}

\end{page}

%%%%%%%%%% output/11--1-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{2}
\label{portion:11}

\subsection{Difference rule}
The set difference: $X \setminus Y = \{x \in X \mid x \notin Y\}$.
\[
\text{If }X \supset Y, \text{ then } |X \setminus Y| = |X| - |Y|.
\]
Instead of counting the number of ``good'' outcomes,
one can count the number of ``bad'' ones and subtract it from the total number of outcomes.


\end{page}

%%%%%%%%%% output/13--1-1-3-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{3}
\label{portion:13}

\begin{exl}
Two dice are thrown. What is the probability that at least one of them shows six?

We give two solutions.
The first one uses the sum and the product rules.
Consider three cases:
\begin{itemize}
\item
Both dice show six. This is one outcome.
\item
The first dice shows six, the second does not. Five outcomes.
\item
The first dice does not show six, the second does. Five outcomes as well.
\end{itemize}
Thus in total we have $1+5+5=11$ possibilities.
To compute the probability, we have to divide by the total number of possibilities, which is $6 \cdot 6 = 36$.

The second solution uses the difference rule.
A ``bad'' outcome is one where neither of the dice shows six.
For each of the dice there are $5$ possibilities, so that this number is $5 \cdot 5 = 25$.
To count the ``good'' outcomes, we subtract $25$ from the number of all possible outcomes:
$36-25=11$.
\end{exl}

\end{page}

%%%%%%%%%% output/15--1-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{3}
\label{portion:15}

\subsection{Quotient rule}
\begin{center}
\parbox{.9\textwidth}{\emph{The number of sheep in a herd is equal to the number of legs divided by four.}}
\end{center}
This is a highly inefficient way of counting sheep.
But if we see only the legs and cannot see the heads, then this is the only available way.


\end{page}

%%%%%%%%%% output/17--1-1-4-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{4}
\label{portion:17}

\begin{exl}
Draw a convex $n$-gon and all of its diagonals.
How many segments (sides and diagonals) do we get?

Every point belongs to $n-1$ segments.
If we multiply this by the number of points, we get $n(n-1)$.
But every segment was counted twice, because it has two endpoints (two ``legs'').
Thus the total number of segments is $\frac{n(n-1)}2$.
\end{exl}

\end{page}

%%%%%%%%%% output/19--1-2-0-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:19}

\section{Counting maps and subsets}

\end{page}

%%%%%%%%%% output/20--1-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:20}

\subsection{Maps}
A \emph{map} $f \colon X \to Y$ is a rule that associates to every element $x \in X$ a unique element of $Y$.
The element associated to $x$ is denoted by $f(x)$.

A map can be pictured as a collection of arrows going from elements of $X$ to elements of $Y$.
At every element of $X$ one and only one arrow must start.
By contrast, at an element of $Y$ several arrows or none at all may end.

\begin{figure}[ht]
\begin{center}
\input{Fig/Map.pdf_t}
\end{center}
\caption{A map $f \colon X \to Y$.}
\label{fig:Map}
\end{figure}

A map $f \colon X \to Y$ is called
\begin{itemize}
\item
\emph{injective}, if no two different elements of $X$ are sent to the same element of $Y$: for every $x_1 \ne x_2$ we have $f(x_1) \ne f(x_2)$;
\item
\emph{surjective}, if to every element of $Y$ some element of $X$ is sent: for every $y \in Y$ there is $x \in X$ such that $f(x) = y$;
\item
\emph{bijective}, if it is injective and surjective.
\end{itemize}


\end{page}

%%%%%%%%%% output/22--1-2-1-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:22}

\begin{exl}
Let $X$ be the set of all countries in the world, and $Y$ be the set of all cities in the world.
Define a map $f \colon X \to Y$ by letting $f(x)$ be the capital of the country $x$.
Then $f$ is injective (no city can be the capital of two countries), but not surjective (some cities are not capitals).
Define a map $g \colon Y \to X$ by letting $g(y)$ be the country in which the city $y$ lies.
Then $g$ is surjective (in every country there is at least one city), but not injective (some countries have more than one city).

It is also interesting to look at the compositions $f \circ g$ and $g \circ f$...
\end{exl}

\end{page}

%%%%%%%%%% output/23--1-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:23}


A map is bijective iff at every element of $Y$ ends exactly one arrow.
By inverting the arrows we obtain the \emph{inverse map} $f^{-1} \colon Y \to X$,
which has the properties $f^{-1}(f(x)) = x$ for all $x \in X$ and $f(f^{-1}(y)) = y$ for all $y \in Y$.

If $f$ is not bijective, then there is no inverse map $f^{-1}$.
However, by abuse of notation one uses $f^{-1}(y)$ to denote the \emph{preimage} of $y$:
\[
f^{-1}(y) = \{x \in X \mid f(x) = y\}.
\]
Similarly one can define the preimage $f^{-1}(B)$ of any subset $B \subset Y$.

Observe that
\begin{itemize}
\item
$f$ injective $\Leftrightarrow$ $|f^{-1}(y)| \le 1$ for all $y$;
\item
$f$ surjective $\Leftrightarrow$ $f^{-1}(y) \ne \emptyset$ for all $y$.
\end{itemize}

We can now formulate the quotient rule in the mathematical language.

\begin{center}
\parbox{.75\textwidth}{\emph{If a map $f \colon X \to Y$ satisfies $|f^{-1}(y)| = k$ for all $y \in Y$, then $|Y| = \frac{|X|}{k}$.}}
\end{center}

A special case of this is the bijection principle:
\begin{center}
\parbox{.75\textwidth}{\emph{If a map $f \colon X \to Y$ is a bijection, then $|X| = |Y|$.}}
\end{center}



\end{page}

%%%%%%%%%% output/24--1-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:24}

\subsection{Counting all maps}

\end{page}

%%%%%%%%%% output/26--1-2-2-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:26}

\begin{thm}
\label{thm:AllMaps}
If $|X| = m$ and $|Y| = n$, then there are $n^m$ different maps $X \to Y$.
\end{thm}

\end{page}

%%%%%%%%%% output/27--1-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:27}

\begin{proof}
Without loss of generality, let $X = \{1, 2, \ldots, m\}$.
In order to define a map $f \colon X \to Y$ we need to make $m$ choices, each time from $n$ possibilities:
$f(1)$ can take $n$ different values, so can $f(2)$, and so on up to $f(m)$.
Thus there are
\[
\underbrace{n \cdots n}_{m} = n^m
\]
different maps $X \to Y$.
\end{proof}


\end{page}

%%%%%%%%%% output/29--1-2-2-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:29}

\begin{thm}
\label{thm:AllSubsets}
The number of different subsets of an $n$-element set is $2^n$.
\end{thm}

\end{page}

%%%%%%%%%% output/30--1-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:30}

\begin{proof}
Let $|X| = n$. With every subset $A \subset X$ we associate a map $\mathbf{1}_A \colon X \to \{0,1\}$ (the \emph{indicator function} of $A$) defined as
\[
\mathbf{1}_A(x) =
\begin{cases}
1, &\text{if } x \in A,\\
0, &\text{if } x \notin A.
\end{cases}
\]
One can show that $A \mapsto \mathbf{1}_A$ is a bijection between the set of all subsets and the set of all maps $X \to \{0,1\}$:
different subsets define different maps and every map $f$ is the indicator function of the subset $f^{-1}(1) \subset X$.

The number of all maps $X \to \{0,1\}$ is $2^{|X|}$ by Theorem \ref{thm:AllMaps}.
By the bijection principle, the number of all subsets of $X$ is the same.
\end{proof}


\end{page}

%%%%%%%%%% output/32--1-2-2-remark.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:32}

\begin{rem}
The set of all maps $X \to Y$ is sometimes denoted by $Y^X$, so that Theorem \ref{thm:AllMaps} can be formulated as $\left|Y^X\right| = |Y|^{|X|}$.
This is not the only reason for the notation $Y^X$.
One can show that $Z^{X \cup Y} = Z^X \times Z^Y$ for disjoint $X$ and $Y$, and $Z^{X \times Y} = (Z^X)^Y$.

Also, the Cartesian power
\[
X^n = \underbrace{X \times \cdots \times X}_{n} = \{(x_1, \ldots, x_n) \mid x_i \in X \text{ for all }i\}
\]
can be viewed as the set $X^{\{1, \ldots, n\}}$:
a sequence $(x_1, \ldots, x_n)$ corresponds to a map $f \colon \{1, \ldots, n\} \to X$, $f(i) = x_i$.
\end{rem}

\end{page}

%%%%%%%%%% output/35--1-2-2-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:35}

\begin{thm}
Tossing a coin $n$ times can lead to $2^n$ different outcomes.
\end{thm}

\end{page}

%%%%%%%%%% output/36--1-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:36}

\begin{proof}
Apply the product rule.
The coin is making $n$ choices, each time from two possibilities.
Thus the number of outcomes is $\underbrace{2  \cdots  2}_n = 2^n$.
\end{proof}


\end{page}

%%%%%%%%%% output/38--1-2-2-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:38}

\begin{thm}
The number of binary sequences of lenght $n$ is $2^n$.
\end{thm}

\end{page}

%%%%%%%%%% output/39--1-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:39}

\begin{proof}
One can argue by the product rule again.
Or, one can establish a bijection between the set of binary sequences and the set of outcomes when tossing a coin:
one encodes an outcome by putting $1$ for ``heads'' and $0$ for ``tails''.
It is easy to see that this is a bijection, thus by the previous theorem the number of binary sequences is $2^n$.
\end{proof}


\end{page}

%%%%%%%%%% output/41--1-2-2-remark.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:41}

\begin{rem}
One might argue that the number of binary sequences of length $n$ is $2^n$ because they represent all numbers from $0$ to $2^n - 1$ in the binary system.
But this result itself requires a proof, which is not easier than our combinatorial argument.
\end{rem}

\end{page}

%%%%%%%%%% output/42--1-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:42}


To summarize, we have found that the number of elements in each of the following sets is $2^n$:
\begin{itemize}
\item
Maps from an $n$-element set to $\{0,1\}$.
\item
Subsets of an $n$-element set.
\item
Outcomes in tossing a coin $n$ times.
\item
Binary sequences of length $n$.
\end{itemize}

Between any two of these sets there is a natural bijection.
Some of these bijections were described above.



\end{page}

%%%%%%%%%% output/43--1-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:43}

\subsection{Counting injective maps and ordered choices}

\end{page}

%%%%%%%%%% output/45--1-2-3-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:45}

\begin{thm}
If $|X|=k$ and $|Y|=n$, then the number of injective maps from $X$ to $Y$ is $n(n-1) \cdots (n-k+1)$.
\end{thm}

\end{page}

%%%%%%%%%% output/46--1-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:46}

\begin{proof}
Without loss of generality, $X = \{1, \ldots, k\}$.
We have $n$ possibilities to choose $f(1)$.
After this, for $f(2)$ there remain $n-1$ possibilities, then $n-2$ possibilities for $f(3)$,
and so on up to $f(k)$, for which $n-(k-1) = n-k+1$ possibilities remain.

The above argument works for $k \le n$, but the formula is true for $k > n$ as well:
there are no injective maps in this case, and the product $n(n-1) \cdots (n-k+1)$ vanishes because it contains a zero factor.
\end{proof}


\end{page}

%%%%%%%%%% output/48--1-2-3-corollary.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:48}

\begin{cor}
\label{cor:NumberOfBijections}
The number of bijective maps between two $n$-element sets is equal $n! = 1 \cdot 2 \cdots n$.
The number of permutations of $n$ elements is equal to $n!$.
\end{cor}

\end{page}

%%%%%%%%%% output/49--1-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:49}

Indeed, a permutation of an $n$-element set $X$ can be thought of as a bijection $\{1, 2, \ldots, n\} \to X$.

The same number appears as the answer to a different problem.
Imagine that we have a bag with $n$ balls.
We take out $k$ balls consecutively and lay them in a line in the order in which they were taken.
How many different outcomes are there?


\end{page}

%%%%%%%%%% output/51--1-2-3-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:51}

\begin{thm}
The number of ordered choices of $k$ balls out of $n$ is
\[
n(n-1)\cdot \ldots \cdot (n-k+1).
\]
\end{thm}

\end{page}

%%%%%%%%%% output/52--1-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:52}

\begin{proof}
One can argue by the product rule, or one can interpret an ordered choice of $k$ balls out of $n$ as an injective map $f \colon \{1, \ldots, k\} \to \{1, \ldots, n\}$.
Here $f(i)$ tells us which of the balls was taken on the $i$-th step.
\end{proof}




\end{page}

%%%%%%%%%% output/53--1-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:53}

\subsection{Unordered choices}
Take the same bag with $n$ balls.
We take out $k$ balls simultaneously.
Equivalently, we take out $k$ balls consecutively, but then forget the order in which they were taken.
How many possibilities are there?


\end{page}

%%%%%%%%%% output/55--1-2-4-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:55}

\begin{thm}
\label{thm:Comb}
The number of unordered choices of $k$ balls out of $n$ is
\[
\frac{n(n-1)\cdot \ldots \cdot (n-k+1)}{k!}.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/56--1-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{11}
\label{portion:56}

\begin{proof}
Let $X$ be the set of all possible ordered choices of $k$ balls out of $n$,
and $Y$ be the set of all unordered choices of $k$ balls.
There is a map $f \colon X \to Y$ (the ``forgetful map'') that associates to
an ordered collection of $k$ balls the same set of balls, but unordered.
(The balls lying in a line are put into another bag.)

For $y \in Y$, what is the cardinality of its preimage $f^{-1}(y)$?
This is the number of ways to order an unordered set of $k$ balls.
An ordering is a bijection to the set $\{1, 2, \ldots, k\}$, and from Corollary \ref{cor:NumberOfBijections} we know that there are $k!$ of them.
Therefore by the quotient rule we have
\[
|Y| = \frac{|X|}{k!} = \frac{n(n-1)\cdot \ldots \cdot (n-k+1)}{k!}.
\]
\end{proof}

As we already said, an unordered choice of $k$ balls out of $n$ is also called a $k$-combination.
Yet another name of this is a \emph{$k$-element subset} of a given $n$-element set.
(By definition, a set is an unordered collection of elements.)

\smallskip

\noindent\textbf{Notation.}
The number of $k$-element subsets of an $n$-element set is denoted by $\binom{n}{k}$
(pronounced ``$n$ choose $k$'').




\end{page}

%%%%%%%%%% output/57--1-2-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{dfn}{11}
\label{portion:57}

\subsection{Birthday problem}
Given $k$ people, what is the probability that some two of them have the same birthday?
How big should $k$ be for this probability to exceed $\frac12$?











\end{page}

%%%%%%%%%% output/58--1-3-0-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:58}

\section{Many faces of $\binom{n}{k}$}
The number $\binom{n}{k}$ that we have introduced in the previous lecture has several interpretations.

\end{page}

%%%%%%%%%% output/59--1-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:59}

\subsection{Subsets or unordered choices}
This is our original definition: $\binom{n}{k}$ is the number of unordered choices of $k$ elements out of $n$.
In a more abstract language, this is the number of $k$-element subsets of an $n$-element set.

We have proved that
\begin{equation}
\label{eqn:NChooseK}
\binom{n}{k} = \frac{n(n-1)\cdot \ldots \cdot (n-k+1)}{k!} = \frac{n!}{k!(n-k)!}.
\end{equation}


\end{page}

%%%%%%%%%% output/61--1-3-1-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:61}

\begin{thm}
\[
\binom{n}{k} = \binom{n}{n-k}
\]
\end{thm}

\end{page}

%%%%%%%%%% output/62--1-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:62}

\begin{proof}[First proof]
This is immediate from \eqref{eqn:NChooseK}: if we replace $k$ by $n-k$, then the factors in the denominator simply interchange.
\end{proof}
\begin{proof}[Second proof]
The theorem can be proved even without knowing a formula for $\binom{n}{k}$, by a bijective argument.
Associate to every subset of an $n$-element set its complement: $A \mapsto X \setminus A$.
This is a bijection, and it sends $k$-element subsets to $(n-k)$-element subsets.
Thus there are as many $k$-element subsets as there are $(n-k)$-element subsets.
\end{proof}


\end{page}

%%%%%%%%%% output/64--1-3-1-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:64}

\begin{thm}
\label{thm:BinomSum}
For every $n$ we have
\begin{equation}
\label{eqn:SumBinCoeff}
\sum_{k=1}^n \binom{n}{k} = \binom{n}{0} + \binom{n}{1} + \cdots + \binom{n}{n} = 2^n.
\end{equation}
\end{thm}

\end{page}

%%%%%%%%%% output/65--1-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:65}

\begin{proof}
By Theorem \ref{thm:AllSubsets}, an $n$-element set has $2^n$ subsets.
By definition, $\binom{n}{k}$ is the number of $k$-element subsets.
(One may say that we are using the sum rule here: the set of all subsets is a disjoint union of sets of $k$-element subsets over all $k$ from $0$ to $n$.)
\end{proof}


\end{page}

%%%%%%%%%% output/67--1-3-1-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:67}

\begin{thm}
\label{thm:BinWordsK}
The number $\binom{n}{k}$ is equal to the number of binary words of length $n$ containing exactly $k$ digits $1$.
\end{thm}

\end{page}

%%%%%%%%%% output/68--1-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:68}

\begin{proof}
A subset of $\{1, 2, \ldots, n\}$ can be encoded with a binary word:
the $i$-th digit of this word is $1$ if and only if the number $i$ belongs to the subset.
The words with $k$ digits $1$ correspond to the $k$-element subsets, hence there are $\binom{n}{k}$ such words.
\end{proof}




\end{page}

%%%%%%%%%% output/69--1-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:69}

\subsection{Monotone paths}
Take the square lattice in the plane.
The nodes of the lattice can be identified with pairs of integers $(k,l)$ (Cartesian coordinates).
Take two non-negative integers $k, l$ and mark the points $(0,0)$ and $(k,l)$.
A \emph{lattice path} from $(0,0)$ to $(k,l)$ is a sequence of segments of the square lattice
that leads from $(0,0)$ to $(k,l)$.
A lattice path is called \emph{monotone} if it runs only upwards and to the right.
See an example on Figure \ref{fig:MonPath}.

\begin{figure}[htb]
\begin{center}
\input{Fig/MonPath.pdf_t}
\end{center}
\caption{A monotone path.}
\label{fig:MonPath}
\end{figure}


\end{page}

%%%%%%%%%% output/71--1-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:71}

\begin{thm}
The number of monotone paths from $(0,0)$ to $(k,l)$ is $\binom{k+l}{k}$.
\end{thm}

\end{page}

%%%%%%%%%% output/72--1-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:72}

\begin{proof}
A monotone path can be encoded by a binary word,
where $1$ stands for ``a step to the right'' and $0$ stands for ``a step upwards''.
In order to attain the point $(k,l)$, we need to make $k$ steps to the right and $l$ steps upwards.
Therefore the monotone paths from $(0,0)$ to $(k,l)$ correspond to words of length $k+l$
containing exactly $k$ digits $1$.
From Theorem \ref{thm:BinWordsK} we know that the number of such words is $\binom{k+l}{k}$.
\end{proof}



\end{page}

%%%%%%%%%% output/73--1-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:73}

\subsection{Pascal's triangle}
Write two rows of $1$ starting at the same place and going one downwards to the left and the other downwards to the right.
Then fill in this frame row after row according to the rule that every number is equal to the sum
of its top-left and top-right neighbors.

\begin{center}
\begin{tabular}{ccccccccccc}
&    &    &    &    &  1\\\noalign{\smallskip\smallskip}
&    &    &    &  1 &    &  1\\\noalign{\smallskip\smallskip}
&    &    &  1 &    &  2 &    &  1\\\noalign{\smallskip\smallskip}
&    &  1 &    &  3 &    &  3 &    &  1\\\noalign{\smallskip\smallskip}
&  1 &    &  4 &    &  6 &    &  4 &    & 1\\\noalign{\smallskip\smallskip}
1&   &  5 &    & 10 &    & 10 &    &  5 &   & 1\\\noalign{\smallskip\smallskip}
\end{tabular}
\end{center}

One adopts the convention that the rows of the Pascal triangle as well as the numbers in each row are numbered starting from $0$.
Thus the row number $n$ contains $n+1$ entries numbered with $0$ up to $n$.


\end{page}

%%%%%%%%%% output/75--1-3-3-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:75}

\begin{thm}
\label{thm:BinPascal}
The $k$-th number in the $n$-th row of the Pascal triangle is equal to $\binom{n}{k}$.
\end{thm}

\end{page}

%%%%%%%%%% output/76--1-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:76}


We will need a lemma.

\end{page}

%%%%%%%%%% output/78--1-3-3-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:78}

\begin{lem}
\label{lem:BinCoeffRec}
For every $0 \le k \le n$ the following identity holds:
\begin{equation}
\label{eqn:BinCoeffRec}
\binom{n}{k} = \binom{n-1}{k-1} + \binom{n-1}{k}.
\end{equation}
\end{lem}

\end{page}

%%%%%%%%%% output/79--1-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:79}

\begin{proof}
We know that $\binom{n}{k}$ is the number of binary words of length $n$ with exactly $k$ digits $1$.
There are two kinds of words like that: those that start with $1$ and those that start with $0$.
How many words of each kind are there?

When we delete the first digit, we are left with a word of length $n-1$.
For the words of the first kind, this word of length $n-1$ must contain $k-1$ digits $1$.
Thus there are $\binom{n-1}{k-1}$ words of the first kind.

Similarly, for a word of second kind we are left with a word of length $n-1$ that contains $k$ digits $1$.
Thus there are $\binom{n-1}k$ words of the second kind.

Since every word is either of the first kind or of the second kind but not both,
identity \eqref{eqn:BinCoeffRec} holds.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:BinPascal}]
Let us write the numbers $\binom{n}{k}$ in a triangle similar to the Pascal triangle:
\begin{center}
\begin{tabular}{ccccccccccc}
&    &    &    &    &  $\binom{0}{0}$\\\noalign{\smallskip\smallskip}
&    &    &    &  $\binom{1}{0}$ &    &  $\binom{1}{1}$\\\noalign{\smallskip\smallskip}
&    &    &  $\binom{2}{0}$ &    &  $\binom{2}{1}$ &    &  $\binom{2}{2}$\\\noalign{\smallskip\smallskip}
&    &  $\binom{3}{0}$ &    &  $\binom{3}{1}$ &    &  $\binom{3}{2}$ &    &  $\binom{3}{3}$\\\noalign{\smallskip\smallskip}
&  $\binom{4}{0}$ &    &  $\binom{4}{1}$ &    &  $\binom{4}{2}$ &    &  $\binom{4}{3}$ &    & $\binom{4}{4}$\\\noalign{\smallskip\smallskip}
$\binom{5}{0}$ &   &  $\binom{5}{1}$ &    & $\binom{5}{2}$ &    & $\binom{5}{3}$ &    &  $\binom{5}{4}$ &   & $\binom{5}{5}$\\\noalign{\smallskip\smallskip}
\end{tabular}
\end{center}
The top-left neighbor of the number $\binom{n}{k}$ is $\binom{n-1}{k-1}$, the top-right neighbor is $\binom{n-1}{k}$.
By Lemma \ref{lem:BinCoeffRec}, the numbers in the $\binom{n}{k}$-triangle satisfy the same rule that the numbers in the Pascal triangle:
each number is the sum of its top-left and top-right neighbors.
The outermost numbers $\binom{n}{0}$ and $\binom{n}{n}$ are also the same as in the Pascal triangle:
\[
\binom{n}{0} = \binom{n}{n} = 1.
\]
It follows that the $\binom{n}{k}$-triangle coincides with the Pascal triangle.
(The formal argument here is proof by induction:
if the $n$-th line of the $\binom{n}{k}$-triangle coincides with the $n$-th line of the Pascal triangle,
then their $(n+1)$-st lines also coincide.)
\end{proof}





\end{page}

%%%%%%%%%% output/80--1-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:80}

\subsection{Binomial theorem}
The binomial theorem is the generalization of the well-known formulas
\[
(a+b)^2 = a^2 + 2ab + b^2, \quad (a+b)^3 = a^3 + 3a^2b + 3ab^2 + b^3.
\]


\end{page}

%%%%%%%%%% output/82--1-3-4-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{7}
\label{portion:82}

\begin{thm}
For any $n \in \N$ we have
\begin{multline*}
(a+b)^n = \sum_{k=0}^n \binom{n}{k} a^{n-k}b^k\\
= \binom{n}{0}a^n + \binom{n}{1}a^{n-1}b + \binom{n}{2}a^{n-2}b^2 + \cdots + \binom{n}{n}b^n
\end{multline*}
\end{thm}

\end{page}

%%%%%%%%%% output/83--1-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{7}
\label{portion:83}


The coefficients in the above formula come from the $n$-th row of the Pascal triangle.
For example, by looking at the Pascal triangle we can conclude
\[
(a+b)^5 = a^5 + 5a^4b + 10a^3b^2 + 10a^2b^3 + 5ab^4 + b^5.
\]

\begin{proof}
How do we prove $(a+b)^2 = a^2 + 2ab + b^2$?
For this, we write $(a+b)^2 = (a+b)(a+b)$, multiply
every term inside the first pair of brackets with every term inside the second pair of brackets,
and finally collect the like terms:
\[
(a+b)^2 = (a+b)(a+b) = a^2 + ab + ba + b^2 = a^2 + 2ab + b^2.
\]
What happens when we multiply out $n$ pairs of brackets $(a+b)$?
\[
(a+b)^n = \underbrace{(a+b)(a+b) \cdots (a+b)}_{n \text{ pairs of brackets}}
\]
Before collecting the like terms, we obtain a sum of products of $n$ factors,
every factor being $a$ or $b$.
That is to say, we are writing down all words of length $n$ consisting of letters $a$ and $b$.
When collecting the like terms, we ignore the order of letters in each word, counting only
the number of $a$'s and $b$'s.
That is to say, the term $a^{n-k}b^k$ occurs in our sum as often as
there are $(a,b)$-words of length $n$ with $n-k$ letters $a$ and $k$ letters $b$.
But we know that there are $\binom{n}{k}$ such words, and the theorem follows.
\end{proof}

Instead of $a$ and $b$ we can substitute any numbers or expressions.
For example, we have
\[
(1+x)^n = \sum_{k=0}^n \binom{n}{k} x^k.
\]
Note that by substituting $x=1$ we obtain a new (but a quite intricate) proof of Theorem \ref{thm:BinomSum}.


\end{page}

%%%%%%%%%% output/85--1-3-4-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{8}
\label{portion:85}

\begin{thm}
\label{thm:BinAltern}
For any $n$ we have
\[
\binom{n}{0} - \binom{n}{1} + \cdots + (-1)^n\binom{n}{n} = 0
\]
\end{thm}

\end{page}

%%%%%%%%%% output/86--1-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{8}
\label{portion:86}

This is obvious for odd $n$: because of $\binom{n}{k} = \binom{n}{n-k}$ the summands can be split into pairs that cancel each other.
For $n$ even there is no cancellation.
\begin{proof}
Substitute $a=1$ and $b=-1$ in the binomial theorem.
\end{proof}




\end{page}

%%%%%%%%%% output/87--1-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{8}
\label{portion:87}

\subsection{Compositions}
\label{sec:Comp}

\end{page}

%%%%%%%%%% output/89--1-3-5-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{9}
\label{portion:89}

\begin{dfn}
A \emph{composition} of a positive integer $n$ is a representation of $n$ as the sum of positive integers.
The order of summands plays the role, for example $5 = 3+1+1$ and $5=1+3+1$ are different compositions of $5$.
\end{dfn}

\end{page}

%%%%%%%%%% output/92--1-3-5-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{10}
\label{portion:92}

\begin{thm}
\label{thm:Compositions}
The number of compositions of $n$ from $k$ parts is $\binom{n-1}{k-1}$.
\end{thm}

\end{page}

%%%%%%%%%% output/93--1-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{10}
\label{portion:93}

\begin{proof}
Lay $n$ stones in a row.
To form a composition of the number $n$ we must separate these stones by $k-1$ sticks.
The numbers of stones between consecutive sticks sum up to $n$, thus form a composition of $n$.
For example,
\[
\bullet\, | \bullet\, \bullet\, \bullet |\, \bullet \quad \longleftrightarrow \quad 5 = 1+3+1
\]
In how many ways can we put $k-1$ sticks between $n$ stones?
There are $n-1$ gaps, and we must choose $k-1$ of them.
There are $\binom{n-1}{k-1}$ ways to do this. The theorem is proved.
\end{proof}


\end{page}

%%%%%%%%%% output/95--1-3-5-corollary.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{11}
\label{portion:95}

\begin{cor}
For every positive integers $k \le n$ the equation
\[
x_1 + x_2 + \cdots + x_k = n
\]
has $\binom{n-1}{k-1}$ solutions in positive integers.
\end{cor}

\end{page}

%%%%%%%%%% output/96--1-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{11}
\label{portion:96}

\begin{proof}
This is nothing else but a formal description of a composition of the number $n$.
\end{proof}


\end{page}

%%%%%%%%%% output/98--1-3-5-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{12}
\label{portion:98}

\begin{dfn}
A \emph{weak composition} of a positive integer $n$ is a representation of $n$ as the sum of non-negative integers.
\end{dfn}

\end{page}

%%%%%%%%%% output/101--1-3-5-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{13}
\label{portion:101}

\begin{thm}
\label{thm:WeakComp}
The number of weak compositions of $n$ from $k$ parts is $\binom{n+k-1}{k-1}$.
\end{thm}

\end{page}

%%%%%%%%%% output/102--1-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{13}
\label{portion:102}

\begin{proof}[First proof]
Let us establish a bijection between the weak compositions of $n$ from $k$ parts and the compositions of $n+k$ from $k$ parts.
Indeed, adding $1$ to every summand in a weak composition of $n$ transforms it into a (strong) composition of $n+k$.
In the opposite direction, subtracting $1$ from every summand of a composition of $n+k$ transforms it into
a weak composition of $n$.
This is a one-to-one correspondence (a bijection).
By Theorem \ref{thm:Compositions}, the number of (strong) compositions of $n+k$ from $k$ parts is $\binom{n+k-1}{k-1}$, and we are done.
\end{proof}

\begin{proof}[Second proof]
You have $n$ stones and $k-1$ sticks.
Mark $n+k-1$ spots on the ground.
You have to choose $k-1$ among them where you lay sticks, then you will lay your stones on the remaining spots.
There are $\binom{n+k-1}{k-1}$ different arrangements, and they correspond to weak compositions of $n$ from $k$ parts.
For example, if the stones are on the first $k-1$ spots, then the first $k-1$ summands are equal to zero, and the $k$-th summand equals $n$.
\end{proof}



\end{page}

%%%%%%%%%% output/103--1-3-6-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{13}
\label{portion:103}

\subsection{Multisets}
Imagine a bag with $k$ balls numbered by $1, 2, \ldots, k$.
We are taking out a ball, writing down its number, and then putting the ball back into the bag.
This is done $n$ times.
From the product rule we know that $k^n$ different sequences of numbers are possible.
(One can see such a sequence as a map $f \colon \{1, \ldots, n\} \to \{1, \ldots, k\}$: here $f(i)$ is the number at the $i$-th place.)
But what if we don't care for the order of the results, but only count how often each ball was taken?
For example, the sequences $(2, 1, 4, 4)$ and $(4, 1, 4, 2)$ are considered as the same.
How many different combinations of balls are possible?

One cannot proceed by the quotient rule, because the number of different orderings of a sequence with repetitions depends on how often the repetitions occur.


\end{page}

%%%%%%%%%% output/105--1-3-6-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{14}
\label{portion:105}

\begin{dfn}
A \emph{multiset} is an unordered collection of elements with possible repetitions.
\end{dfn}

\end{page}

%%%%%%%%%% output/106--1-3-6-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{14}
\label{portion:106}

Our question can be formulated as ``how many $n$-multisets, with elements taken from $\{1, 2, \ldots, k\}$ are there?''


\end{page}

%%%%%%%%%% output/108--1-3-6-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{15}
\label{portion:108}

\begin{thm}
The number of different $n$-multisets with elements taken from $\{1, 2, \ldots, k\}$ is $\binom{n+k-1}{k-1}$.
\end{thm}

\end{page}

%%%%%%%%%% output/109--1-3-6-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{dfn}{15}
\label{portion:109}

\begin{proof}
There is a bijection between $n$-multisets and weak compositions of $n$ from $k$ parts.
Namely, to a weak composition $n = x_1 + \cdots + x_k$ of $n$ we associate the multiset
where $1$ occurs $x_1$ times, $2$ occurs $x_2$ times etc.
The number of weak compositions was computed in Theorem \ref{thm:WeakComp}.
\end{proof}



\end{page}

%%%%%%%%%% output/110--1-4-0-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:110}

\section{Multinomial coefficients}

\end{page}

%%%%%%%%%% output/111--1-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:111}

\subsection{Words with repeating letters}
Consider the following problem:
\begin{quote}
\emph{How many different words of length $n=k+l+m$ can be written with $k$ letters $a$, $l$ letters $b$, and $m$ letters $c$?}
\end{quote}

We have $n$ places for the letters.
First choose $k$ places where to put the letters $a$.
This can be done in $\binom{n}{k}$ different ways.
Then from $n-k$ remaining places choose $l$ places where to put the letters $b$.
This can be done in $\binom{n-k}{l}$ different ways.
Thus by the (general) product rule the number of different words is
\[
\binom{n}{k} \binom{n-k}{l} = \frac{n!}{k!(n-k)!} \frac{(n-k)!}{l!(n-k-l)!} = \frac{n!}{k!l!m!}.
\]

We might have started by choosing $l$ places for the letters $b$, and then, say, choose $m$ places for the letters $c$.
Then we would compute the product
\[
\binom{n}{l} \binom{n-l}{m}
\]
which is the same.

The following notation is used:
\[
\frac{n!}{k!l!m!} =: \binom{n}{k, l, m}.
\]

Let us now consider a more general problem and solve it in a different way.


\end{page}

%%%%%%%%%% output/113--1-4-1-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:113}

\begin{thm}
Let $n$ balls of $m$ different colors be given, with $k_i$ balls of $i$-th color for all $i$ (so that $k_1 + \cdots + k_m = n$).
Considering balls of the same color undistinguishable, the whole set of $n$ balls can be arranged in a row in
\[
\binom{n}{k_1, \ldots, k_m} = \frac{n!}{k_1! \cdots k_m!}
\]
different ways.
\end{thm}

\end{page}

%%%%%%%%%% output/114--1-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:114}

\begin{proof}
Make the balls of the same color distinguishable (by writing on them numbers, for example).
Then all $n$ balls can be arranged in $n!$ different ways.
Now apply the quotient rule.
There is a forgetful map from the set $X$ of arrangements with balls of the same color distinguishable
to the set $Y$ of arrangements where balls of the same color are undistinguishable.
What is the multiplicity of this map, that is the cardinalities of the preimages $|f^{-1}(y)|$?
The balls of the $i$-th color can be permuted amongst themselves in $k_i!$ different ways.
Permutations within every color can be performed independently, which gives us $k_1! \cdots k_m!$ elements in $Y$
all of which correspond to the same element in $X$.
Thus we have $|f^{-1}(y)| = k_1! \cdots k_m!$, and hence
\[
|Y| = \frac{|X|}{k_1! \cdots k_m!} = \frac{n!}{k_1! \cdots k_m!}.
\]
\end{proof}




\end{page}

%%%%%%%%%% output/115--1-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:115}

\subsection{Multinomial theorem}
The numbers
\[
\binom{n}{k_1, \ldots, k_r}
\]
are called \emph{multinomial coefficients}.
(It would be just logical to call $\binom{n}{k, l, m}$ a trinomial coefficient.
Unfortunately, this term is sometimes used for the entries in an analog of Pascal triangle,
where every number is equal to the sum of three numbers from the previous line, so a confusion may arise...)
Similarly to the binomial coefficients, the multinomial coefficients appear in the expansion of the $n$-th power of a sum.
Now the sum has not two, but $m$ terms.


\end{page}

%%%%%%%%%% output/117--1-4-2-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:117}

\begin{thm}[Multinomial theorem]
\[
(a_1 + \cdots + a_m)^n = \sum_{\substack{k_1 + \cdots + k_m = n\\ k_1, \ldots, k_m \ge 0}}
\binom{n}{k_1, \ldots, k_m} a_1^{k_1} \cdots a_m^{k_m}
\]
\end{thm}

\end{page}

%%%%%%%%%% output/118--1-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{2}
\label{portion:118}

\begin{proof}
Multiply the brackets respecting the order of the factors.
We obtain the sum of all words of length $n$ from the letters $a_1, \ldots, a_m$.
When collecting the like terms, we disregard the order of the letters, looking only at the number of occurences of every letter.
The words with $k_i$ letters $a_i$ (for all $i$ from $1$ to $m$) become the terms $a_1^{k_1} \cdots a_m^{k_m}$.
The coefficient at this term is the number of the words made of $k_1$ letters $a_1$,... $k_m$ letters $a_m$,
that is $\binom{n}{k_1, \ldots, k_m}$.
\end{proof}



\end{page}

%%%%%%%%%% output/119--1-4-3-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{2}
\label{portion:119}

\subsection{Monotone paths in higher dimensions}
Consider the cubical grid in $\R^m$.
It is possible to visualize for $m=3$; for larger $m$ one uses an abstract description.
Mark the points $(0, \ldots, 0)$ and $(k_1, \ldots, k_m)$ in this grid.
A monotone path is one that moves along the grid lines and in the positive directions only.


\end{page}

%%%%%%%%%% output/121--1-4-3-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{3}
\label{portion:121}

\begin{thm}
The number of monotone paths from $(0, \ldots, 0)$ to $(k_1, \ldots, k_m)$ is $\binom{n}{k_1, \ldots, k_m}$.
\end{thm}

\end{page}

%%%%%%%%%% output/122--1-4-3-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{dfn}{3}
\label{portion:122}

\begin{proof}
Monotone paths can be encoded by words: write the letter $a_i$ for a step along the $i$-th axis.
Then a monotone path from $(0, \ldots, 0)$ to $(k_1, \ldots, k_m)$ corresponds to a word
consisting of $k_i$ letters $a_i$ for all $i$ from $1$ to $m$.
\end{proof}




\end{page}

%%%%%%%%%% output/123--1-5-0-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:123}

\section{Inclusion-exclusion formula}

\end{page}

%%%%%%%%%% output/124--1-5-1-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:124}

\subsection{The formula}
If $A \cap B \ne \emptyset$, then
\[
|A \cup B| = |A| + |B| - |A \cap B|.
\]
Indeed, in $|A|+|B|$ we have counted all elements that belong to both $A$ and $B$ twice, see Figure \ref{fig:IncExc2}, left.
Subtracting $|A \cap B|$ makes our count correct, see Figure \ref{fig:IncExc2}, right.

\begin{figure}[ht]
\begin{center}
\input{Fig/IncExc2.pdf_t}
\end{center}
\caption{Counting the elements in the union of two sets.}
\label{fig:IncExc2}
\end{figure}

Let us now count the elements in the union of three sets $A \cup B \cup C$.
In the sum
\[
|A| + |B| + |C|
\]
every element is counted as many times as to how many sets it belongs, see Figure \ref{fig:IncExc3}, left.
Let us subtract the numbers of the elements in the pairwise intersections:
\[
|A| + |B| + |C| - |A \cap B| - |B \cap C| - |A \cap C|.
\]
Now every element that belongs to one or two sets is counted exactly once,
but the elements in $A \cap B \cap C$ are not counted at all, see Figure \ref{fig:IncExc3}, middle.
So it remains to add the number of these elements to obtain the final formula:
\[
|A \cup B \cup C| = |A| + |B| + |C| - |A \cap B| - |B \cap C| - |A \cap C| + |A \cap B \cap C|.
\]

\begin{figure}[ht]
\begin{center}
\input{Fig/IncExc3.pdf_t}
\end{center}
\caption{Counting the elements in the union of three sets.}
\label{fig:IncExc3}
\end{figure}

What will the formula for the number of elements in the union of $n$ sets look like?
The formulas for $n=2$ and $n=3$ suggest that this will be the sum of the cardinalities of all sets
minus the sum of the cardinalities of pairwise intersections plus all the triple intersections
minus all the quadruple intersections, and so on.
This conjecture is true, and we will prove it now.


\end{page}

%%%%%%%%%% output/126--1-5-1-theorem.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:126}

\begin{thm}[Inclusion-exclusion formula]
\label{thm:IncExc}
\begin{multline*}
|A_1 \cup \cdots \cup A_n| = \sum_{i=1}^n |A_i| - \sum_{1 \le i < j \le n} |A_i \cap A_j| 
+ \sum_{1 \le i < j < k \le n} |A_i \cap A_j \cap A_k| - \cdots \\
\cdots +(-1)^{n-1} |A_1 \cap \cdots \cap A_n|
\end{multline*}
\end{thm}

\end{page}

%%%%%%%%%% output/127--1-5-1-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:127}

\begin{proof}
We will show that every element of the union is counted exactly once on the right hand side of the formula.

(Can make a warm-up for the elements in the intersection of all sets.)
Take an element that belongs to exactly $k$ of the sets $A_1, \ldots, A_n$.
In the first sum on the right hand side it is counted $k$ times,
in the second sum $\binom{k}{2}$ times, and so on.
In total, this element is counted with the multiplicity
\[
\binom{k}{1} - \binom{k}{2} + \cdots + (-1)^{k-1}\binom{k}{k}.
\]
Due to Theorem \ref{thm:BinAltern}, this sum is equal to $\binom{k}{0} = 1$.
\end{proof}


\end{page}

%%%%%%%%%% output/129--1-5-1-remark.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:129}

\begin{rem}
One can give an exact meaning to the words ``how many times was an element counted''.
Instead of the intersections $A_i \cap A_j$ etc. consider their indicator functions $\mathbf{1}_{A_i \cap A_j}$.
Instead of summing the cardinalities, sum these functions.
The diagrams on Figures \ref{fig:IncExc2} and \ref{fig:IncExc3} show the values of certain sums of indicator functions.
The argument in the proof of Theorem \ref{thm:IncExc} shows that
\begin{multline*}
\mathbf{1}_{|A_1 \cup \cdots \cup A_n|} = \sum_{i=1}^n \mathbf{1}_{|A_i|} - \sum_{1 \le i < j \le n} \mathbf{1}_{|A_i \cap A_j|} 
+ \sum_{1 \le i < j < k \le n} \mathbf{1}_{|A_i \cap A_j \cap A_k|} - \cdots \\
\cdots +(-1)^{n-1} \mathbf{1}_{|A_1 \cap \cdots \cap A_n|}
\end{multline*}
by comparing the values of the functions on the left and on the right at every point.
The formula for the number of elements is obtained by taking the ``integrals'' of both sides,
that is replacing each function $f$ by the number $\sum_{x \in A_1 \cup \cdots \cup A_n} f(x)$.
\end{rem}

\end{page}

%%%%%%%%%% output/131--1-5-2-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:131}

\subsection{De Montmort problem, or counting the derangements}
The problem was originally posed by Pierre R\'emond de Montmort in 1708, and was solved by him and, independently, Nicholas Bernoulli.

De Montmort stated it in terms of a game of cards, later it became popular in the following formulation:

\begin{quote}
\emph{The guests leaving a party are taking their hats in the garderobe.
In the darkness they cannot tell the hats one from the other, so everybody takes a hat by chance.
What is the probability that nobody will get his own hat?}
\end{quote}

Here is a formal description of the problem.
Consider a bijection
\[
f \colon \{1, 2, \ldots, n\} \to \{1, 2, \ldots, n\}.
\]
Such a bijection is called a permutation.
An element $x \in \{1, 2, \ldots, n\}$ is called a \emph{fixed point} of $f$ if $f(x) = x$.
A permutation without fixed points is sometimes called a \emph{derangement}.
The probability to be computed is equal to
\[
\frac{\#\text{derangements}}{\#\text{permutations}}.
\]
The number of permutations is known: it is $n!$.
Thus we have to count the number of derangements.


\end{page}

%%%%%%%%%% output/133--1-5-2-theorem.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:133}

\begin{thm}
The number of all derangements of $n$ elements is equal to
\[
\sum_{k=0}^n (-1)^k \binom{n}{k} (n-k)! = \sum_{k=0}^n (-1)^k \frac{n!}{k!}.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/134--1-5-2-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{3}
\setcounter{dfn}{3}
\label{portion:134}

\begin{proof}
Let $A_i$ be the set of permutations $f$ such that $f(i) = i$.
Then $A_1 \cup \cdots \cup A_n$ is the set of all non-derangements, permutations with at least one fixed points.
Calculate its cardinality by the inclusion-exclusion formula.
The intersection $|A_{i_1} \cap \cdots \cap A_{i_k}|$ consists of all permutations satisfying the conditions
\[
f(i_1) = i_1, \ldots, f(i_k) = i_k.
\]
The number of such permutations is $(n-k)!$
(to determine $f$, it remains to map bijectively an $(n-k)$-element set to an $(n-k)$-element set).
Thus we have
\[
|A_{i_1} \cap \cdots \cap A_{i_k}| = (n-k)!
\]
for any choice of $k$ sets out of $A_1, \ldots, A_n$.
Since the number of such choices is $\binom{n}{k}$, we have
\[
|A_1 \cup \cdots \cup A_n| = n (n-1)! - \binom{n}{2} (n-2)! + \cdots + (-1)^{n-1} \binom{n}{n} 0!.
\]
To count the derangements, we subtract the number of non-derangements from the number of all permutations,
which leads to the formula in the theorem.
\end{proof}

Now we can compute the probability that no guest gets his hat:
\[
\frac{\#\text{derangements}}{\#\text{permutations}} = \frac{\sum_{k=0}^n (-1)^k \frac{n!}{k!}}{n!} = \sum_{k=0}^n \frac{(-1)^k}{k!}.
\]
Recall that
\[
\sum_{k=0}^\infty \frac{x^k}{k!} = e^x.
\]
It follows that the above probability converges to $\frac{1}{e} \approx 0.37$.



\end{page}

%%%%%%%%%% output/135--1-5-3-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{3}
\setcounter{dfn}{3}
\label{portion:135}

\subsection{Euler's totient function}
Two positive integers whose greatest common divisor equals $1$ are called relatively prime.
Denote by $\phi(n)$ the number of positive integers $\le n$ which are relatively prime to $n$:
\[
\phi(n) = \#\{k \in \{1, 2, \ldots, n\} \mid \gcd(k,n) = 1\}.
\]
For example, for $n = 6$ only the numbers $1$ and $5$ among $1, 2, 3, 4, 5, 6$ are relatively prime to $6$,
so that we have $\phi(6) = 2$.
In $n=p$ is prime, then all numbers $1, \ldots, p-1$ are relatively prime to $p$, so that $\phi(p) = p-1$.


\end{page}

%%%%%%%%%% output/137--1-5-3-theorem.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:137}

\begin{thm}
Let $n = p_1^{\alpha_1} \cdots p_m^{\alpha_m}$ be a prime factorization of $n$, that is
$p_1, \ldots, p_m$ are distinct prime numbers, and $\alpha_1, \ldots, \alpha_m$ are positive integers.
Then
\[
\phi(n) = n\left(1-\frac{1}{p_1}\right) \cdots \left(1-\frac{1}{p_m}\right)
= p_1^{\alpha_1-1}(p_1-1) \cdots p_m^{\alpha_m-1}(p_m-1).
\]
\end{thm}

\end{page}

%%%%%%%%%% output/138--1-5-3-other.tex
\begin{page}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{dfn}{4}
\label{portion:138}

\begin{proof}
Let $A_i$ be the set of numbers among $1, 2, \ldots, n$ divisible by $p_i$.
Then $A_1 \cup \cdots \cup A_m = \{k \in \{1, 2, \ldots, n\} \mid \gcd(k,n) > 1\}$ and we have
\[
\phi(n) = n - |A_1 \cup \cdots \cup A_m|.
\]
To compute $|A_1 \cup \cdots \cup A_m|$, use the inclusion-exclusion formula.
We have
\[
A_i = \{p_i, 2p_i, \ldots, \frac{n}{p_i} p_i\},
\]
thus $|A_i| = \frac{n}{p_i}$.
Similarly, we have
\[
|A_{i_1} \cap \cdots \cap A_{i_k}| = \frac{n}{p_{i_1} \cdots p_{i_k}}.
\]
By the inclusion-exclusion formula,
\[
|A_1 \cup \cdots \cup A_m| = \sum_{i=1}^m \frac{n}{p_i} - \sum_{i<j} \frac{n}{p_ip_j} + \cdots + (-1)^{m-1} \frac{n}{p_1\cdots p_m}.
\]
Thus we have
\begin{multline*}
\phi(n) = n - |A_1 \cup \cdots \cup A_m|\\
= n \left( 1 - \sum_{i<j} \frac{1}{p_i} + \cdots + (-1)^m \frac{1}{p_1 \cdots p_m} \right)\\
= n \left(1-\frac{1}{p_1}\right) \cdots \left(1-\frac{1}{p_m}\right).
\end{multline*}


\end{proof}





\end{page}

%%%%%%%%%% output/139--2-0-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{0}
\setcounter{dfn}{4}
\label{portion:139}

\chapter{Graph theory}

\end{page}

%%%%%%%%%% output/140--2-1-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:140}

\section{Basic notions}

\end{page}

%%%%%%%%%% output/141--2-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:141}

\subsection{Types of graphs}
Intuitively, a graph is a set of points, some of which are joined by lines.
The points are called \emph{vertices} of the graph, the lines are called \emph{edges} of the graph.
Quite often, we represent a graph by drawing it in the plane.
For some graphs, one can draw the edges in such a way that they do not intersect.
But for other graphs self-intersections are inavoidable.
In order not to confuse the intersection points with the ``true'' vertices, we draw the vertices as small disks.
See Figure \ref{fig:TwoDrawings} for two drawings of the same graph: one with, the other without self-intersections.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.7\textwidth]{HouseTwoDrawings.pdf}
\end{center}
\caption{Two drawings of the same graph.}
\label{fig:TwoDrawings}
\end{figure}

Let us give a formal definition.

\end{page}

%%%%%%%%%% output/143--2-1-1-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:143}

\begin{dfn}
\label{dfn:Graph}
A graph $G$ is a pair $(V,E)$, where $V$ is some set and $E$ is a collection of two-element subsets (unordered pairs of elements) of $V$.
The set $V$ is called the vertex set of $G$, the set $E$ is called the edge set of $G$.
\end{dfn}

\end{page}

%%%%%%%%%% output/144--2-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:144}


Examples of graphs: transport networks, neural networks, ``friendship'' graphs (social networks).
Usually the set $V$ of vertices is assumed to be finite.
However it can be quite large (and it is in some of the above examples).

In some situations one is lead to consider graphs with \emph{loops} (lines joining a vertex to itself)
and \emph{multiple edges} (several lines between the same pair of vertices), see Figure \ref{fig:OtherGraphs}, left.
In some other situations one wants to draw arrows instead of lines, see Figure \ref{fig:OtherGraphs}, right.
Graphs with oriented edges are called \emph{directed graphs}.
Another type of graphs are \emph{weighted graphs}: here to every edge a number is assigned.
When we say ``a graph'', we mean it in the sense of definition \ref{dfn:Graph}: an undirected graph without loops and multiple edges
and without assignment of weights.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{OtherGraphs.pdf}
\end{center}
\caption{A graph with loops and multiple edges; a directed graph.}
\label{fig:OtherGraphs}
\end{figure}



\end{page}

%%%%%%%%%% output/145--2-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:145}

\subsection{Some graphs known by names}
Below are some important families of graphs.
\begin{itemize}
\item
The \emph{complete graph} $K_n$ is a graph with $n$ vertices, with each pair of vertices joined by an edge.
\item
The \emph{cycle graph} $C_n$ is a graph with $n$ vertices $v_1, \cdots, v_n$ and the edges $\{v_i, v_{i+1}\}$ for $i = 1, \ldots, n-1$ and $\{v_1, v_n\}$.
\item
The \emph{path graph} $P_n$ is a graph with $n$ vertices $v_1, \cdots, v_n$ and the edges $\{v_i, v_{i+1}\}$ for $i = 1, \ldots, n-1$.
\end{itemize}


\end{page}

%%%%%%%%%% output/147--2-1-2-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:147}

\begin{dfn}
A graph $G = (V, E)$ is called \emph{bipartite} if its vertex set can be partitioned in two sets $V_1$ and $V_2$ such that
no two vertices from $V_1$ are joined by an edge and no two vertices from $V_2$ are joined by an edge.
\end{dfn}

\end{page}

%%%%%%%%%% output/148--2-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{2}
\label{portion:148}


\begin{itemize}
\item
The \emph{complete bipartite graph} $K_{m,n}$ is a bipartite graph with the vertex set $V_1 \cup V_2$,
where $|V_1| = m$, $|V_2| = n$, and every vertex from $V_1$ is joined with every vertex from $V_2$.
The graph $K_{3,3}$ is shown in Figure~\ref{fig:K33}, left.
\end{itemize}



\end{page}

%%%%%%%%%% output/149--2-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{2}
\label{portion:149}

\subsection{Isomorphic graphs and subgraphs}
\label{sec:Subgraphs}

\end{page}

%%%%%%%%%% output/151--2-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{3}
\label{portion:151}

\begin{dfn}
Two graphs $(V, E)$ and $(V', E')$ are called \emph{isomorphic} if there is a bijection $f \colon V \to V'$
between their vertex sets such that
\[
\{x, y\} \in E \text{ if and only if } \{f(x), f(y)\} \in E'.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/154--2-1-3-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:154}

\begin{exl}
The graphs in Figure \ref{fig:K33} are isomorphic (check this!).
Thus the graph on the right is also $K_{3,3}$.
\end{exl}

\end{page}

%%%%%%%%%% output/155--2-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:155}


\begin{figure}[ht]
\begin{center}
\includegraphics[width=.9\textwidth]{K33.pdf}
\end{center}
\caption{The complete bipartite graph $K_{3,3}$.}
\label{fig:K33}
\end{figure}


\end{page}

%%%%%%%%%% output/157--2-1-3-remark.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:157}

\begin{rem}
Isomorphic graphs obviously have the same number of vertices and the same number of edges.
The converse is not true: one can find two graphs with the same number of vertices and the same number of edges
which are not isomorphic.
\end{rem}

\end{page}

%%%%%%%%%% output/160--2-1-3-exercise.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:160}

\begin{exc}
The graph shown in Figure \ref{fig:Petersen} is called \emph{Petersen graph}.
Show that it is isomorphic to the following graph:
\begin{gather*}
V = \{\text{all two-element subsets of }\{1, 2, 3, 4, 5\}\},\\
E = \{\{A, B\} \mid A \cap B = \emptyset\}
\end{gather*}
\end{exc}

\end{page}

%%%%%%%%%% output/161--2-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:161}


\begin{figure}[h]
\begin{center}
\includegraphics[width=.4\textwidth]{Fig/PetersenGraph.pdf}
\end{center}
\caption{The Petersen graph.}
\label{fig:Petersen}
\end{figure}


\end{page}

%%%%%%%%%% output/163--2-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:163}

\begin{dfn}
A graph $G' = (V', E')$ is called a \emph{subgraph} of graph $G = (V, E)$ if $V' \subset V$ and $E' \subset E$.
\end{dfn}

\end{page}

%%%%%%%%%% output/164--2-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:164}

Note that we cannot take any pair of subsets $V' \subset V, E' \subset E$:
if $\{v, w\} \in E'$, then we must have $v, w \in V'$.

An \emph{$n$-cycle} in a graph is a subgraph isomorphic to $C_n$.
Bipartite graphs contain no $3$-cycles, and more generally no cycles of odd length.


\end{page}

%%%%%%%%%% output/166--2-1-3-exercise.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:166}

\begin{exc}
Prove the converse: if a graph contains no cycles of odd length, then it is bipartite.
\end{exc}

\end{page}

%%%%%%%%%% output/167--2-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{8}
\label{portion:167}


A \emph{Hamiltonian cycle} in a graph is a cycle that contains all of its vertices.
It provides a closed path visiting all of its vertices exactly once
A graph is called \emph{Hamiltonian} if it has a Hamiltonian cycle.
The graph $K_{3,3}$ is Hamiltonian, the Petersen graph is not.




\end{page}

%%%%%%%%%% output/168--2-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{8}
\label{portion:168}

\subsection{Incidence and adjacency}
An edge is said to be \emph{incident} with a vertex if it contains this vertex.
In other words, the edge $\{v, w\}$ is incident with the vertices $v$ and $w$.

The \emph{degree} of a vertex $v$, denoted $\deg v$, is the number of edges incident with $v$.
A vertex of degree zero, that is without incident edges, is called an \emph{isolated} vertex.

In the complete graph $K_n$ all vertices have degree $n-1$.
In the cycle $C_n$, all vertices have degree $2$.


\end{page}

%%%%%%%%%% output/170--2-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{9}
\label{portion:170}

\begin{dfn}
A graph is called \emph{$k$-regular} if all of its vertices have degree~$k$.
\end{dfn}

\end{page}

%%%%%%%%%% output/171--2-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{9}
\label{portion:171}


% The sequence of degrees of all vertices of $G$ is called the degree sequence of $G$:
% \[
% (\deg v_1, \ldots, \deg v_n)
% \]
% Clearly, isomorphic graphs have the same degree sequences.
% The converse is false: $C_6$ and the union of two triangles both have degree sequences $(2,2,2,2,2,2)$,
% but are not isomorphic (one is connected, the other is not).


\end{page}

%%%%%%%%%% output/173--2-1-4-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:173}

\begin{thm}[Handshake lemma]
\label{thm:Handshake}
In every graph, the sum of all vertex degrees is twice the number of edges:
\[
\sum_{v \in V} \deg v = 2 |E|.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/174--2-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:174}

\begin{proof}
Count the vertex-edge incidences in two ways.
From the vertices viewpoint, every vertex $v$ is incident to $\deg v$ many edges.
Thus the number of incidences is the sum of the degrees of all vertices.
From the edges viewpoint, every edge is incident to two vertices.
Thus the number of incidences is twice the number of edges.
The theorem follows.
% Cut every edge in half and count the number of half-edges.
% On one hand, every vertex $v_i$ has $\deg v_i$ half-edges.
% On the other hand, the number of half-edges is twice the number of edges.
\end{proof}
The name ``handshake lemma'' suggests a reformulation of the above argument.
In a group of people, several handshakes take place. How to count the number of handshakes?
One way to do it is to ask every person how many handshakes it made and to add all these numbers.
Since every handshake is counted twice, we have to divide the result by two.


\end{page}

%%%%%%%%%% output/176--2-1-4-corollary.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:176}

\begin{cor}
In every graph, the number of vertices of odd degree is even.
\end{cor}

\end{page}

%%%%%%%%%% output/177--2-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:177}

Indeed, the sum of all vertex degrees must be even by Theorem \ref{thm:Handshake}.

Two vertices joined by an edge are called \emph{adjacent}.


\end{page}

%%%%%%%%%% output/179--2-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:179}

\begin{dfn}
Let $G = (V, E)$ be a graph on $n$ vertices.
Denote the vertices by $v_1, \ldots, v_n$ (in an arbitrary order).
The \emph{adjacency matrix} of $G$ (with respect to a given ordering of vertices) is an $n \times n$ matrix $A$ with the following entries
\[
a_{ij} =
\begin{cases}
1 &\text{if } \{v_i, v_j\} \in E\\
0 &\text{if } \{v_i, v_j\} \notin E.
\end{cases}
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/180--2-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:180}


Recall the matrix multiplication: for $n \times n$ matrices $A$ and $B$ we define their product $C = AB$ as
\[
c_{ij} = \sum_{l=1}^n a_{il} b_{lj}.
\]


\end{page}

%%%%%%%%%% output/182--2-1-4-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:182}

\begin{thm}
Let $G$ be a graph with vertex set $V = \{v_1, v_2, \ldots, v_n\}$ and let $A$ be its adjacency matrix.
Let $a^{(k)}_{ij}$ denote the element of the matrix $A^k$ at the position $(i,j)$.
Then $a^{(k)}_{ij}$ is the number of walks of length exactly $k$ from the vertex $v_i$ to the vertex $v_j$ in the graph $G$.
\end{thm}

\end{page}

%%%%%%%%%% output/183--2-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{13}
\label{portion:183}

\begin{proof}
Exercise.
% Induction on $k$. For $k=1$, a walk of length $1$ is just an edge, and the statement holds by definition of the adjacency matrix.
% 
% The induction step: assume the statement holds for $k-1$, prove it for $k$.
% Take any two vertices $v_i, v_j \in V$.
% A walk starts with the first step.
% Every walk of length $k$ from $v_i$ to $v_j$ consists of an edge from $v_i$ to some neighbor $v_l$
% followed by a walk of length $k-1$ from $v_l$ to $v_j$.
% Hence, in order to compute the number of walks of length $k$ from $v_i$ to $v_j$
% we have to sum the number of walks of length $k-1$ from all neighbors of $v_i$ to $v_j$.
% Since by the induction assumption there are $a^{(k-1)}_{lj}$ walks of length $k-1$ from $v_l$ to $v_j$, this sum is equal to
% \[
% \sum_{\{v_i,v_l\} \in E} a^{(k-1)}_{lj} = \sum_{l=1}^n a_{il} a^{(k-1)}_{lj},
% \]
% which is exactly the $(i,j)$-entry in the matrix $A \cdot A^{k-1} = A^k$.
\end{proof}

The reader is invited to define analogs of the adjacency matrix for directed and weighted graphs.



\end{page}

%%%%%%%%%% output/184--2-1-5-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{13}
\label{portion:184}

\subsection{Connectivity and components}

\end{page}

%%%%%%%%%% output/186--2-1-5-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{14}
\label{portion:186}

\begin{dfn}
A \emph{walk} in a graph $G = (V, E)$ is a sequence
\[
(v_0, e_1, v_1, e_2, \ldots, e_k, v_k),
\]
where $v_i \in V$, and for each $i = 1, \ldots, k$ we have $e_i = \{v_{i-1}, v_i\} \in E$.
\end{dfn}

\end{page}

%%%%%%%%%% output/189--2-1-5-remark.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{15}
\label{portion:189}

\begin{rem}
A walk can revisit vertices and go along an edge several times.
That is, one can have $v_i = v_j$ or $e_i = e_j$ for $i \ne j$.
\end{rem}

\end{page}

%%%%%%%%%% output/192--2-1-5-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{16}
\label{portion:192}

\begin{dfn}
\label{dfn:PathCycle}
A \emph{path} in a graph is a walk with distinct vertices: $v_i \ne v_j$ for $i \ne j$.
\end{dfn}

\end{page}

%%%%%%%%%% output/193--2-1-5-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{16}
\label{portion:193}

In other words, a \emph{path} in a graph $G$ is a subgraph of $G$ isomorphic to $P_n$ for some $n$.


\end{page}

%%%%%%%%%% output/195--2-1-5-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{17}
\label{portion:195}

\begin{dfn}
A graph $G$ is called \emph{connected} if for any two vertices $x, y \in V(G)$ there is a path between $x$ and $y$.
\end{dfn}

\end{page}

%%%%%%%%%% output/196--2-1-5-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{17}
\label{portion:196}


This is the same as to say that there is a walk between $x$ and $y$:
every path is a walk, and from every walk one can remove cycles so that it becomes a path.


\end{page}

%%%%%%%%%% output/198--2-1-5-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{18}
\label{portion:198}

\begin{dfn}
A \emph{component} of a graph $G$ is a maximal connected subgraphs of $G$.
\end{dfn}

\end{page}

%%%%%%%%%% output/200--2-1-6-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{6}
\setcounter{dfn}{18}
\label{portion:200}

\subsection{Eulerian graphs}
Euler's ``Seven bridges of K\"onigsberg'' problem.

Informally speaking, we are asking what graphs can be drawn without lifting the pencil from paper
(and drawing every edge only once).
You can try to draw the graph on Figure \ref{fig:TwoDrawings} in this way.


\end{page}

%%%%%%%%%% output/202--2-1-6-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{6}
\setcounter{dfn}{19}
\label{portion:202}

\begin{dfn}
A \emph{Euler walk} on a graph $G$ is a walk that takes each edge of $G$ exactly once.
A \emph{Eulerian circuit} is a closed walk that takes each edge exactly once.
\end{dfn}

\end{page}

%%%%%%%%%% output/203--2-1-6-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{6}
\setcounter{dfn}{19}
\label{portion:203}

In other words, a Eulerian circuit or Eulerian walk is an ordering of the edges of $G$
such that two consecutive edges share a vertex.

A graph is called \emph{Eulerian} if it has a Eulerian circuit and \emph{semi-Eulerian} if it has a Eulerian walk.


\end{page}

%%%%%%%%%% output/205--2-1-6-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{6}
\setcounter{dfn}{20}
\label{portion:205}

\begin{thm}
A graph is Eulerian if and only if all of its vertex degrees are even
and all vertices of positive degree belong to the same connected component.
A graph is semi-Eulerian if and only if it has exactly two vertices of odd degree
and all vertices of positive degree belong to the same connected component.
\end{thm}

\end{page}

%%%%%%%%%% output/206--2-1-6-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{0}
\setcounter{dfn}{20}
\label{portion:206}

The somewhat awkward condition ``all vertices of positive degree belong to the same connected component''
is equivalent to ``the graph becomes connected after deleting all isolated vertices''.
Another way to deal with the isolated vertices is to define the notion of a Eulerian circuit/walk differently:
it must visit all vertices.
Then a graph is (semi-)Eulerian if and only if it is connected and the degree evenness condition is satisfied.
\begin{proof}
The ``only if'' direction. Assume that the graph has a Eulerian walk or a Eulerian circuit.
Then there is a walk between any two vertices of positive degree:
one can use a piece of the walk or circuit to get from one to the other.
In order to prove that the degrees of all vertices (except two in the semi-Eulerian case) are even,
orient the edges in the direction of the walk: orient $e_i$ from $v_{i-1}$ to $v_i$.
We obtain a directed graph.
In a directed graph, every vertex $v$ has the \emph{in-degree} $\deg_+ v$ and the \emph{out-degree} $\deg_- v$:
the number of edges entering $v$ and the number of edges leaving $v$.
Clearly, $\deg v = \deg_+ v + \deg_- v$.
On the other hand, a Eulerian circuit enters and leaves every vertex equal number of times: $\deg_+ v = \deg_- v$.
This implies that $\deg v$ is even for every vertex of a Eulerian graph.

In a semi-Eulerian graph we have $\deg_+ v = \deg_- v$ for every intermediate vertex of the walk, but
\[
\deg_+ v_0 = \deg_- v_0 - 1, \quad \deg_+ v_m = \deg_- v_m + 1
\]
for the initial and the final vertices of the walk, respectively.
It follows that the degrees of all vertices except $v_0$ and $v_m$ are even.

The ``if'' direction.
First, consider the case when all vertex degrees are even.
Start to walk from any vertex without going along any edge twice.
At some point we must stop because all edges incident to the current vertex are used.
This can only happen at the initial vertex of our walk because if you stop at a different vertex, then its in-degree will be one bigger than the out-degree,
which contradicts the assumption that all vertex degrees are even.
Thus we obtain a circuit (which does not necessarily cover all edges).
Remove this circuit from the graph.
We obtain a possibly disconnected graph where all vertex degrees are even.
Repeat the procedure until the edge set of our graph will be partitioned into circuits.
Then start to merge the circuits: if two circuits have a common vertex, then they can be replaced by a single circuit.
If some circuit has no common vertices with the other circuits,
then its vertices and edges form a connected component of the graph,
and we will have at least two non-trivial connected components.
Thus all circuits can be merged to a circuit covering all edges of the graph.

If the graph has two vertices of odd degree, then start our first walk from one of these vertices.
This walk will necessarily stop at the other odd vertex (again, by consideration of the in- and out-degrees).
Removing this walk from the graph yields a graph with all vertices of even degree, and we proceed as in the previous case.
\end{proof}

Try this algorithm on the graph on Figure \ref{fig:EulerianGraph}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=.4\textwidth]{Fig/EulerianGraph.pdf}
\end{center}
\caption{Find a Eulerian circuit in this graph.}
\label{fig:EulerianGraph}
\end{figure}

Note that for graphs with two vertices of odd degree we have proved a bit more:
every Eulerian circuit starts in one of the odd vertices and ends in the other one.


% The skeleton of the cube is Hamiltonian but not Eulerian.
% The graph $K_{4,6}$ is Eulerian but not Hamiltonian (why?).





% Would be good to discuss depth-first search here. Connected components, existence of cycles, planarity testing.





\end{page}

%%%%%%%%%% output/207--2-2-0-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:207}

\section{Trees}

\end{page}

%%%%%%%%%% output/208--2-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:208}

\subsection{Basics}
Recall that a cycle in a graph is a subgraph isomorphic to $C_n$ for some $n$.
A graph without cycles is called \emph{acyclic}.

\end{page}

%%%%%%%%%% output/210--2-2-1-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:210}

\begin{dfn}
A \emph{tree} is a connected acyclic graph.
\end{dfn}

\end{page}

%%%%%%%%%% output/211--2-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:211}

Examples are shown in Figure \ref{fig:Trees}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{Trees.pdf}
\end{center}
\caption{Some trees.}
\label{fig:Trees}
\end{figure}

A disconnected acyclic graph is called a \emph{forest}.
Every connected component of a forest is a tree.
Indeed, a component is connected by definition; it has no cycles because, clearly, every subgraph of an acyclic graph is acyclic.


\end{page}

%%%%%%%%%% output/213--2-2-1-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:213}

\begin{thm}
In a tree, any two vertices are connected by exactly one path.
\end{thm}

\end{page}

%%%%%%%%%% output/214--2-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:214}

\begin{proof}
Take any two vertices of a tree.
Since a tree is connected, there is at least one path between these two vertices.
If there is more than one path, then this implies the existence of a cycle.
Namely, take the first vertex where the two paths diverge and the first vertex where they meet again;
the union of the segments of our paths between these vertices will be a cycle.
(We are not working out the details here.)
This contradicts the assumption that our graph is a tree, thus there cannot be more than one path between two vertices.
\end{proof}

A \emph{rooted} tree is a tree $T$ with a specified vertex $x$, called the \emph{root} of~$T$.
The edges of a tree can be equipped with orientation so that for every vertex $v$ the (unique) path from $x$ to $v$ always follows the directions of edges.
(Again, this looks intuitively clear, but requires a formal proof.)
See Figure~\ref{fig:RootedTreeOrient} for an example.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.6\textwidth]{RootedTreeOrient.pdf}
\end{center}
\caption{A canonically oriented rooted tree.}
\label{fig:RootedTreeOrient}
\end{figure}



\end{page}

%%%%%%%%%% output/215--2-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:215}

\subsection{Leaves}

\end{page}

%%%%%%%%%% output/217--2-2-2-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:217}

\begin{dfn}
A vertex of degree $1$ is called a \emph{leaf}.
\end{dfn}

\end{page}

%%%%%%%%%% output/218--2-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:218}


Find leaves of trees in Figure \ref{fig:Trees}.


\end{page}

%%%%%%%%%% output/220--2-2-2-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:220}

\begin{lem}
\label{lem:LeafTree}
Every tree has at least two leaves.
\end{lem}

\end{page}

%%%%%%%%%% output/221--2-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:221}

\begin{proof}
Take a path $P \subset T$ of maximum length.
(If there are several paths of maximum length, take one of them.)
I claim that its endpoints $v$ and $w$ are leaves.
Indeed, assume $\deg v > 1$.
Then there is an edge of $T$ incident with $v$ and not belonging to $P$.
If the other end of this edge is also a vertex of $P$, then $T$ contains a cycle, which contradicts to it being a tree.
If the other end is not in $P$, then adding it to $P$ we obtain a longer path.
This contradicts the choice of $P$.
Thus the assumption $\deg v > 1$ was false, $v$ is a leaf, and $w$ is a leaf as well.
\end{proof}


\end{page}

%%%%%%%%%% output/223--2-2-2-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:223}

\begin{dfn}
Let $G$ be a graph and $v$ a vertex of $G$.
Denote by $G - v$ the graph obtained by deleting the vertex $v$ and all edges incident to it.
This operation is called \emph{vertex deletion}.
\end{dfn}

\end{page}

%%%%%%%%%% output/226--2-2-2-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:226}

\begin{lem}
\label{lem:LeafDel}
Let $T$ be a tree and $v$ a leaf of $T$.
Then $T - v$ is also a tree.
\end{lem}

\end{page}

%%%%%%%%%% output/227--2-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:227}

\begin{proof}
The graph $T - v$ is acyclic, because it is a subgraph of an acyclic graph.
It remains to prove that $T - v$ is connected.
Let $x$ and $y$ be two vertices of $T$ different from $v$.
There is a path in $T$ connecting $x$ and $y$.
This path does not contain $v$, because otherwise the degree of $v$ would be at least $2$.
Thus it can be considered as a path in $T - v$, so $x$ and $y$ are connected within $T - v$.
\end{proof}



\end{page}

%%%%%%%%%% output/228--2-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:228}

\subsection{The number of edges in a tree}
Lemma \ref{lem:LeafDel} allows to use induction when proving theorems about trees.


\end{page}

%%%%%%%%%% output/230--2-2-3-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:230}

\begin{thm}
\label{thm:TreeEdges}
If a graph $T = (V, E)$ is a tree, then $|E| = |V| - 1$.
\end{thm}

\end{page}

%%%%%%%%%% output/231--2-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:231}

\begin{proof}
Let $|V| = n$.
Use induction on $n$.
For $n=1$, there is only one tree with one vertex.
It has no edges, which proves the induction base.

Now let us prove the induction step: if every tree with $n$ vertices has $n-1$ edges, then every tree with $n+1$ vertices has $n$ edges.
Take any tree $T$ with $n+1$ vertices.
By Lemma \ref{lem:LeafTree} it has a leaf $v$.
By Lemma \ref{lem:LeafDel} the graph $T - v$ is also a tree.
The tree $T - v$ has $n$ vertices, therefore by the induction assumption it has $n-1$ edges.
But then $T$ has $n$ edges, and the induction step is proved.
\end{proof}


\end{page}

%%%%%%%%%% output/233--2-2-3-corollary.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:233}

\begin{cor}
\label{cor:ForestEdges}
A forest with $n$ vertices and $k$ components has $n-k$ edges.
\end{cor}

\end{page}

%%%%%%%%%% output/234--2-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:234}

\begin{proof}
Let the components have $n_1, \ldots, n_k$ vertices.
By the above theorem, they have $n_1 - 1, \ldots, n_k - 1$ edges, respectively.
By summing up the number of edges we obtain the desired result.
\end{proof}

% Also, Theorem \ref{thm:TreeEdges} implies yet another characterization of trees.
% 

\end{page}

%%%%%%%%%% output/236--2-2-3-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:236}

% \begin{thm}
% \label{thm:Tree3}
% A graph is a tree if and only if it has no cycles and its number of edges is one less than the number of vertices.
% \end{thm}

\end{page}

%%%%%%%%%% output/237--2-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{9}
\label{portion:237}

% \begin{proof}
% It suffices to prove that a graph with $n$ vertices and without cycles is connected if and only if it has $n-1$ edges.
% A graph without cycles is a forest,
% and by Corollary \ref{cor:ForestEdges} its number of edges is the number of vertices minus the number of components.
% A graph is connected if and only if it has just one component.
% This implies the statement.
% \end{proof}



\end{page}

%%%%%%%%%% output/238--2-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{9}
\label{portion:238}

\subsection{Spanning trees}

\end{page}

%%%%%%%%%% output/240--2-2-4-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:240}

\begin{dfn}
Let $G = (V, E)$ be a graph.
A subgraph of a graph $G$ is called a \emph{spanning tree} if it is a tree with the same vertex set as $G$.
\end{dfn}

\end{page}

%%%%%%%%%% output/241--2-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:241}


Figure \ref{fig:SpanningTrees} shows examples of spanning trees.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{SpanningTrees.pdf}
\end{center}
\caption{Examples of spanning trees.}
\label{fig:SpanningTrees}
\end{figure}


\end{page}

%%%%%%%%%% output/243--2-2-4-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:243}

\begin{thm}
\label{thm:SpanTree}
Every connected graph has a spanning tree.
\end{thm}

\end{page}

%%%%%%%%%% output/244--2-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:244}

We will find a spanning tree by deleting edges from the graph one by one while taking care that the graph remains connected.
For any graph $G = (V, E)$ and any its edge $e \in E$ denote by $G - e$ the graph $(V, E \setminus \{e\})$.
(Note that we are not removing any vertices, even if after deletion of $e$ an isolated vertex appears.)
This is the operation of \emph{edge deletion}.
\begin{proof}
Let $G$ be a connected graph.
If $G$ contains a cycle $C$, then let $e$ be any edge of $C$.
I claim that the graph $G - e$ is connected.
Indeed, let $v, w \in V$ be any two vertices of $G$.
Since $G$ is connected, there is a path in $G$ between $v$ and $w$.
If this path never uses the edge $e$, then this is also a path in $G - e$.
If it does use $e$, then instead going on $e$, take a detour via the path $C - e$.
This produces a walk in $G - e$ from $v$ to $w$.
A walk can be transformed into a path by removing cycles.

Thus $G - e$ is connected.
If it is acyclic, then it is a spanning tree.
Otherwise repeat the operation: take another cycle and remove an edge from it etc.
until we arrive at an acyclic connected subgraph with the same vertex set as $G$.
\end{proof}

The following theorem is a strengthening of Theorem \ref{thm:TreeEdges}.

\end{page}

%%%%%%%%%% output/246--2-2-4-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:246}

\begin{thm}
Let $G = (V, E)$ be a graph.
If $|E| > |V| - 1$, then $G$ contains a cycle.
If $|E| < |V| - 1$, then $G$ is not connected.
\end{thm}

\end{page}

%%%%%%%%%% output/247--2-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{12}
\label{portion:247}

\begin{proof}
Both statements are proved by contraposition.
That is, we prove the following: if $G$ contains no cycle, then $|E| \le |V| - 1$; if $G$ is connected, then $|E| \ge |V| - 1$.

Let $G$ be acyclic.
Then by Corollary \ref{cor:ForestEdges} it has $|V| - k \le |V| - 1$ edges, where $k$ is the number of components of $G$.

Let $G$ be connected.
By Theorem \ref{thm:SpanTree}, $G$ has a spanning tree which, by Theorem \ref{thm:TreeEdges}, has $|V| - 1$ edges.
Thus $G$ has at least $|V| - 1$ edges.
\end{proof}





\end{page}

%%%%%%%%%% output/248--2-2-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{12}
\label{portion:248}

\subsection{The number of spanning trees}
The number of spanning trees of a given graph is an interesting combinatorial problem.


\end{page}

%%%%%%%%%% output/250--2-2-5-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{13}
\label{portion:250}

\begin{thm}[Borchardt, Cayley]
The complete graph on $n$ vertices has $n^{n-2}$ spanning trees.
\end{thm}

\end{page}

%%%%%%%%%% output/251--2-2-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{13}
\label{portion:251}

For example, $K_3$ has $3$ spanning trees (obtained by deleting one arbitrary edge),
$K_4$ has $16$, and $K_5$ already $125$ different spanning trees.

In order to count the spanning trees in an arbitrary graph, the following matrix is needed.

\end{page}

%%%%%%%%%% output/253--2-2-5-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{14}
\label{portion:253}

\begin{dfn}
The \emph{Laplacian matrix} of a graph $G$ is
\[
L = D - A,
\]
where $D$ is the degree matrix, that is a diagonal matrix with $d_{ii} = \deg v_i$, and $A$ is the adjacency matrix of $G$.
\end{dfn}

\end{page}

%%%%%%%%%% output/254--2-2-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{14}
\label{portion:254}


The matrix $L$ has zero determinant (because the vector $(1, 1, \ldots, 1)$ belongs to its kernel).
Moreover, its rank equals $n-k$, where $n = |V|$ and $k$ is the number of connected components of $G$.
In particular, if $G$ is connected, then $L$ contains an $(n-1) \times (n-1)$ minor with non-zero determinant.
In fact, $\det L = 0$ implies that all cofactors $(-1)^{i+j} \det L_{ij}$ are equal (you can try to prove this).
This common value is the number of the spanning trees.


\end{page}

%%%%%%%%%% output/256--2-2-5-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{15}
\label{portion:256}

\begin{thm}[Kirchhoff]
Let $G$ be a connected graph, and $L$ be its Laplacian matrix.
Then the number of spanning trees of $G$ is equal to each of the following numbers.
\begin{itemize}
\item $(-1)^{i+j} \det L_{ij}$, where $L_{ij}$ is the matrix obtained by removing the $i$-th row and the $j$-th column from $L$;
\item $\frac{1}{n} \lambda_1 \lambda_2 \cdots \lambda_{n-1}$, where $\lambda_i$ are the non-zero eigenvalues of $L$.
\end{itemize}
\end{thm}

\end{page}

%%%%%%%%%% output/257--2-2-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{15}
\label{portion:257}


In particular, for the complete graph we have the determinant of the following $(n-1) \times (n-1)$ matrix:
\[
\det
\begin{pmatrix}
n-1 & -1 & \cdots & -1\\
-1 & n-1 & \cdots & -1\\
\vdots & \vdots & \ddots & \vdots\\
-1 & -1 & \cdots & n-1
\end{pmatrix}.
\]
Cayley's theorem indicates that it is equal to $n^{n-2}$.
Of course, it is easier (but still not straightforward) to prove this equality without invoking two difficult theorems above.




\end{page}

%%%%%%%%%% output/258--2-2-6-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{15}
\label{portion:258}

\subsection{Minimum spanning tree: Kruskal's algorithm}
Every spanning tree in a connected graph on $n$ vertices has $n-1$ edges.
However, if edges are equipped with weights, then we can speak about the spanning tree of minimum total weight.
For example, if the weights of edges are the costs of building railroads between towns,
then the minimum spanning tree is the cheapest railroad network connecting all towns.


\end{page}

%%%%%%%%%% output/260--2-2-6-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{16}
\label{portion:260}

\begin{dfn}
A \emph{weighted graph} is a pair $(G, \omega)$, where $G$ is a usual graph, and
\[
\omega \colon E(G) \to \R
\]
is a map. The number $\omega(e)$ is called the \emph{weight} of the edge $e$.
\end{dfn}

\end{page}

%%%%%%%%%% output/263--2-2-6-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{17}
\label{portion:263}

\begin{dfn}
A \emph{minimum spanning tree} of a weighted connected graph $(G, \omega)$ is a spanning tree $(V, E')$ of $G$ such that
the sum $\sum_{e \in E'} \omega(e)$ has the minimum possible value among all spanning trees of $G$.
\end{dfn}

\end{page}

%%%%%%%%%% output/264--2-2-6-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{17}
\label{portion:264}

A minimum spanning tree is not necessarily unique: if all weights are the same, then all spanning trees have the same total weight.


\end{page}

%%%%%%%%%% output/266--2-2-6-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{18}
\label{portion:266}

\begin{thm}[Kruskal's algorithm]
Order the edges of a weighted connected graph according to their weights:
\[
(e_1, \ldots, e_m) \quad \text{such that} \quad \omega(e_1) \le \cdots \le \omega(e_m).
\]
Going through this list, mark an edge if it does not create a cycle together with the previously marked edges.
More exactly, put $G_0 = (V, \emptyset)$: the graph with isolated vertices only.
If $G_i$ is already defined, then put
\[
G_{i+1} =
\begin{cases}
G_i + e_{i+1} &\text{ if } G_i + e_{i+1} \text{ has no cycles}\\
G_i &\text{ otherwise.}
\end{cases}
\]
The output of the algorithm is the graph $G_m$.
\end{thm}

\end{page}

%%%%%%%%%% output/267--2-2-6-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{18}
\label{portion:267}

This is a greedy algorithm: at each step we do what seems to us the best, we take the lightest edge.
Of course, a ``locally optimal'' procedure does not always lead to a ``globally optimal'' result.
We must prove that Kruskal's algorithm outputs a minimum spanning tree.
But first of all, we must show that the output is a tree at all.

\begin{proof}[Proof that the output is a tree]
By construction, every graph $G_i$, and in particular $G_m$, is acyclic.
Let us show that $G_m$ is connected.
Assume the converse.
Take any two connected components of $G_m$.
Since $(V, E)$ is connected, there is an edge $e_i \in E$ joining two vertices $v$ and $w$ from these components.
This $e_i$ does not belong to $G_m$.
Thus at the $i$-th step of the algorithm, when we were deciding to take $e_i$ or not,
this edge created a cycle with edges of $G_{i-1}$.
This means that in $G_{i-1}$ (and hence in $G_m$) there is a path from $v$ to $w$.
But then $v$ and $w$ are in the same connected component of $G_m$, which is a contradiction.
\end{proof}

\begin{proof}[Proof that the tree is minimal]
We prove by induction on $i$ that every graph $G_i$ is contained in some minimum spanning tree.
For $i=m$ this will tell us that $G_m$ is a minimum spanning tree.

As the induction base take $i=0$.
Here the assertion is trivially true, because there exists at least one minimum spanning tree.

For the induction step, assume that $G_i$ is a subgraph of a minimum spanning tree $T_i$.
Consider the next edge $e_{i+1}$.
If $G_{i+1} = G_i$ (which happens if $e_{i+1}$ creates a cycle), then $G_{i+1}$ is a subgraph of $T_i$, and we are good.
If $G_{i+1} = G_i + e_{i+1}$ and $e_{i+1}$ is an edge of $T_i$, then $G_{i+1}$ is a subgraph of $T_i$ as well.

The only non-trivial case is when $G_{i+1} = G_i + e_{i+1}$ and $e_{i+1}$ is not an edge of $T_i$.
Then the graph $T_i + e_{i+1}$ contains a cycle,
which is formed by the edge $e_{i+1}$ and the path $P$ in $T_i$ connecting the endpoints of $e_{i+1}$.
There is an edge $f$ of $P$ that does not belong to $G_i$: if this is not the case, then $G_{i+1}$ contains a cycle.
The graph $T_i + e_{i+1} - f$ is a tree: it is connected and has $|V|-1$ edges.
We claim that this tree is also a minimum tree.
For this one has to show that $\omega(f) = \omega(e_{i+1})$.

Assume that $\omega(f) < \omega(e_{i+1})$, then the edge $f$ appears on the list of all edges earlier than $e_{i+1}$.
But why did not we add $f$ to the graph that we are constructing?
This can only be because $f$ would create a cycle.
But then $f$ also creates a cycle when added to $G_i$.
Since $G_i + f \subset T_i$, it follows that $T_i$ contains a cycle, which is a contradiction.

Thus $\omega(f) \ge \omega(e_{i+1})$.
It is not possible that $\omega(f) > \omega(e_{i+1})$, because then the tree $T_i + e_{i+1} - f$ has smaller weight than $T_i$,
which by assumption is a minimum spanning tree.
Thus we have $\omega(f) = \omega(e_{i+1})$.
But then $G_{i+1}$ is a subgraph of the minimum spanning tree $T_i + e_{i+1} - f$, so the induction step is proved.
\end{proof}



\end{page}

%%%%%%%%%% output/269--2-2-6-remark.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{19}
\label{portion:269}

\begin{rem}
There are other algorithms that find a minimum spanning tree, for example Prim's algorithm.
Closely related to Kruskal's algorithm is the \emph{reverse-delete algorithm}.
Here we order the edges in the nonincreasing order of weights, go through the list
and delete an edge if this does not disconnect the graph.
\end{rem}

\end{page}

%%%%%%%%%% output/272--2-2-6-remark.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{20}
\label{portion:272}

\begin{rem}
Another problem for a connected weighted graph is to find a minimum path between two given vertices.
This can be done with Dijkstra's algorithm.
Note that a minimum spanning tree contains a path between any pair of vertices, but this path is not necessarily minimal.
\end{rem}

\end{page}

%%%%%%%%%% output/274--2-3-0-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:274}

\section{Planar graphs}

\end{page}

%%%%%%%%%% output/275--2-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:275}

\subsection{Basics}

\end{page}

%%%%%%%%%% output/277--2-3-1-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:277}

\begin{dfn}
A graph is called \emph{planar} if it can be drawn in the plane in such a way that its edges do not cross each other.
A drawing with pairwise non-crossing edges is called a \emph{planar embedding} of the graph or, for short, a \emph{plane graph}.
\end{dfn}

\end{page}

%%%%%%%%%% output/278--2-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:278}


A graph can have ``different'' embeddings in the plane, see Figure~\ref{fig:TwoEmbeddings}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{TwoEmbeddings.pdf}
\end{center}
\caption{Two different embeddings of the same graph.}
\label{fig:TwoEmbeddings}
\end{figure}

Two embeddings are called \emph{non-isotopic} if one cannot be deformed into the other continuously
while remaining an embedding during the deformation.


\end{page}

%%%%%%%%%% output/280--2-3-1-remark.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:280}

\begin{rem}
Let us stress the difference between ``planar graph'' and ``plane graph''.
A planar graph is an abstract graph that \emph{can be} drawn in the plane.
A plane graph is a graph that \emph{is} already drawn in the plane.
The reason for distinguishing between plane and planar is that different drawings can be possible.
As the above example shows there are different (non-isotopic) plane graphs which represent the same (isomorphic) planar graphs.
\end{rem}

\end{page}

%%%%%%%%%% output/283--2-3-1-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:283}

\begin{thm}
The graph $K_5$ is not planar.
\end{thm}

\end{page}

%%%%%%%%%% output/284--2-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:284}

\begin{proof}[Sketch of proof]
Let $v_1, \ldots, v_5$ be the vertices of $K_5$.
We denote by the same letters their images in the plane.
The vertices $v_1, v_2, v_3, v_4$ and the edges between them form a planar embedding of $K_4$.
Up to relabeling of vertices and isotopy there is a unique embedding of $K_4$ in the plane, see Figure \ref{fig:K4Embedding}, left.
The graph $K_4$ separates the plane into four regions.
The fifth vertex $v_5$ must lie in one of these regions.
In whatever region it lies, it will be separated from one of the first four vertices.
For example, if it lies in the outer region,
then the edge $v_5v_1$ must intersect the contour $v_2v_3v_4$ at least once, see Figure \ref{fig:K4Embedding}, right.
\end{proof}

\begin{figure}[ht]
\begin{center}
\input{Fig/K4Embedding.pdf_t}
\end{center}
\caption{Proof of the non-planarity of $K_5$.}
\label{fig:K4Embedding}
\end{figure}

The above is only a sketch of the proof, because the assertions
``there is a unique embedding of $K_4$'' and ``the edge must intersect the contour'' require formal proofs.
They follow from the Jordan curve theorem: any embedded closed curve in the plane separates the plane in two connected components.
This might seem trivial but you should take into account that there are curves which are neither smooth nor polygonal
(maybe you have heard about fractals).

On the other hand, there is the following theorem (whose proof relies on the Jordan curve theorem).

\end{page}

%%%%%%%%%% output/286--2-3-1-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:286}

\begin{thm}[F\'ary]
If a graph can be drawn in the plane, then it can also be drawn with straight edges.
\end{thm}

\end{page}

%%%%%%%%%% output/287--2-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:287}

Thus we may always imagine our graphs being drawn with straight edges and rely on our geometric intuition.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{Straightening.pdf}
\end{center}
\caption{Any planar graph can be drawn with straight edges.}
\label{fig:Straightening}
\end{figure}



\end{page}

%%%%%%%%%% output/288--2-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:288}

\subsection{Euler's formula}
A plane graph separates the plane into regions, called \emph{faces}.
Every embedding has one unbounded face, called the \emph{outer face}.

The graphs on Figure \ref{fig:TwoEmbeddings} have two faces each.
If the graph has $n$ vertices and no edges, then its embedding has only one face: the plane punctured at $n$ points.

A face is called incident with an edge if the edge belongs to the boundary of the face.

\end{page}

%%%%%%%%%% output/290--2-3-2-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:290}

\begin{dfn}
The \emph{degree} of a face of a plane graph is the number of edges incident with it.
If on both sides of an edge lies the same face, then this edge is counted twice when calculating the degree of the face.
\end{dfn}

\end{page}

%%%%%%%%%% output/291--2-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:291}

For example, on Figure \ref{fig:TwoEmbeddings}, left, the outer face has degree 7 and the inner face has degree 3.
On Figure \ref{fig:TwoEmbeddings}, right, both the inner and the outer face have degrees 5.


\end{page}

%%%%%%%%%% output/293--2-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:293}

\begin{thm}
\label{thm:DualHandshake}
For every plane graph the sum of the degrees of its faces is twice the number of edges:
\[
\sum_{f \in F} \deg f = 2 |E|.
\]
(Here $F$ denotes the set of all faces.)
\end{thm}

\end{page}

%%%%%%%%%% output/294--2-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:294}

\begin{proof}
Count the face-edge incidences in two ways:
from the viewpoint of the faces and from the viewpoint of the edges.
\end{proof}
Observe that in the previous theorem the graph may be disconnected; it may have no edges at all.


\end{page}

%%%%%%%%%% output/296--2-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:296}

\begin{thm}[Euler]
Let $G = (V, E)$ be a connected plane graph.
Denote by $F$ the set of its faces.
Then we have
\[
|V| - |E| + |F| = 2.
\]
In particular, the number of faces does not depend on the choice of an embedding.
\end{thm}

\end{page}

%%%%%%%%%% output/299--2-3-2-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{8}
\label{portion:299}

\begin{exl}
The graphs of platonic solids (and more generally, of all convex polytopes) are planar.
Thus the Euler formula holds for the numbers of vertices, edges, and faces of any convex polytope.
For example, the cube has $8$ vertices, $12$ edges, and $6$ faces, and we have indeed $8-12+6=2$.
\end{exl}

\end{page}

%%%%%%%%%% output/300--2-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{8}
\label{portion:300}


\begin{proof}
Induction on the number of edges.

Let $|E| = 0$. The only connected graph without edges is the graph with one vertex.
It has one face, and we have $1 - 0 + 1 = 2$. The induction base is proved.

Now take a graph with $n$ edges, $n \ge 1$.
Consider two cases.

\noindent 1) The graph is acyclic.
Then it is a tree. A tree does not separate the plane, so we have $|F| = 1$ in this case.
By Theorem \ref{thm:TreeEdges}, $|E| = |V|-1$.
Thus we have
\[
|V| - |E| + |F| = |V| - (|V|-1) + 1 = 2.
\]

\noindent 2) The graph contains a cycle.
Let $C \subset G$ be any cycle and let $e$ be any edge of $C$.
The graph $G - e$ is still connected and has one edge less.
Let us show that it also has one face less.
Indeed, the points on different sides of $e$ belong to different faces of $G$:
any arc connecting them must intersect the cycle $C$.
These two faces are merged to one face in $G - e$; all other faces are unchanged.
Thus $G$ has the same number of vertices, one edge less, and one face less than $G - e$.
By the induction assumption, Euler's formula holds for $G - e$.
Thus it also holds for $G$.
\end{proof}



\end{page}

%%%%%%%%%% output/302--2-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{9}
\label{portion:302}

\begin{thm}
\label{thm:3V-6}
Let $G$ be a connected plane graph with $n \ge 3$ vertices.
Then $|E| \le 3|V| - 6$.
Moreover, equality holds if and only if all faces of $G$ are triangles.
\end{thm}

\end{page}

%%%%%%%%%% output/303--2-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:303}

\begin{proof}
Observe that in a plane graph with $\ge 3$ vertices the degree of every face is at least $3$.
Indeed, the only way for a face of a connected graph to have degree $2$ is to enclose an edge, in which case the graph has two vertices and one edge.
A face cannot have degree $1$. And if a face of a connected graph has degree $0$, then the graph consists of a single vertex.

Then from Theorem \ref{thm:DualHandshake} and Euler's formula we get the inequality
\[
2 |E| = \sum_{f \in F} \deg f \ge 3 |F| = 3 (2 - |V| + |E|),
\]
which implies $|E| \le 3|V| - 6$.
The equality takes place only if the $\deg f = 3$ for all $f$, that is if all faces are triangles.
\end{proof}

Theorem \ref{thm:3V-6} implies that the graph $K_5$ is not planar.
Indeed, it has $5$ vertices and $10 > 3\cdot 5 - 6$ edges.
This proof of non-planarity of $K_5$ looks very nice and seems to avoid the intricacies of Jordan's curve theorem.
The simplicity is deceiving: Jordan's curve theorem is needed in the proof of Euler's formula (when we say that the cycle separates the plane).

An attempt to prove the non-planarity of $K_{3,3}$ in the same way fails:
this graph has $6$ vertices and $9 \le 3 \cdot 6 - 6$ edges.
Note however that a planar embedding of $K_{3,3}$ (if it exists) has no faces of degree $3$ (a bipartite graph contains no odd cycles).
For any plane graph without triangles we have
\[
2 |E| = \sum_{f \in F} \deg f \ge 4 |F| = 4 (2 - |V| + |E|),
\]
which implies $|E| \le 2|V| - 4$.
Since $K_{3,3}$ does not satisfy this inequality: $9 > 2 \cdot 6 - 4$, it is not planar.





\end{page}

%%%%%%%%%% output/304--2-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:304}

\subsection{Planarity criteria}

\end{page}

%%%%%%%%%% output/306--2-3-3-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:306}

\begin{thm}[Kuratowski]
A graph is planar if and only if it does not contain a subgraph isomorphic to a subdivision of $K_5$ or $K_{3,3}$.
\end{thm}

\end{page}

%%%%%%%%%% output/307--2-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:307}


A graph $G'$ is a \emph{subdivision} of a graph $G$ if $G'$ is obtained from $G$ by repeated \emph{edge subdivisions}.
To subdivide an edge $e = \{v,w\}$ of a graph $G$ means to introduce a new vertex $x$, delete $e$, and introduce two new edges $\{x,v\}$ and $\{x,w\}$.
Figure \ref{fig:KSubdivisions} shows a subdivision of $K_5$ and a subdivision of $K_{3,3}$.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{KSubdivisions.pdf}
\end{center}
\caption{Some subdivisions of $K_5$ and $K_{3,3}$.}
\label{fig:KSubdivisions}
\end{figure}

One direction of the Kuratowski theorem is easy to prove: If a graph contains a subdivision of $K_5$ or $K_{3,3}$, then it cannot be planar.
Indeed, an embedding of the graph would contain an embedding of a subdivision of $K_5$ or $K_{3,3}$,
and hence an embedding of $K_5$ or $K_{3,3}$.
It is the opposite direction which is the most interesting and non-obvious:
the only obstacles to existence of a planar embedding of $G$ are graphs $K_5$ or $K_{3,3}$ contained in $G$ (in the form of subdivisions).


There is a similar planarity criterion that uses the notion of a \emph{minor}.


\end{page}

%%%%%%%%%% output/309--2-3-3-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{11}
\label{portion:309}

\begin{thm}[Wagner]
A graph is planar if and only if it does not have a minor isomorphic to $K_5$ or $K_{3,3}$.
\end{thm}

\end{page}

%%%%%%%%%% output/310--2-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:310}


A minor of a graph $G$ is any graph obtained by repeated vertex deletions, edge deletions and edge contractions.
See Figure \ref{fig:K5Contraction} for an example of an edge contraction.
If an edge contraction results in a multiple edge, then we replace a multiple edge by a simple edge.
If it results in a loop, then we remove a loop.

\begin{figure}[ht]
\begin{center}
\input{Fig/K5Contraction.pdf_t}
\end{center}
\caption{An example of edge contraction.}
\label{fig:K5Contraction}
\end{figure}

Note that a minor of $G$ is not necessarily isomorphic to a subgraph of $G$.
For example, the cycle $C_3$ is a minor of $C_4$ but not its subgraph.

Similarly to the Kuratowski theorem, one direction of the Wagner theorem is easy to prove, but not the other.




\end{page}

%%%%%%%%%% output/311--2-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:311}

\subsection{Duality for embedded graphs}
Let $G$ be a connected plane graph.
Define a new plane graph $G^*$ as follows.
Inside every face $f$ of $G$ choose a point $f^*$.
For every edge $e$ of $G$ draw an arc $e^*$ crossing the edge $e$ and joining the points $f_1^*$ and $f_2^*$ inside the faces incident with $e$.
(If $e$ is incident to one face only, then the arc $e^*$ is a loop.)

It is possible to draw all arcs $e^*$ so that they do not intersect each other.
(Mark a point in the interior of every edge;
inside every face $f$, join the point $f^*$ to the points marked on the incident edges in a non-self-intersecting way.)

See Figure \ref{fig:DualGraph} for an example.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.6\textwidth]{DualGraph.pdf}
\end{center}
\caption{A graph and its dual.}
\label{fig:DualGraph}
\end{figure}

Different planar embeddings of the same graph may have different duals.
For example, consider the duals of the graphs on Figure \ref{fig:TwoEmbeddings}.

Let us describe those graphs whose duals have no loops and multiple edges.


\end{page}

%%%%%%%%%% output/313--2-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:313}

\begin{dfn}
A graph is called \emph{$k$-edge connected} if one needs to delete at least $k$ edges in order to disconnect the graph.
\end{dfn}

\end{page}

%%%%%%%%%% output/314--2-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:314}

Thus a graph is $1$-edge connected graphs if and only if it is connected.
A connected graph is not $2$-edge connected if it has a \emph{bridge} or \emph{cut edge}: an edge whose deletion disconnects the graph.


\end{page}

%%%%%%%%%% output/316--2-3-4-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:316}

\begin{lem}
The dual of a plane graph has no loops if and only if the graph is $2$-connected.
The dual of a plane graph has no multiple edges if and only if the graph is $3$-connected.
\end{lem}

\end{page}

%%%%%%%%%% output/317--2-3-4-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{dfn}{13}
\label{portion:317}


The faces of the dual graph $G^*$ correspond to the vertices of $G$:
the duals of the edges incident to $v \in V(G)$ form a cycle around $v$.
Thus we have bijections
\[
V(G) \mapsto F(G^*), \quad E(G) \mapsto E(G^*), \quad F(G) \mapsto V(G^*).
\]

Observe that Theorem \ref{thm:DualHandshake} is equivalent to the handshake lemma for the dual graph $G^*$.

Finally, note that the dual of $G^*$ is $G$ again:
\[
(G^*)^* = G.
\]

\bigskip

For more information on planar graphs, see \cite[Chapter 10]{BM}.





\end{page}

%%%%%%%%%% output/318--2-4-0-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:318}

\section{Matchings}

\end{page}

%%%%%%%%%% output/319--2-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:319}

\subsection{Basics}

\end{page}

%%%%%%%%%% output/321--2-4-1-definition.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:321}

\begin{dfn}
Let $G = (V, E)$ be a graph.
A set of its edges $M \subset E$ is called a \emph{matching} if no two edges in $M$ have a common vertex.
If a vertex $v$ belongs to an edge of $M$, then $v$ is called \emph{matched}, otherwise \emph{unmatched}.
A matching is called \emph{perfect} if all vertices of $G$ are matched.
\end{dfn}

\end{page}

%%%%%%%%%% output/322--2-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:322}


A graph that has at least one perfect matching is called \emph{matchable}.
A matchable graph must have an even number of vertices.
(More generally, every connected component of a matchable graph must have an even number of vertices.)
But there are also connected unmatchable graphs with an even number of vertices, see Figure \ref{fig:PerfMatch}, right.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.7\textwidth]{PerfMatch.pdf}
\end{center}
\caption{Two connected graphs on six vertices: one matchable (matching is shown by thick edges) and one non-matchable.}
\label{fig:PerfMatch}
\end{figure}

If it is not possible to match all vertices, then one can ask for a maximum possible matching.
There are two notions of maximality.

\end{page}

%%%%%%%%%% output/324--2-4-1-definition.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:324}

\begin{dfn}
A matching $M$ in a graph $G$ is called a \emph{maximum matching} if it has the maximum possible number of edges among all matchings in $G$
(in other words if it covers a maximum possible number of vertices).

A matching is called \emph{inclusion-maximal} if it is not contained in any larger matching.
\end{dfn}

\end{page}

%%%%%%%%%% output/325--2-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:325}


A maximum matching is obviously inclusion-maximal, but not vice versa.
See Figure \ref{fig:MaxNotMax} for two examples.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.7\textwidth]{MaxNotMax.pdf}
\end{center}
\caption{Two inclusion-maximal but not maximum matchings.}
\label{fig:MaxNotMax}
\end{figure}

It follows that the greedy algorithm (add edges as long as it is possible) does not always find a maximum matching.




\end{page}

%%%%%%%%%% output/326--2-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:326}

\subsection{Augmenting paths and maximum matchings}
Assume that $M$ is a non-maximum matching in a graph.
We want to modify it so that to obtain a matching with more edges.
By the previous section, it is not always possible just to add edges to $M$, we also have to remove some.


\end{page}

%%%%%%%%%% output/328--2-4-2-definition.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:328}

\begin{dfn}
Let $M$ be a matching in $G$.
An \emph{$M$-alternating path} in $G$ is a path whose edges are alternately in $M$ and not in $M$.
\end{dfn}

\end{page}

%%%%%%%%%% output/329--2-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:329}


Figure \ref{fig:AltPath} shows possible types of alternating paths.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.6\textwidth]{AltPath.pdf}
\end{center}
\caption{Alternating paths.}
\label{fig:AltPath}
\end{figure}


\end{page}

%%%%%%%%%% output/331--2-4-2-definition.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:331}

\begin{dfn}
An \emph{$M$-augmenting path} is an $M$-alternating path that starts and ends with unmatched vertices.
\end{dfn}

\end{page}

%%%%%%%%%% output/332--2-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:332}


It follows that an $M$-augmenting path can look only as the bottom path on Figure \ref{fig:AltPath}.
In addition, there must be no edge of $M$ incident to the initial and terminal vertices of the path.
See Figure \ref{fig:AugmPathDef}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.6\textwidth]{AugmPathDef.pdf}
\end{center}
\caption{An augmenting path.}
\label{fig:AugmPathDef}
\end{figure}

An $M$-augmenting path can be used to modify $M$ to a bigger matching by ``switching'' the edges along the path.
This is illustrated in Figure \ref{fig:AugmPath}.


\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{AugmPath1.pdf}
\end{center}
\caption{Modifying a matching with the help of an augmenting path.}
\label{fig:AugmPath}
\end{figure}


The following theorem provides a basis to an algorithm for finding a maximum matching.


\end{page}

%%%%%%%%%% output/334--2-4-2-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:334}

\begin{thm}[Berge]
A matching $M$ in $G$ is a maximum matching if and only if $G$ contains no $M$-augmenting path.
\end{thm}

\end{page}

%%%%%%%%%% output/335--2-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:335}

\begin{proof}
Let us prove that if $M$ is a maximum matching, then there is no $M$-augmenting path.
By contraposition we have to show that if $G$ contains an $M$-augmenting path, then $M$ is not a maximum matching.
This follows from the observation we made before stating the theorem:
an augmenting path can be used to increase the number of edges in a matching.

For the opposite direction we have to show that if $M$ is not a maximum matching, then there is an $M$-augmenting path.
Let $M'$ be a maximum matching in $G$.
Then $|M'| > |M|$.
Let $H$ be the graph formed by those edges that belong to exactly one of $M$ and $M'$: the edge set of $H$ is
\[
(M \setminus M') \cup (M \setminus M').
\]
(This is called symmetric difference of $M$ and $M'$.)
Every vertex of $H$ has degree $1$ or $2$ because it is incident to at most one edge from $M$ and at most one edge from $M'$.
Therefore each connected component of $H$ is either an even cycle with edges alternately in $M$ and $M'$ or a path with edges alternately in $M$ and $M'$,
see Figure \ref{fig:MM'}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.7\textwidth]{TwoMatchings.pdf}
\end{center}
\caption{Symmetric difference of a non-maximum matching $M$ (blue) and a maximum matching $M'$ (red).}
\label{fig:MM'}
\end{figure}

Due to $|M'| > |M|$, in $H$ there are more edges from $M'$ than from $M$.
Therefore there is a path that starts and ends with an $M'$-edge.
The endpoints of this path have no incident $M$-edges, otherwise such an edge would also belong to $H$, and the path would not stop here.
This path is an $M$-augmenting path and the theorem is proved.
\end{proof}


In order to find a maximum matching in a graph, start with any matching (for example, an empty set of edges).
Then, recursively, find an augmenting path and modify the current matching.
If no augmenting path can be found, then the current matching is a maximum one.
Algorithms for finding an augmenting path are described in \cite[Section 16.5]{BM}.



\end{page}

%%%%%%%%%% output/336--2-4-3-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:336}

% \subsection{Matchings in bipartite graphs: the assignment problem and the optimal assignment problem}
% Let $G = (X \cup Y, E)$ be a bipartite graph.
% Since every edge joins a vertex from $X$ with a vertex from $Y$, there can be a perfect matching only if $|X| = |Y|$.
% We will assume that $|X| \le |Y|$ and look for a matching that covers all of $X$.
% 
% For example, let $X$ be a set of people and $Y$ be a set of jobs.
% The edge set is formed by those pairs $\{x,y\}$ where the person $x$ is qualified for the job $y$.
% To find a matching that covers $X$ means to assign all people to jobs for which they are qualified.




\end{page}

%%%%%%%%%% output/337--2-4-4-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:337}

\subsection{Matchings in bipartite graphs: Hall's theorem}

\end{page}

%%%%%%%%%% output/339--2-4-4-definition.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:339}

\begin{dfn}
Let $G = (V, E)$ be a graph.
For any subset $S \subset V$ of the vertex set denote by $N(S)$ the set of all vertices adjacent to vertices in $S$.
The set $N(S)$ is called the \emph{neighbor set} of $S$.
\end{dfn}

\end{page}

%%%%%%%%%% output/340--2-4-4-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:340}


In a bipartite graph $(X \cup Y, E)$, if $S \subset X$, then $N(S) \subset Y$.



\end{page}

%%%%%%%%%% output/342--2-4-4-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{7}
\label{portion:342}

\begin{thm}[Hall]
\label{thm:Hall}
Let $G = (X \cup Y, E)$ be a bipartite graph.
Then $G$ contains a matching that covers $X$ if and only if
\begin{equation}
\label{eqn:Hall}
|N(S)| \ge |S| \text{ for all } S \subset X.
\end{equation}
\end{thm}

\end{page}

%%%%%%%%%% output/343--2-4-4-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{7}
\label{portion:343}

In particular, if $|X| = |Y|$, then the above theorem provides a necessary and sufficient condition for the existence of a perfect matching.
\begin{proof}
Assume there is a matching that covers all of $X$.
This defines an injective map $f \colon X \to Y$ associating to every $x \in X$ its matched vertex in $Y$.
For every subset $S \subset X$ we have $f(S) \subset N(S)$, hence $|N(S)| \ge |f(S)| = |S|$.
Thus condition \eqref{eqn:Hall} is necessary.

Let us show that it is sufficient.
Let $M$ be a maximum matching of $G$.
Suppose that $M$ does not cover $X$; we will prove that \eqref{eqn:Hall} is violated.
Take a vertex $u \in X$ unmatched by $M$ and consider all alternating paths starting from $u$.
All these paths start with non-$M$-edges.

There are alternating paths of two sorts.
Those made of an even number of edges end in $X$.
Denote the set of their endpoints by $S$.
(We have $u \in S$, because we also consider the path of zero length starting and ending at $u$.)
Alternating paths made of an odd number of edges end in $Y$.
Denote the set of their endpoints by $T$.
See Figure \ref{fig:HallProof}.

\begin{figure}[ht]
\begin{center}
\input{Fig/HallProof.pdf_t}
\end{center}
\caption{To the proof of Theorem \ref{thm:Hall}.}
\label{fig:HallProof}
\end{figure}

We now prove a series of claims.

\emph{Claim 1.} Every vertex $v \in S$ other than $u$ is matched to a vertex in $T$.
Indeed, $v$ is the endpoint of a non-trival alternating path $P$ of even length.
The last edge of $P$ belongs to $M$, thus $v$ is matched to some vertex $w \in Y$.
We have $w \in T$, because $w$ is the endpoint of an alternating path $P - \{v,w\}$.

\emph{Claim 2.} Every vertex $w \in T$ is matched to a vertex in $S$.
Assume that some $w \in T$ is not matched.
Then the alternating path $P$ from $u$ to $w$ is an augmenting path, which contradicts the assumption of maximality of $M$.
If $w$ is matched to $v \in X$, then the path $P + \{v,w\}$ is also an augmenting path, thus $v \in S$.

\emph{Claim 3.} $|T| = |S| - 1$.
Indeed, by the previous two claims the matching $M$ establishes a bijection between $S \setminus \{u\}$ and $T$.

\emph{Claim 4.} $N(S) = T$.
Every edge incident to $u$ is a length $1$ alternating path.
Thus all neighbors of $u$ belong to $T$.
Let $e$ be an edge incident to $v \in S$, $v \ne u$.
If $e$ belongs to $M$, then its other endpoint is in $T$ by Claim 1.
If $e$ does not belong to $M$, then it extends an alternating path ending in $v$, thus again ends in $T$.
This proves $N(S) \subset T$.
We have $T \subset N(S)$ by construction of $S$ and $T$: every vertex $w \in T$ is the endpoint of an alternating path starting at $u$.
Just before coming to $w$, this path visited a vertex in $S$.

It follows that $|N(S)| = |T| = |S| - 1 < |S|$, which violates \eqref{eqn:Hall}.
Thus the assumption that a maximum matching does not cover all of $X$ was false, and the theorem is proved.
\end{proof}



\end{page}

%%%%%%%%%% output/345--2-4-4-corollary.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{8}
\label{portion:345}

\begin{cor}
Every $k$-regular ($k>0$) bipartite graph has a perfect matching.
\end{cor}

\end{page}

%%%%%%%%%% output/346--2-4-4-other.tex
\begin{page}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{dfn}{8}
\label{portion:346}

\begin{proof}
Let $G = (X \cup Y, E)$ be a $k$-regular bipartite graph.
Since every edge is incident to exactly one vertex from $X$, and every vertex is incident to exactly $k$ edges, we have $|E| = k|X|$.
Similarly, $|E| = k|Y|$. Thus we have $|X| = |Y|$.

Let us show that $k$-regularity implies condition \eqref{eqn:Hall}.
Take any $S \subset X$ and consider the bipartite graph $G' = (S \cup N(S), E')$, where $E'$ is the set of all edges incident to a vertex from $S$.
By the above argument, $|E'| = k|S|$.
On the other hand, for every $v \in N(S)$ we have $\deg_{G'} v \le k$, which implies $|E'| \le k|N(S)|$.
It follows that
\[
|N(S)| \ge \frac{|E'|}{k} = |S|,
\]
and we are done.
\end{proof}








\end{page}

%%%%%%%%%% output/347--3-0-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{0}
\setcounter{dfn}{8}
\label{portion:347}

\chapter{Propositional logic}
\section*{Introduction}
We will study two logical systems: \emph{propositional logic} and \emph{predicate logic}.
The predicate logic is also know as first-order logic; there is also second-order logic and higher order logics.

Every logic has two aspects: \emph{syntax} and \emph{semantics}.
On the syntactic side, logic consists of a \emph{language}, which is a set of expressions built according to certain rules.
The semantics interprets each of these expressions as true or false.

There are obvious parallels to human languages, but also important differences.
Every human language has syntax rules, but firstly a real language is a living thing, so that the vocabulary and syntax are constantly changing,
and secondly one may intentionally break the rules in order to create an artistic effect (you will easily find examples in literature or cinema).
Also, a human language contains sentences without truth values, such as ``What's for lunch?'' or ``Mind the gap''.

Thus, if we apply logic to a human language, then we can deal only with declarative sentences.
An example of a declarative sentence is ``It will snow for Christmas''.
This is certainly either true or false, although we do not know it at the moment.

Here we come to an important point.
Although we have said that every logical expression has a well-defined truth value, it may be not clear what this value is.
A \emph{proof theory} provides tools for determining this truth value.
Every proof theory contains a set of \emph{inference rules} or \emph{deduction rules},
which allow to determine the truth value of an expression if the truth values of some other expressions are known.
This is, of course, the way we are using logic in the everyday life, when we are trying to convince someone.
A classical example of deduction is
\begin{quote}
All men are mortal.\\
Socrates is a man.\\
Therefore, Socrates is mortal.
\end{quote}
(If the first two statements are true, then so is the third one.)

Formalization of human reasoning was at the origin of logic and stimulated its development over the centuries.
Another important motivation was the search for foundations of mathematics (end of XIX -- beginning of XX century)
as one has tried to axiomatize mathematics and formalize the mathematical reasoning.
Every mathematical proposition is either true or false, but it can be difficult to determine which way it is.
The four-color theorem and Fermat's Last Theorem are famous examples.

The following logic textbooks are recommended: \cite{Gallier, CL1, CL2, Dalen}.




\end{page}

%%%%%%%%%% output/348--3-1-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:348}

\section{Syntax and semantics of propositional logic}

\end{page}

%%%%%%%%%% output/349--3-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:349}

\subsection{Propositional formulas}
\label{sec:PropFormulas}
The language of propositional logic consists of strings of symbols, where each of the symbols is one of the following:
\begin{itemize}
\item
A proposition symbol $p$, $q$, $r$, $s$, $p_1, p_2, \ldots$.
(A countably infinite set.)
\item
A logical connective $\wedge$, $\vee$, $\to$, $\neg$.
\item
An auxiliary symbol $($ or $)$.
\end{itemize}
Sometimes to the list of logical connectives one adds $\leftrightarrow$ and $\perp$.
We will abstain from this.

The logical connectives have the following names.

\begin{center}
\begin{tabular}[c]{l@{\hspace{1cm}}l@{\hspace{1cm}}l}
$\wedge$ & and & conjunction\\
$\vee$ & or & disjunction\\
$\to$ & if ..., then ... & implication\\
$\neg$ & not & negation
\end{tabular}
\end{center}

Now there come the syntax rules describing what strings of symbols are allowed.


\end{page}

%%%%%%%%%% output/351--3-1-1-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:351}

\begin{dfn}
\label{dfn:PropForm}
The set of \emph{propositions} or \emph{propositional formulas} $\Prop$ is defined as follows.
\begin{enumerate}
\item
Proposition symbols belong to $\Prop$. They are called \emph{atoms} or \emph{atomic propositions}.
\item
If $A \in \Prop$, then $\neg A \in \Prop$.
\item
If $A, B \in \Prop$, then $(A \wedge B)$, $(A \vee B)$, $(A \to B) \in \Prop$.
\item
A string of symbols belongs to $\Prop$ only if it can be obtained by applying the above rules.
\end{enumerate}
\end{dfn}

\end{page}

%%%%%%%%%% output/352--3-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:352}

The last condition can be replaced by the requirement that $\Prop$ is the smallest (in the sense of inclusion) set satisfying the first three conditions.

A propositional formula has a recursive structure that can be represented with a \emph{parse tree}.
See Figure \ref{fig:ParseTreeProp} for an example.

\begin{figure}[ht]
\begin{center}
\input{Fig/ParseTreeProp.pdf_t}
\end{center}
\caption{Parse tree for $((p \vee q) \to \neg q)$.}
\label{fig:ParseTreeProp}
\end{figure}

Parentheses in a propositional formula ensure that the parse tree is unique.
If the root of the parsing tree is labeled with $\wedge, \vee$, or $\to$, then the formula starts with $($ and ends with $)$.
This pair of parentheses is not needed for parsing, and we will often omit it.
Strictly speaking, this is syntactically wrong, but should not lead to confusion.



\end{page}

%%%%%%%%%% output/353--3-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:353}

\subsection{Truth tables}
The set of \emph{truth values} is a two-element set $\{\text{true}, \text{false}\}$.
For brevity we will denote
\[
1 = \text{true}, \quad 0 = \text{false}.
\]
Various other abbreviations are used, for example $\mathbb{T}$ for ``true'' and $\mathbb{F}$ for ``false''.


\end{page}

%%%%%%%%%% output/355--3-1-2-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:355}

\begin{dfn}
Let $S = \{p, q, r, s, p_1, p_2, \ldots\}$ be the set of all proposition symbols.
A \emph{valuation} is a map
\[
v \colon S \to \{0,1\}
\]
assigning a truth value to all propositional symbols.
\end{dfn}

\end{page}

%%%%%%%%%% output/356--3-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:356}


Once the truth values of all proposition symbols are known, one can determine the truth value of any propositional formula.
This is done with the help of the truth tables for logical connectives given below.

\begin{center}
\begin{tabular}{|c||c|}
\hline
$A$ & $\neg A$\\\hline
$0$ & $1$\\\hline
$1$ & $0$\\\hline
\end{tabular}
\hspace{2cm}
\begin{tabular}{|c|c||c|c|c|}
\hline
$A$ & $B$ & $A \wedge B$ & $A \vee B$ & $A \to B$\\\hline
$0$ & $0$ & $0$ & $0$ & $1$\\\hline
$0$ & $1$ & $0$ & $1$ & $1$\\\hline
$1$ & $0$ & $0$ & $1$ & $0$\\\hline
$1$ & $1$ & $1$ & $1$ & $1$\\\hline
\end{tabular}
\end{center}

Here $A$ and $B$ are arbitrary propositions.
The tables tell you the truth values of $\neg A$, $A \wedge B$, $A \vee B$, $A \to B$ once the truth values of $A$ and $B$ are known.
They allow to write the truth table for any propositional formula recursively by ``climbing'' the parse tree.


\end{page}

%%%%%%%%%% output/358--3-1-2-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:358}

\begin{exl}
\label{exl:TruthTable}
The following table gives the truth values for the formula $((p \vee q) \to \neg q)$ and all of its subformulas
depending on a valuation $\{p, q\} \to \{0,1\}$.
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$p$ & $q$ & $(p \vee q)$ & $\neg q$ & $((p \vee q) \to \neg q)$\\\hline
$0$ & $0$ & $0$ & $1$ & $1$\\\hline
$0$ & $1$ & $1$ & $0$ & $0$\\\hline
$1$ & $0$ & $1$ & $1$ & $1$\\\hline
$1$ & $1$ & $1$ & $0$ & $0$\\\hline
\end{tabular}
\end{center}
\end{exl}

\end{page}

%%%%%%%%%% output/359--3-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{3}
\label{portion:359}


Speaking more formally, any valuation $v$ extends to a unique map
\[
\hat{v} \colon \Prop \to \{0,1\}
\]
defined recursively by $\hat{v}(x) = v(x)$ for all $x \in S$ and by
\begin{align*}
\hat{v}(A \wedge B) &= \hat{v}(A) \wedge \hat{v}(B) & \hat{v}(A \vee B) &= \hat{v}(A) \vee \hat{v}(B)\\
\hat{v}(A \to B) &= \hat{v}(A) \to \hat{v}(B) & \hat{v}(\neg A) &= \neg \hat{v}(A),
\end{align*}
where the values at the right hand sides are computed according to the truth tables of logical connectives.

Although by definition every valuation $v$ assigns truth values to \emph{all} proposition symbols,
in order to determine $\hat{v}(A)$ we need to know only the values of symbols occuring in $A$.



\end{page}

%%%%%%%%%% output/360--3-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{3}
\label{portion:360}

\subsection{Satisfiability, tautologies, logical equivalence}

\end{page}

%%%%%%%%%% output/362--3-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:362}

\begin{dfn}
Let $A$ be a proposition and $v$ be a valuation.
If $\hat{v}(A) = 1$, then we say that $v$ \emph{satisfies} $A$ and denote this by $v \vDash A$.
If $\hat{v}(A) = 0$, then we say that $v$ \emph{falsifies} $A$ and denote this by $v \nvDash A$.
\end{dfn}

\end{page}

%%%%%%%%%% output/365--3-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:365}

\begin{dfn}
A proposition $A$ is \emph{satisfiable} if it is satisfied by at least one valuation.
A proposition is \emph{unsatisfiable} if it is not satisfied by any valuation.

A proposition $A$ is \emph{valid} or a \emph{tautology} if it is satisfied by all valuations: $\hat{v}(A) = 1$ for all $v$.
We denote this by $\vDash A$.
\end{dfn}

\end{page}

%%%%%%%%%% output/366--3-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:366}


For example, looking at the truth table of Example \ref{exl:TruthTable} we see that the formula $((p \vee q) \to \neg q)$ is satisfiable but not a tautology.


\end{page}

%%%%%%%%%% output/368--3-1-3-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:368}

\begin{thm}
\begin{enumerate}
\item
Every tautology is satisfiable.
\item
A proposition $A$ is a tautology if and only if $\neg A$ is unsatisfiable.
\end{enumerate}
\end{thm}

\end{page}

%%%%%%%%%% output/369--3-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:369}

\begin{proof}
The set of all valuations is non-empty.
Therefore if all valuations satisfy $A$, then there is at least one valuation that satisfies $A$.

From the truth table for $\neg$ it follows that
\[
v \vDash A \text{ if and only if } v \nvDash \neg A.
\]
It follows that $A$ is satisfied by all valuations if and only if $\neg A$ is not satisfied by any.
\end{proof}

The problem of determining whether a given proposition is satisfiable is called \emph{satisfiability problem}, abbreviated $SAT$.
The problem of determining whether a given proposition is a tautology is called \emph{tautology problem}, abbreviated $TAUT$.
The satisfiability problem is $NP$-complete, that is every non-deterministically solvable in polynomial time problem can be reduced to it.
Therefore if $SAT$ can be solved in polynomial time, then every $NP$-problem can, that is $P=NP$.
On the other hand, if $TAUT$ is not $NP$, then $P \ne NP$.
For more details, see \cite[Section 3.3.5]{Gallier}.


\end{page}

%%%%%%%%%% output/371--3-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:371}

\begin{dfn}
\label{dfn:LogEqProp}
Two propositions $A$ and $B$ are called \emph{logically equivalent}, which is denoted by $A \simeq B$ if they are satisfied by the same valuations:
\[
\hat{v}(A) = \hat{v}(B) \text{ for all }v.
\]
In other words, $A \simeq B$ if and only if $A$ and $B$ have the same truth table.
\end{dfn}

\end{page}

%%%%%%%%%% output/374--3-1-3-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:374}

\begin{exl}
Looking at the truth table of Example \ref{exl:TruthTable} we see that
\[
((p \vee q) \to \neg q) \simeq \neg q.
\]
\end{exl}

\end{page}

%%%%%%%%%% output/377--3-1-3-remark.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:377}

\begin{rem}
The symbol $\simeq$ of the logical equivalence does not belong to the alphabet of propositional logic, so that $A \simeq B$ is not a propositional formula.
The symbols $\simeq$ and $\vDash$, and also symbols $A$ and $B$ used to denote arbitrary propositions,
all belong to the \emph{metalanguage}, a language that we use to describe propositional logic and prove its properties.
Also the symbol $\Leftrightarrow$ that we will use in our arguments as an abbreviation for ``if and only if'' is a metasymbol.
The definitions and statements in this section define notions of a metalanguage and formulate metatheorems.

When we prove some properties of propositional logic,
we are implicitly using another, more complicated, logical system with its own semantics (what is true and what is not true).
If, in turn, we want to discuss this more complicated system, then we need a metametalanguage and so on.

Imagine a computer program which is able to recognize propositional formulas and compute their truth values for any valuation.
This program speaks the language of propositional logic, and it cannot analize its own actions.
The programmer speaks a metalanguage and can predict the behavior of the program.
\end{rem}

\end{page}

%%%%%%%%%% output/380--3-1-3-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:380}

\begin{thm}
Propositions $A$ and $B$ are logically equivalent if and only if the proposition
\[
(A \to B) \wedge (B \to A)
\]
is a tautology.
\end{thm}

\end{page}

%%%%%%%%%% output/381--3-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:381}

\begin{proof}
One proves that $\hat{v}((A \to B) \wedge (B \to A)) = 1$ if and only if $\hat{v}(A) = \hat{v}(B)$ by a case distinction,
that is by considering all four possible combinations of values $\hat{v}(A)$ and $\hat{v}(B)$.
Thus if $\hat{v}(A) = \hat{v}(B)$ for all $v$, then $(A \to B) \wedge (B \to A)$ is a tautology, and vice versa.
\end{proof}


There are several simple and useful equivalences between propositional formulas.
\begin{itemize}
\item
Associativity laws
\[
(A \wedge B) \wedge C \simeq A \wedge (B \wedge C) \qquad (A \vee B) \vee C \simeq A \vee (B \vee C)
\]
\item
Commutativity laws
\[
A \wedge B \simeq B \wedge A \qquad A \vee B \simeq B \vee A
\]
\item
Distributivity laws
\[
A \wedge (B \vee C) \simeq (A \wedge B) \vee (A \wedge C) \qquad A \vee (B \wedge C) \simeq (A \vee B) \wedge (A \vee C)
\]
\item
De Morgan's laws
\[
\neg(A \wedge B) \simeq \neg A \vee \neg B \qquad \neg(A \vee B) \simeq \neg A \wedge \neg B
\]
\item
Idempotency laws
\[
A \wedge A \simeq A \qquad A \vee A \simeq A
\]
\item
Double negation law
\[
\neg\neg A \simeq A
\]
\end{itemize}

The associativity laws allow us to omit parentheses in conjunctions or disjunctions.
Due to it we can allow abuse of notation and write strings like this one:
\[
p \wedge q \wedge r \wedge s.
\]
This is against the syntax rules.
In order to conform the rules, we must put some parentheses.
This leads to several different propositional formulas, like $((p \wedge q) \wedge (r \wedge s))$ or $((p \wedge (q \wedge r)) \wedge s)$.
Although these formulas are different, they are logically equivalent.
Thus, the string $p \wedge q \wedge r \wedge s$ stands for a class of equivalent formulas.

The distributivity laws allow us to operate with brackets as if $\wedge$ is the multiplication and $\vee$ is the addition
or vice versa.
For example,
\[
\neg p \vee (p \wedge q) \simeq (\neg p \vee p) \wedge (\neg p \vee q).
\]

Let us extend the language $\Prop$ by adding two new symbols $\top$ and $\perp$.
We define a language $\widetilde{\Prop}$ by adding to the point 1. in Definition \ref{dfn:PropForm} that $\top$ and $\perp$ belong to $\widetilde{\Prop}$
and leaving the other conditions as they are.
The symbols $\top$ and $\perp$ are \emph{logical constants}: when computing the truth value of a proposition $A$
we replace each $\top$ by $1$ and each $\perp$ by $0$.
In $\widetilde{\Prop}$ there are the following logical equivalences.
\begin{itemize}
\item
Laws of zero and one.
\begin{gather*}
(A \wedge \perp) \simeq \perp \qquad (A \vee \perp) \simeq A\\
(A \wedge \top) \simeq A \qquad (A \vee \top) \simeq \top\\
(A \wedge \neg A) \simeq \perp \qquad (A \vee \neg A) \simeq \top
\end{gather*}
\end{itemize}

These equivalences can be used in order to simplify certain propositions from $\Prop$.
As an example, let us simplify the formula we just obtained.
\[
(\neg p \vee p) \wedge (\neg p \vee q) \simeq \top \wedge (\neg p \vee q) \simeq \neg p \vee q.
\]




\end{page}

%%%%%%%%%% output/382--3-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:382}

\subsection{Boolean functions}
Although the set of all proposition symbols is infinite,
every proposition contains only a finite number of distinct proposition symbols.
Assume that $A \in \Prop$ contains only symbols from the set $\{p_1, p_2, \ldots, p_n\}$.
Then $A$ defines a map
\[
f_A \colon \{0,1\}^n \to \{0,1\}
\]
in the following way.
Every element $(x_1, \ldots, x_n) \in \{0,1\}^n$ can be viewed as a partial valuation, giving $p_i$ the truth value $x_i$ for $i = 1, \ldots, n$.
Then $f_A(x_1, \ldots, x_n)$ is the truth value of $A$ corresponding to this valuation:
\[
f_A(x_1, \ldots, x_n) = \hat{v}(A), \text{ where } v(p_i) = x_i.
\]

The function $f_A$ is described by the truth table of proposition $A$.
As an immediate reformulation of Definition \ref{dfn:LogEqProp},
\[
A \simeq B \Leftrightarrow f_A = f_B.
\]

In fact, $0$-$1$-valued functions of $0$-$1$-valued agruments is a very basic object which can be studied irrespective of the propositional logic.

\end{page}

%%%%%%%%%% output/384--3-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:384}

\begin{dfn}
A map $f \colon \{0,1\}^n \to \{0,1\}$ is called a \emph{boolean function} of $n$ variables.
\end{dfn}

\end{page}

%%%%%%%%%% output/385--3-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:385}


We have shown that every propositional formula $A$ determines a boolean function $f_A$.
One can now ask whether every boolean function $f$ can appear in this way.
The answer is ``yes''.


\end{page}

%%%%%%%%%% output/387--3-1-4-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:387}

\begin{thm}
\label{thm:BooleNF}
Every boolean function $\{0,1\}^n \to \{0,1\}$ can be represented by a propositional formula.
\end{thm}

\end{page}

%%%%%%%%%% output/388--3-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:388}

\begin{proof}
First let us find a formula that represents a very simple function: a function that takes value $1$ at one point only.
Take any vector $x \in \{0,1\}^n$ and put
\[
f(x) = 1, \quad f(y) = 0 \text{ for all }y \ne x.
\]
We need a propositional formula with symbols $p_1, \ldots, p_n$ that evaluates to $1$ only when $v(p_i) = x_i$ for all $i$.
This is achieved by a conjunction
\[
B = C_1 \wedge C_2 \wedge \cdots \wedge C_n,
\]
where
\[
C_i =
\begin{cases}
p_i, &\text{ if } x_1 = 1,\\
\neg p_i, &\text{ if }x_1 = 0.
\end{cases}
\]
Let us prove that $f_B = f$ in a formal way.
Take any $y \in \{0,1\}^n$.
By definition, $f_B(y) = \hat{v}(B)$ for the valuation $v(p_i) = y_i$.
Apply the recursive definition of $\hat{v}$:
\[
f_B(y) = \hat{v}(B) = \hat{v}(C_1) \wedge \hat{v}(C_2) \wedge \cdots \wedge \hat{v}(C_n).
\]
The right hand side is equal to $1$ if and only if $\hat{v}(C_i) = 1$ for all $i$.
By definition of $C_i$ we have
\begin{align*}
\text{if }x_i = 1,\ &\text{then }\hat{v}(C_i) = \hat{v}(p_i) = v(p_i) = y_i\\
\text{if }x_i = 0,\ &\text{then } \hat{v}(C_i) = \hat{v}(\neg p_i) = \neg \hat{v}(p_i) = \neg y_i,
\end{align*}
which implies that $\hat{v}(C_i) = 1$ if and only if $y_i = x_i$.
It follows that $f_B(y) = 1$ if and only if $y_i = x_i$ for all $i$, that is $f_B = f$.

Now let $f$ be an arbitrary Boolean function of $n$ arguments.
If $f(x) = 0$ for all $x$, then one can represent $f$ by the formula $p_1 \wedge \neg p_1$.
Assume that there is at least one $x$ such that $f(x) = 1$.
For every $x \in \{0,1\}^n$ such that $f(x) = 1$ construct a conjunction $B_x$ as above and take the disjunction of all such $B_x$:
\[
A = \bigvee_{f(x)=1} B_x.
\]
We claim that $f_A = f$.
Indeed, $A$ evaluates to $1$ if and only if at least one of $B_x$ evaluates to $1$
and, as we know, $B_x$ evaluates to $1$ only at $x$.
Thus $A$ evaluates to $1$ exactly at those points where $f(x) = 1$.

Formally, for every $y$ we have
\[
f_A(y) = \hat{v}(A) = \bigvee_{f(x)=1} \hat{v}(B_x)
\]
(where $v(p_i) = y_i$).
The right hand side equals $1$ if and only if $\hat{v}(B_x) = 1$ for some $x$.
But $\hat{v}(B_x) = 1$ if and only if $y=x$.
Thus $f_A(y) = 1$ if and only if $y=x$ for some $x$ such that $f(x)=1$, which is just a complicated way to say if and only if $f(y) = 1$.
Hence $f_A = f$, and the theorem is proved.
\end{proof}

The above argument provides a procedure of writing a formula with a given truth table.
We will illustrate it on the example of the function given by the table below.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$p$ & $q$ & ?\\\hline
$0$ & $0$ & $0$\\\hline
$0$ & $1$ & $1$\\\hline
$1$ & $0$ & $1$\\\hline
$1$ & $1$ & $0$\\\hline
\end{tabular}
\end{center}

Mark the rows with $1$ at the end.
For each of these rows write a conjunction of all proposition symbols, negated or not depending on whether the value of this symbol in this row is $0$ or $1$.
In our case these are $\neg p \wedge q$ and $p \wedge \neg q$.
Then write the disjunction of the obtained conjunctions:
\[
(\neg p \wedge q) \vee (p \wedge \neg q).
\]

Observe that the formula constructed in Theorem \ref{thm:BooleNF} does not use the connective $\to$.

\end{page}

%%%%%%%%%% output/390--3-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:390}

\begin{dfn}
A system of logical connectives is called \emph{functionally complete} if every Boolean function can be represented by a formula using only these connectives.
\end{dfn}

\end{page}

%%%%%%%%%% output/391--3-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:391}

Thus the system $\neg, \wedge, \vee$ is functionally complete.
One can dispense of one of the connectives $\wedge$ or $\vee$ as well.


\end{page}

%%%%%%%%%% output/393--3-1-4-lemma.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:393}

\begin{lem}
Each of the systems of logical connectives $\neg, \wedge$ and $\neg, \vee$ is functionally complete.
\end{lem}

\end{page}

%%%%%%%%%% output/394--3-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:394}

\begin{proof}
By Theorem \ref{thm:BooleNF} every Boolean function can be represented by a formula using $\neg, \wedge, \vee$ only.
Replace every occurrence of $\vee$ using a De Morgan law and the double negation:
\[
A \vee B \simeq \neg(\neg A \wedge \neg B).
\]
This gives an equivalent formula with connectives $\neg$ and $\wedge$ only.
One removes conjunctions in a similar way with the help of the equivalence
\[
A \wedge B \simeq \neg (\neg A \vee \neg B).
\]
\end{proof}

For example,
\begin{align*}
(\neg p \wedge q) \vee (p \wedge \neg q) &\simeq \neg(\neg(\neg p \wedge q) \wedge \neg (p \wedge \neg q))\\
&\simeq \neg(p \vee \neg q) \vee \neg(\neg p \vee q)
\end{align*}

One can achieve an absolute minimalism by introducing a logical connective $\uparrow$ with the truth table
\begin{center}
\begin{tabular}{|c|c||c|c|c|}
\hline
$A$ & $B$ & $A \uparrow B$\\\hline
$0$ & $0$ & $1$\\\hline
$0$ & $1$ & $1$\\\hline
$1$ & $0$ & $1$\\\hline
$1$ & $1$ & $0$\\\hline
\end{tabular}
\end{center}
(also called $NAND$ for obvious reasons).


\end{page}

%%%%%%%%%% output/396--3-1-4-lemma.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{15}
\label{portion:396}

\begin{lem}
The system of a single logical connective $\uparrow$ is functionally complete.
\end{lem}

\end{page}

%%%%%%%%%% output/397--3-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{15}
\label{portion:397}

\begin{proof}
Exercise.
\end{proof}





\end{page}

%%%%%%%%%% output/398--3-1-5-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{15}
\label{portion:398}

\subsection{Disjunctive and conjunctive normal forms}
Propositional formulas that we obtained in Theorem \ref{thm:BooleNF} have a very special form:
they are disjunction of conjunctions of (possibly negated) proposition symbols.

Let us fix some terminology.
A \emph{literal} is a proposition symbol or its negation.
A \emph{conjunctive clause} is a conjunction of one or several literals.


\end{page}

%%%%%%%%%% output/400--3-1-5-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{16}
\label{portion:400}

\begin{dfn}
A propositional formula is said to be in \emph{disjunctive normal form} (for brevity, \emph{DNF})
if it is a disjunction of conjunctive clauses.
\end{dfn}

\end{page}

%%%%%%%%%% output/403--3-1-5-corollary.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{17}
\label{portion:403}

\begin{cor}
\label{cor:DNF}
Every propositional formula is logically equivalent to a formula in DNF.
\end{cor}

\end{page}

%%%%%%%%%% output/404--3-1-5-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{17}
\label{portion:404}

\begin{proof}
This follows from Theorem \ref{thm:BooleNF}.
Every formula represents a boolean function.
As we proved in Theorem \ref{thm:BooleNF}, every boolean function is represented by a formula in DNF.
Formulas representing the same function are logically equivalent.
\end{proof}

Similarly, a \emph{disjunctive clause} is a disjunction of several literals.

\end{page}

%%%%%%%%%% output/406--3-1-5-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{18}
\label{portion:406}

\begin{dfn}
A propositional formula is said to be in \emph{conjunctive normal form} (for brevity, \emph{DNF})
if it is a conjunction of disjunctive clauses.
\end{dfn}

\end{page}

%%%%%%%%%% output/409--3-1-5-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{19}
\label{portion:409}

\begin{thm}
Every propositional formula is logically equivalent to a formula in CNF.
\end{thm}

\end{page}

%%%%%%%%%% output/410--3-1-5-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{19}
\label{portion:410}

\begin{proof}
Let $A$ be a propositional formula.
By Corollary \ref{cor:DNF} the negation of $A$ is equivalent to some formula in DNF:
\[
\neg A \simeq \bigvee_{\alpha = 1}^N B_\alpha, \quad B_\alpha = C_{\alpha,1} \wedge \cdots \wedge C_{\alpha,k_{\alpha}}, \quad C_{\alpha,i} \text{ literals}.
\]
By De Morgan's law we have
\[
A \simeq \neg\neg A \simeq \neg \bigvee_{\alpha = 1}^N B_\alpha \simeq \bigwedge_{\alpha = 1}^N \neg B_\alpha
\simeq \bigwedge_{\alpha = 1}^N (\neg C_{\alpha,1} \vee \cdots \vee \neg C_{\alpha,k_{\alpha}}).
\]
The negation of a literal is a negated or a doubly negated symbol.
Double negations can be removed, and we obtain a formula in CNF.
\end{proof}

An unsatisfiable formula is equivalent to $p \wedge \neg p$.
This formula is at the same time in DNF (it is a single conjunctive clause) and in CNF (it is a conjunction of two disjunctive clauses).
A tautology is equivalent to $p \vee \neg p$, which is also in DNF and in CNF.


\end{page}

%%%%%%%%%% output/412--3-1-5-remark.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{20}
\label{portion:412}

\begin{rem}
If the logical symbols $\top$ and $\perp$ are present, then one can consider $\top$ as a disjunction of zero length
(and thus a CNF and a DNF for a tautology), and $\perp$ as a conjunction of zero length (thus a CNF and a DNF for an unsatisfiable formula).
\end{rem}

\end{page}

%%%%%%%%%% output/413--3-1-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{0}
\setcounter{dfn}{20}
\label{portion:413}


DNF is not unique: different formulas in DNF can be equivalent.
The procedure described in Theorem \ref{thm:BooleNF} produces a disjunction of clauses of length $n$ (where $n$ is the number of variables in the input formula).
For example, the formula $p \vee q$ (although it is already in DNF) will be represented by $(p \wedge q) \vee (p \wedge \neg q) \vee (\neg p \wedge q)$.
The same applies to CNF.




\end{page}

%%%%%%%%%% output/414--3-2-0-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:414}

\section{Proof theories}

\end{page}

%%%%%%%%%% output/415--3-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:415}

\subsection{Deductive systems}
A proof theory is a method of establishing the validity of a proposition (that is whether a given proposition is a tautology).
There is a type of proof theories called deductive systems.
A deductive system consists of
\begin{itemize}
\item a set of propositional formulas called \emph{axioms};
\item a set of inference rules.
\end{itemize}

An inference rule has the form $\Gamma \vdash A$, where $A$ is a proposition and $\Gamma$ is a set of propositions.
It is convenient to write an axiom $A$ in the form $\vdash A$.
This allows to define provable propositions recursively.


\end{page}

%%%%%%%%%% output/417--3-2-1-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:417}

\begin{dfn}
A proposition is called \emph{provable} if and only if it is an axiom or can be obtained from provable propositions by applying rules of inference,
that is there is a set of provable propositions $\Gamma$ such that $\Gamma \vdash A$ is an inference rule.
\end{dfn}

\end{page}

%%%%%%%%%% output/418--3-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:418}


One can be more concrete by defining formal proofs as follows.

\end{page}

%%%%%%%%%% output/420--3-2-1-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:420}

\begin{dfn}
\label{dfn:HilbertProof}
A \emph{formal proof} is a sequence of propositional formulas,
where each formula is an axiom or is obtained from some of the previous formulas by an inference rule.
That is, if $\Gamma \vdash A$ is an inference rule and the sequence already contains all the formulas from the list $\Gamma$,
then you can add $A$ at the end of the sequence.

The final formula of a formal proof is called a \emph{theorem}.
\end{dfn}

\end{page}

%%%%%%%%%% output/421--3-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:421}

Thus, a formula is called provable if and only if it is a theorem.
An axiom is also a theorem.
Its proof is a sequence consisting of a single term.



\end{page}

%%%%%%%%%% output/423--3-2-1-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:423}

\begin{dfn}
A proof system is called \emph{sound} if every provable formula is a tautology.
A proof system is called \emph{complete} if every tautology is provable.
\end{dfn}

\end{page}

%%%%%%%%%% output/424--3-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:424}


In terms of the turnstile notation, a system is sound if $\vdash A \Rightarrow  \vDash A$, and complete if $\vDash A \Rightarrow \vdash A$.



\end{page}

%%%%%%%%%% output/425--3-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:425}

\subsection{A Hilbert system}
A Hilbert-style deductive system has a single inference rule, the so-called \emph{modus ponens}:
\[
A, A \to B \vdash B
\]
(If $A$ is provable and $A \to B$ is provable, then $B$ is provable.)
We will abbreviate modus ponens by MP.

There are many different axyom systems.
Here is one of them, the third \L{}ukasiewicz' system.

\begin{align*}
&\vdash A \to (B \to A)\\
&\vdash (A \to (B \to C)) \to ((A \to B) \to (A \to C))\\
&\vdash (\neg A \to \neg B) \to (B \to A)
\end{align*}

Here $A$, $B$, and $C$ may be any propositional formulas.
Thus, in fact, we have infinitely many axioms (axiom \emph{instances}) that can be obtained from the above axiom \emph{schemata}
by substituting for $A$, $B$, and $C$ some particular formulas.


\end{page}

%%%%%%%%%% output/427--3-2-2-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:427}

\begin{exl}
Let us prove the tautology $A \to A$.
First, substitute in the second axiom $(A \to A)$ for $B$ and $A$ for $C$:
\[
\vdash (A \to ((A \to A) \to A)) \to ((A \to (A \to A)) \to (A \to A))
\]
Now substitute $(A \to A)$ for $B$ in the first axiom:
\[
\vdash (A \to ((A \to A) \to A))
\]
This coincides with the ``left half'' of the previous formula, so by modus ponens we infer the ``right half'' of that formula:
\[
\vdash (A \to (A \to A)) \to (A \to A)
\]
Now take the first axiom with $B$ substituted for $A$:
\[
\vdash (A \to (A \to A)).
\]
This is the left half of the previous formula, so again by modus ponens we infer
\[
\vdash (A \to A),
\]
and the proof is finished.
\end{exl}

\end{page}

%%%%%%%%%% output/430--3-2-2-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:430}

\begin{thm}
A Hilbert system is sound if and only if all of its axioms are tautologies.
\end{thm}

\end{page}

%%%%%%%%%% output/431--3-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:431}

\begin{proof}
Exercise.
\end{proof}





\end{page}

%%%%%%%%%% output/432--3-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:432}

\subsection{Gentzen's sequent calculus: the idea}
Given a proposition $A$, we want to determine whether it is a tautology.
Recall that $A$ is \emph{not} a tautology if and only if there is a valuation $v$ that falsifies $A$.
(We will also call $v$ a \emph{counterexample} to $A$.)
So let us try to falsify $A$ or to show that this is impossible.
As an example, take
\[
A = (p \to q) \to (p \vee q).
\]

In order to falsify a proposition of the form $B \to C$ one has to satisfy $B$ and falsify $C$ at the same time.
This reduces our problem to a combination of two simpler ones.
Let us write our new task on the top of the old one:
\begin{prooftree}
\AxiomC{$p \to q \vdash p \vee q$}
\UnaryInfC{$\vdash (p \to q) \to (p \vee q)$}
\end{prooftree}
The turnstile symbol is used as a separator: on the left we write the things we want to satisfy, on the right the things we want to falsify.
(One can use any other separator.
The choice of $\vdash$ looks awkward because the symbol was used earlier to denote provability.
This choice is partially motivated by the subsequent sections.)

So, now we want to satisfy $p \to q$ and falsify $p \vee q$.
In order to falsify $p \vee q$ we have to falsify both of them at the same time.
We express this by $p \to q \vdash p, q$.
Now on the right hand side we have a list of formulas that we want to falsify simultaneously.
Our diagram takes the following form:
\begin{prooftree}
\AxiomC{$p \to q \vdash p, q$}
\UnaryInfC{$p \to q \vdash p \vee q$}
\UnaryInfC{$\vdash (p \to q) \to (p \vee q)$}
\end{prooftree}

There are two ways to satisfy $p \to q$: one has either to satisfy $q$ or to falsify $p$.
This introduces branching in the diagram:
\begin{center}
\AxiomC{$q \vdash p, q$}
\AxiomC{$\vdash p, p, q$}
\BinaryInfC{$p \to q \vdash p, q$}
\UnaryInfC{$p \to q \vdash p \vee q$}
\UnaryInfC{$\vdash (p \to q) \to (p \vee q)$}
\DisplayProof
\end{center}

Now, any valuation that solves one of the problems on the top of the diagram also solves the problem in the bottom.
The first problem sounds ``satisfy $p$ and falsify $p$ and $q$''.
This is, of course, impossible.
But the second problem says ``falsify $p$ and $q$''.
This is solved by setting $v(p) = 0$, $v(q) = 0$.
From the construction principle of the diagram it follows that this valuation falsifies the initial proposition $A = (p \to q) \to (p \vee q)$.
Thus we have shown that $A$ is not a tautology.

A couple of remarks are in order.
First, there is no need to repeat two equal propositions on the same side of $\vdash$ as we did with $p$ on the top right.
(We did it just to show that trying to satisfy $p \to q$ brings $q$ to the left or $p$ to the right.)
Second, sometimes we have a choice which connective to eliminate.
Here we had it at the second step.
If we choose to eliminate $p \to q$ before $p \vee q$, then the branching happens earlier, and the final diagram looks as follows.
\begin{prooftree}
\AxiomC{$q \vdash p,q$}
\UnaryInfC{$q \vdash p \vee q$}
\AxiomC{$\vdash p,q$}
\UnaryInfC{$\vdash p, p \vee q$}
\BinaryInfC{$p \to q \vdash p \vee q$}
\UnaryInfC{$\vdash (p \to q) \to (p \vee q)$}
\end{prooftree}




\end{page}

%%%%%%%%%% output/433--3-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:433}

\subsection{Sequents and inference rules}
In the above example we have operated with sets of propositions split into two subsets: those to satisy and those to falsify.

\end{page}

%%%%%%%%%% output/435--3-2-4-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:435}

\begin{dfn}
A \emph{sequent} is a pair $(\Gamma, \Delta)$ of finite (possibly empty) sets of propositions.
The set $\Gamma$ is called the \emph{antecedent}, the set $\Delta$ is called the \emph{succedent}.
A sequent is written as $\Gamma \vdash \Delta$ with the elements of $\Gamma$ and $\Delta$ listed in an arbitrary order.
\end{dfn}

\end{page}

%%%%%%%%%% output/436--3-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:436}


For any proposition in the sequent there is a rule that eliminates its outermost connective.
This rule depends on whether the proposition occurs in the antecedent or in the succedent.
Thus there are eight rules operating on sequents, two for each of the connectives $\wedge$, $\vee$, $\to$, $\neg$.


\end{page}

%%%%%%%%%% output/438--3-2-4-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{7}
\label{portion:438}

\begin{dfn}
\label{dfn:GentzenInference}
The \emph{inference rules} of the sequent calculus are as follows.
\begin{align*}
&(\wedge\text{ \rm left}):\
\AxiomC{$A, B, \Gamma \vdash \Delta$}
\UnaryInfC{$A \wedge B, \Gamma \vdash \Delta$}
\DisplayProof
&&(\wedge\text{ \rm right}):\
\AxiomC{$\Gamma \vdash A, \Delta$}
\AxiomC{$\Gamma \vdash B, \Delta$}
\BinaryInfC{$\Gamma \vdash A \wedge B, \Delta$}
\DisplayProof
\\
&(\vee\text{ \rm left}):\
\AxiomC{$A, \Gamma \vdash \Delta$}
\AxiomC{$B, \Gamma \vdash \Delta$}
\BinaryInfC{$A \vee B, \Gamma \vdash \Delta$}
\DisplayProof
&&(\vee\text{ \rm right}):\
\AxiomC{$\Gamma \vdash A, B, \Delta$}
\UnaryInfC{$\Gamma \vdash A \vee B, \Delta$}
\DisplayProof
\\
&(\to\text{ \rm left}):\
\AxiomC{$\Gamma \vdash A, \Delta$}
\AxiomC{$\Gamma, B \vdash \Delta$}
\BinaryInfC{$A \to B, \Gamma \vdash \Delta$}
\DisplayProof
&&(\to\text{ \rm right}):\
\AxiomC{$\Gamma, A \vdash B, \Delta$}
\UnaryInfC{$\Gamma \vdash A \to B, \Delta$}
\DisplayProof
\\
&(\neg\text{ \rm left}):\
\AxiomC{$\Gamma \vdash A, \Delta$}
\UnaryInfC{$\neg A, \Gamma \vdash \Delta$}
\DisplayProof
&&(\neg\text{ \rm right}):\
\AxiomC{$A, \Gamma \vdash \Delta$}
\UnaryInfC{$\Gamma \vdash \neg A, \Delta$}
\DisplayProof
\end{align*}
The sequents above the line are called \emph{premises}, the sequents below the line are called \emph{conclusions}.
\end{dfn}

\end{page}

%%%%%%%%%% output/441--3-2-4-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{8}
\label{portion:441}

\begin{dfn}
\label{dfn:FalsifySequent}
A valuation $v$ \emph{falsifies} a sequent $A_1, \ldots, A_m \vdash B_1, \ldots, B_n$ if it satisfies all of $A_i$ and falsifies all of $B_j$:
\[
v \vDash (A_1 \wedge \cdots \wedge A_m) \wedge (\neg B_1 \wedge \cdots \neg B_n).
\]
A sequent is called \emph{falsifiable} if it is falsified by some valuation.
A sequent is called \emph{valid} if it is not falsifiable.
\end{dfn}

\end{page}

%%%%%%%%%% output/444--3-2-4-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{9}
\label{portion:444}

\begin{lem}
\label{lem:RuleTrees}
For each of the rules given in Definition \ref{dfn:GentzenInference},
a valuation $v$ falsifies the conclusion if and only if it falsifies at least one of the premises.
\end{lem}

\end{page}

%%%%%%%%%% output/445--3-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{9}
\label{portion:445}

\begin{proof}
The proof is done by looking at the truth tables of the logical connectives.
Let us prove, for example, the ($\wedge$ right) rule:
\begin{prooftree}
\AxiomC{$\Gamma \vdash A, \Delta$}
\AxiomC{$\Gamma \vdash B, \Delta$}
\BinaryInfC{$\Gamma \vdash A \wedge B, \Delta$}
\end{prooftree}
A valuation $v$ falsifies the conclusion if and only if
it satisfies all propositions in $\Gamma$, falsifies $A \wedge B$, and falsifies all propositions in $\Delta$.
But $v$ falsifies $A \wedge B$ if and only if it either falsifies $A$ or falsifies $B$.
Thus $v$ falsifies the conclusion if and only if it
\begin{itemize}
\item
satisfies $\Gamma$ and falsifies $A$ and $\Delta$, or
\item
satisfies $\Gamma$ and falsifies $B$ and $\Delta$.
\end{itemize}
These conditions are the premises of the ($\wedge$ right) rule, therefore the statement of the lemma holds for this rule.
Similar arguments work for all of the other rules.
\end{proof}

For any proposition $A$ one can form a sequent $\vdash A$, which is falsifiable, respectively valid, if and only if
$A$ is falsifiable, respectively valid (a tautology).
Thus if we learn to prove all valid sequents, then we will be able to prove all tautologies.




\end{page}

%%%%%%%%%% output/446--3-2-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{9}
\label{portion:446}

\subsection{Axioms and proofs}
\label{sec:AxiomsProofs}

\end{page}

%%%%%%%%%% output/448--3-2-5-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{10}
\label{portion:448}

\begin{dfn}
A sequent $\Gamma \vdash \Delta$ is an \emph{axiom} if and only if $\Gamma \cap \Delta \ne \emptyset$:
there is a proposition listed both in $\Gamma$ and $\Delta$.
\end{dfn}

\end{page}

%%%%%%%%%% output/451--3-2-5-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{11}
\label{portion:451}

\begin{exl}
The following sequents are axioms:
\begin{itemize}
\item $q \vdash p, q$,
\item $(p \to q), p \vdash (p \to q), q$.
\end{itemize}
\end{exl}

\end{page}

%%%%%%%%%% output/454--3-2-5-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{12}
\label{portion:454}

\begin{lem}
\label{lem:AxiomValid}
All axioms are valid.
\end{lem}

\end{page}

%%%%%%%%%% output/455--3-2-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{12}
\label{portion:455}

\begin{proof}
In order to falsify a sequent $\Gamma \vdash \Delta$, we have to satisfy all propositions in $\Gamma$ and falsify all propositions in $\Delta$.
If $\Gamma$ and $\Delta$ contain a common proposition, then no valuation falsifies $\Gamma \vdash \Delta$.
Thus, $\Gamma \vdash \Delta$ is valid.
\end{proof}


\end{page}

%%%%%%%%%% output/457--3-2-5-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{13}
\label{portion:457}

\begin{dfn}
\label{dfn:DeductionTree}
A \emph{deduction tree} is a rooted tree whose vertices are labeled with sequents so that
every parent vertex forms an inference rule together with its children, where the children are the premises and the parent is the conclusion.
\end{dfn}

\end{page}

%%%%%%%%%% output/458--3-2-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{13}
\label{portion:458}

A parent vertex in a rooted tree is a vertex with the out-degree $\ge 1$ in the canonical orientation of the edges, see Figure \ref{fig:RootedTreeOrient}.
Vertices of out-degree $0$ will be called \emph{leaves} of a rooted tree.
Since the in-degree of every non-root vertex is $1$, non-root leaves are leaves in the usual sense.
However, the root is a leaf if and only if the tree has only one vertex.

A standalone sequent is a simplest deduction tree (tree with one vertex).
Next to it are the deduction trees copied from the inference rules as shown in Figure \ref{fig:DeductionTree}.
Note that a deduction tree the root is at the bottom, and the children of every vertex are situated above the vertex.

\begin{figure}[ht]
\begin{center}
\raisebox{1cm}{
\AxiomC{$\Gamma \vdash A, \Delta$}
\AxiomC{$\Gamma \vdash B, \Delta$}
\BinaryInfC{$\Gamma \vdash A \wedge B, \Delta$}
\DisplayProof
}
\hspace{1cm}
\input{Fig/DeductionTree.pdf_t}
\end{center}
\caption{The ($\wedge$: right) inference rule as a deduction tree.}
\label{fig:DeductionTree}
\end{figure}


\end{page}

%%%%%%%%%% output/460--3-2-5-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{dfn}{14}
\label{portion:460}

\begin{dfn}
\label{dfn:ProofTree}
A deduction tree is called a \emph{proof tree} if all of its leaves are axioms.
A sequent is called \emph{provable} if there is a proof tree with this sequent at the root.
\end{dfn}

\end{page}

%%%%%%%%%% output/462--3-2-6-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{14}
\label{portion:462}

\subsection{Soundness of the sequent calculus}

\end{page}

%%%%%%%%%% output/464--3-2-6-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{15}
\label{portion:464}

\begin{thm}
\label{thm:GentzenSound}
The sequent calculus is sound: every provable sequent is valid.
\end{thm}

\end{page}

%%%%%%%%%% output/467--3-2-6-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{dfn}{16}
\label{portion:467}

\begin{lem}
\label{lem:FalsifyRoot}
A valuation falsifies the root of a non-trivial deduction tree if and only if it falsifies at least one of its leaves.
\end{lem}

\end{page}

%%%%%%%%%% output/468--3-2-6-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{7}
\setcounter{dfn}{16}
\label{portion:468}

\begin{proof}
Induction on the number of vertices.

For trees with one vertex the statement is trivial.

Assume that the statement is proved for all deduction trees with $< n$ vertices.
Let $T$ be a deduction tree with $n$ vertices.
By definition, if we delete the root of $T$, then we obtain one or two deduction trees ``growing'' from the children of the root,
see Figure \ref{fig:InductionDedTree}.
Besides, the root of $T$ is the conclusion and its children are the premises of an inference rule.
Hence by Lemma \ref{lem:RuleTrees} a valuation falsifies the root if and only if it falsifies one of its children.
The subtrees growing from the children have $< n$ vertices,
therefore by induction assumption a valuation falsifies a child if and only if it falsifies one of the leaves of the corresponding subtree.
Such a vertex is also childless in the big tree.
This proves the induction step.
\end{proof}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{InductionDedTree.pdf}
\end{center}
\caption{Deduction trees near their roots.}
\label{fig:InductionDedTree}
\end{figure}

\begin{proof}[Proof of Theorem \ref{thm:GentzenSound}]
We want to show that for every proof tree the sequent at its root is valid.
By Lemma \ref{lem:FalsifyRoot}, if some valuation falsifies the root, then it falsifies one of the leaves.
But all leaves of a proof tree are axioms, and by Lemma \ref{lem:AxiomValid} they cannot be falsified.
\end{proof}



\end{page}

%%%%%%%%%% output/469--3-2-7-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{7}
\setcounter{dfn}{16}
\label{portion:469}

\subsection{Closed deduction trees and completeness of the sequent calculus}

\end{page}

%%%%%%%%%% output/471--3-2-7-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{7}
\setcounter{dfn}{17}
\label{portion:471}

\begin{dfn}
A deduction tree is called \emph{closed} if the sequents labeling its leaves consist of proposition symbols only.
\end{dfn}

\end{page}

%%%%%%%%%% output/474--3-2-7-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{7}
\setcounter{dfn}{18}
\label{portion:474}

\begin{lem}
Every sequent is the root of some closed deduction tree.
\end{lem}

\end{page}

%%%%%%%%%% output/475--3-2-7-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{7}
\setcounter{dfn}{18}
\label{portion:475}

\begin{proof}
Induction on the number of connectives.

A sequent without connectives consists of proposition symbols.
Assume that every sequent with $< n$ connectives is the root of a closed tree.
Take any sequent $S$ with $n$ connectives, choose the outermost connective in any of its propositions and eliminate it.
This produces a small tree with $S$ at the root.
In every inference rule, each of the premises contains less connectives than the conclusion.
Thus by induction assumption the leaves of our small tree are roots of closed deduction trees.
Together, this produces a closed deduction tree with $S$ at the root.
\end{proof}


\end{page}

%%%%%%%%%% output/477--3-2-7-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{7}
\setcounter{dfn}{19}
\label{portion:477}

\begin{thm}
The sequent calculus is complete: every valid sequent is provable.
\end{thm}

\end{page}

%%%%%%%%%% output/478--3-2-7-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{8}
\setcounter{dfn}{19}
\label{portion:478}

\begin{proof}
For a given sequent, consider its closed deduction tree.
It has leaves of two kinds:
\begin{itemize}
\item
Leaves labeled with axioms $p_1, \ldots, p_k \vdash q_1, \ldots, q_l$ such that $p_i = q_j$ for some $i$, $j$.
\item
Leaves with labels $p_1, \ldots, p_k \vdash q_1, \ldots, q_l$ such that $p_i \ne q_j$ for all $i$, $j$.
These are called counterexample leaves.
\end{itemize}
A counterexample leaf is falsifiable: it suffices to set $v(p_i) = 1$ for all $i$ and $v(q_j) = 0$ for all $j$.
Thus if the closed deduction tree has a counterexample leaf, then the sequent at the root is also falsifiable.

If our sequent is valid, then all of the leaves are of the first kind, and the tree is a proof tree.
\end{proof}

Note that we get more than just completeness: constructing a closed deduction tree provides a concrete counterexample to a falsifiable sequent.



\end{page}

%%%%%%%%%% output/479--3-2-8-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{8}
\setcounter{dfn}{19}
\label{portion:479}

\subsection{A byproduct: CNF and DNF}


\end{page}

%%%%%%%%%% output/481--3-2-8-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{8}
\setcounter{dfn}{20}
\label{portion:481}

\begin{thm}
Let $A$ be a proposition.
Take a closed deduction tree with $\vdash A$ at the root.
For each counterexample leaf $p_1, \ldots, p_k \vdash q_1, \ldots, q_l$ of this tree write a disjunctive clause
\[
\neg p_1 \vee \cdots \vee \neg p_k \vee q_1 \vee \cdots \vee q_l.
\]
Then the conjunction of all such clauses is a CNF for $A$.
\end{thm}

\end{page}

%%%%%%%%%% output/482--3-2-8-other.tex
\begin{page}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{dfn}{20}
\label{portion:482}

\begin{proof}
By Lemma \ref{lem:FalsifyRoot}, a valuation $v$ satisfies $A$ if and only if it satisfies all leaves of the deduction tree.
The leaf $p_1, \ldots, p_k \vdash q_1, \ldots, q_l$ is satisfied if and only if
the disjunctive clause above is satisfied (see Definition \ref{dfn:FalsifySequent}).
Finally, $v$ satisfies all leaves if and only if it satisfies the conjunction of all these clauses.
\end{proof}

One constructs a DNF for $A$ in a similar way starting from the sequent~$A \vdash$.






\end{page}

%%%%%%%%%% output/483--4-0-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{0}
\setcounter{dfn}{20}
\label{portion:483}

\chapter{Predicate logic}
\section*{Introduction}
Predicate logic is a more complicated and powerful system than the propositional logic.

Before proceeding to formal definitions, let us have a glimpse at how it works.
Consider the statement
\begin{quote}
For every number $x$ one has $x < x+1$.
\end{quote}
It can be expressed by a predicate formula
\begin{equation}
\label{eqn:PredForm}
\forall x P(x, f(x)),
\end{equation}
where $f(x) = x+1$, and $P(x,y)$ means $x < y$.
Functions of one or several arguments that take truth values (such as $x < y$ or ``$x$ is blue'') are called \emph{predicates}.
Formula \eqref{eqn:PredForm} contains all the main building blocks of the predicate logic: a variable $x$, a function $f$, and a predicate $P$.

Let us now forget the origin of formula \eqref{eqn:PredForm}.
In order to make sense of it, its elements must be interpreted:
what kind of objects are represented by the variable $x$, how is the function $f$ defined, and what does $P(x,y)$ mean.
This interpretation can be as above, but can also be different.
For example, the same formula can be interpreted as
\begin{quote}
It gets colder every day.
\end{quote}
Now $x$ is a day, $f(x)$ is the day after day $x$, and $P(x,y)$ means ``$y$ is colder than $x$''.

Our first interpretation evaluates the formula \eqref{eqn:PredForm} to true, while the second evaluates it to false.
A formula of the predicate logic is \emph{valid} if it evaluates to true in all interpretations.
Similarly to the propositional logic, one aims at finding a method (a proof theory) to establish the validity of a formula.




\end{page}

%%%%%%%%%% output/484--4-1-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:484}

\section{Syntax and semantics of predicate logic}

\end{page}

%%%%%%%%%% output/485--4-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:485}

\subsection{First-order languages}
The alphabet of the predicate logic consists of
\begin{itemize}
\item
variables;
\item
function symbols;
\item
predicate symbols;
\item
logical connectives $\wedge, \vee, \neg, \to, \forall, \exists$;
\item
auxiliary symbols $($ and $)$;
\item
equality symbol $=$.
\end{itemize}

Informally speaking (and as indicated in the introduction), a variable is an object,
a function is an operation with objects whose result is also an object,
and a predicate is a statement about one or several object (in other words, an operation with objects whose result is a truth value).

Compared to the propositional logic, we have two new logical connectives:
the \emph{universal quantifier} $\forall$ and the \emph{existential quantifier} $\exists$.

The equality symbol is not always included in the alphabet.
Accordingly, there are two slightly different versions of predicate logic: logic with equality and logic without equality.

Before stating the rules according to which the alphabet symbols can be combined one has to fix a \emph{signature}.
This is a list of function symbols and predicate symbols together with the number of arguments for each of them.


\end{page}

%%%%%%%%%% output/487--4-1-1-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:487}

\begin{dfn}
A \emph{signature} is a triple of sets $(\cV, \cF, \cP)$ together with two maps $\cF \to \N \cup \{0\}$ and $\cP \to \N \cup \{0\}$
which describe the \emph{arity} of functions and predicates.
\end{dfn}

\end{page}

%%%%%%%%%% output/488--4-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:488}


One can use the same set $\cV$ of variables in all signatures.
This is a countably infinite set; we use the letters $x$, $y$, $z$, $x_1, x_2, \ldots $ etc. to denote its elements.
As function symbols we will use $f$, $g$, $h$, $f_1, f_2, \ldots$
and as predicate symbols $P$, $Q$, $R$, $P_1, P_2, \ldots$.


The strange word \emph{arity} describes the number of arguments: a function or a predicate can be unary, binary, $k$-ary, and even nullary.
The arity can be indicated by the number of dots (or other placeholders) between the brackets.
Below is an example of a signature.
\begin{center}
\begin{tabular}{ll}
$\cF:\ f(\cdot)$ & \text{one unary function symbol}\\
$\cP:\ P(\cdot, \cdot), Q(\cdot)$ & \text{one binary and one unary predicate symbol}
\end{tabular}
\end{center}
Nullary functions and nullary predicates have no arguments.



\end{page}

%%%%%%%%%% output/490--4-1-1-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:490}

\begin{dfn}
\emph{Terms} are expressions that can be obtained by finitely many applications of the following rules.
\begin{itemize}
\item
Any variable is a term.
\item
If $f$ is a $k$-ary function symbol, and $t_1, \ldots, t_k$ are terms, then $f(t_1, \ldots, t_k)$ is a term.
\end{itemize}
\end{dfn}

\end{page}

%%%%%%%%%% output/491--4-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:491}

In particular, if $f$ is a nullary function, then $f()$ is a term.
We will usually omit the brackets in this situation.
This has one drawback: syntactically, a nullary function becomes indistinguishable from a variable.
(Later we will see that their semantics is different.)

For example, with a ternary function $f$, a binary function $g$, and a $0$-ary function $a$ one can compose the following terms:
\[
f(x, g(y,x), a), \quad g(f(x,y,a), g(x,z)).
\]


\end{page}

%%%%%%%%%% output/493--4-1-1-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:493}

\begin{dfn}
\emph{Formulas} are expressions that can be obtained by finitely many applications of the following rules.
\begin{itemize}
\item
If $P$ is a $k$-ary predicate symbol, and $t_1, \ldots, t_k$ are terms, then $P(t_1, \ldots, t_k)$ is a formula.
\item
If $t_1$ and $t_2$ are terms, then $t_1 = t_2$ is a formula.
\item
If $\phi$ is a formula, then so is $\neg \phi$.
\item
If $\phi$ and $\psi$ are formulas, then so are $\phi \wedge \psi$, $\phi \vee \psi$, $\phi \to \psi$.
\item
If $\phi$ is a formula, and $x$ is a variable, then $\forall x \phi$ and $\exists x \phi$ are formulas.
\end{itemize}
\end{dfn}

\end{page}

%%%%%%%%%% output/494--4-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:494}

Formulas obtained by applying the first or the second rule are called \emph{atomic} formulas (because they cannot be decomposed into smaller formulas).

For example, in the signature
\begin{align*}
\cF:\ &a(), f(\cdot) \\
\cP:\ &P(\cdot, \cdot)
\end{align*}
one can build the following formulas:
\begin{gather*}
\forall x \neg(f(x) = a)\\
\forall x (f(x)=f(y) \to x=y)\\
\forall x P(x, f(x)).
\end{gather*}



\end{page}

%%%%%%%%%% output/496--4-1-1-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:496}

\begin{dfn}
The set of all formulas (based on a given signature) is called a \emph{first-order language}.
\end{dfn}

\end{page}

%%%%%%%%%% output/498--4-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:498}

\subsection{Free and bound variables}
A variable in a predicate formula may be \emph{bound} by a quantifier or be \emph{free}.
For example, in the formula $\forall x P(x,y)$ the variable $x$ is bound, while the variable $y$ is free.

Unfortunately, this is not as simple as it seems.
Consider the formula
\[
(\forall x P(x,y)) \to (\exists y Q(y)).
\]
The variable $y$ in the first half of the formula is free, the same variable $y$ in the second half is bound.
It is more convenient to work with formulas where such things do not happen.


\end{page}

%%%%%%%%%% output/500--4-1-2-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:500}

\begin{dfn}
A formula is called \emph{rectified} if the following two conditions are satisfied:
\begin{itemize}
\item
no variable occurs as free and as bound;
\item
distinct quantifiers bind distinct variables.
\end{itemize}
\end{dfn}

\end{page}

%%%%%%%%%% output/501--4-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:501}


One can rectify any formula by renaming the variables as follows.
\begin{itemize}
\item
For every quantifier in the formula determine its ``scope''.
For this, look at the recursive structure of the formula,
namely at the step when the quantifier appeared: $\phi \rightsquigarrow \forall x \phi$.
The subformula $\phi$ is the scope of $\forall x$.
\item
If the variable $x$ bound by a quantifier occurs not only inside the scope,
but also outside, then replace all occurrences of $x$ inside the scope and under the quantifier by some completely new symbol.
\end{itemize}

The resulting formula is semantically equivalent to the original one (although we have not yet described the semantics).


\end{page}

%%%%%%%%%% output/503--4-1-2-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:503}

\begin{exl}
Rectification of
\[
\exists x \forall y P(x,y) \to \forall y \exists x P(x,y)
\]
produces, for example,
\[
\exists z \forall t P(z,t) \to \forall y \exists x P(x,y).
\]
\end{exl}

\end{page}

%%%%%%%%%% output/506--4-1-2-remark.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:506}

\begin{rem}
Non-rectified formulas are absolutely legitimate.
The main reason to introduce rectification is that
some formal definitions and procedures are easier to describe for rectified formulas than for non-rectified ones.
\end{rem}

\end{page}

%%%%%%%%%% output/508--4-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:508}

\subsection{Closed formulas and universal closure}

\end{page}

%%%%%%%%%% output/510--4-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:510}

\begin{dfn}
A formula is called \emph{closed} if it does not contain free variables.
\end{dfn}

\end{page}

%%%%%%%%%% output/513--4-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:513}

\begin{dfn}
The \emph{universal closure} of a formula is obtained by adding a universal quantifier for every free variable.
\end{dfn}

\end{page}

%%%%%%%%%% output/516--4-1-3-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:516}

\begin{exl}
The universal closure of $Q(x) \wedge \exists y P(y,z)$ is
\[
\forall x \forall z (Q(x) \wedge \exists y P(y,z)).
\]
\end{exl}

\end{page}

%%%%%%%%%% output/518--4-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:518}

\subsection{First-order structures}
A signature only lists symbols of functions and predicates with the number of their arguments;
it does not specify what operations and relations stand behind these symbols.


\end{page}

%%%%%%%%%% output/520--4-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:520}

\begin{dfn}
A \emph{first-order structure} (based on a given signature) is a pair $(U, I)$, where
$U$ is a non-empty set and $I$ is a map which assigns to each $k$-ary function symbol $f$
a function $I(f) \colon U^k \to U$ and to each $k$-ary predicate symbol $P$ a predicate $I(P) \colon U^k \to \{0,1\}$.
\end{dfn}

\end{page}

%%%%%%%%%% output/521--4-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:521}


For $k=0$ the set $U^k$ is a one-element set.
Therefore an interpretation of a nullary function $I(a) \colon U^0 \to U$ is equivalent to choosing an element of $U$.
This is the reason why nullary functions are also called \emph{constants}.
Similarly, an interpretation of a nullary predicate is a choice of a truth value for it.


\end{page}

%%%%%%%%%% output/523--4-1-4-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:523}

\begin{exl}
Consider again the signature
\begin{align*}
\cF:\ &a(), f(\cdot) \\
\cP:\ &P(\cdot, \cdot)
\end{align*}
Here is one of possible structures for it:
\[
U = \N, \quad a = 1, \quad I(f)(x) = x+1, \quad I(P) = \{x \le y\}.
\]
Here $I(P) = \{x \le y\}$ means $I(P)(x,y)$ is true if and only if $x \le y$.
In other words, we define $I(P)$ by describing the preimage of $1$ under the map $I(P)$.
\end{exl}

\end{page}

%%%%%%%%%% output/524--4-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:524}


Given a structure, one can evaluate every closed formula.
For example, the formula $\forall x \neg(f(x) = a)$ acquires the meaning
\[
\forall x \in \N \  x+1 \ne 1,
\]
which we identify as a true statement.

It should be intuitively clear how to find the truth value of a given closed formula.
But one needs a formal definition.
Of course, it proceeds by recursion because formulas have a recursive structure.
This means that, even if we want to determine the truth values only of closed formulas,
we also need to determine the truth values of formulas with free variables (which appear as intermediate steps when building a closed formula).

A \emph{variable assignment} is a map $\mu \colon \cV \to U$ which associated to every variable an element from the universe.
Together with interpretation $I$ it allows to evaluate every term to an element of $U$ and every formula to a truth value.
\begin{itemize}
\item Given terms $t_1, \ldots, t_k$ which evaluate to $u_1, \ldots, u_k \in U$ and a $k$-ary function symbol $f$,
the term $f(t_1, \ldots, t_k)$ evaluates to $I(f)(u_1, \ldots, u_k)$.
\item Similarly, a formula $P(t_1, \ldots, t_k)$ evaluates to $I(P)(u_1, \ldots, u_k)$ if $t_i$ evaluates to $u_i$.
\item Given terms $t_1$ and $t_2$ which evaluate to $u_1$ and $u_2$, the formula $t_1 = t_2$ evaluates to true if and only if $u_1 = u_2$.
\item Formulas of the form $\phi \wedge \psi$, $\phi \vee \psi$, $\phi \to \psi$, $\neg\phi$ evaluate according to the truth tables
for logical connectives.
\item A formula $\exists x \phi$ evaluates to true if there exists an evaluation $\mu'$ that differs from $\mu$ only in the value of $x$
such that $\phi$ evaluates under $\mu'$ to true.
\item A formula $\forall x \phi$ evaluates to true if $\phi$ evaluates to true under all assignments $\mu'$ that differ from $\mu$ only in the value of $x$.
\end{itemize}
One sees that the truth values of $\exists x \phi$ and $\forall x \phi$ do not depend on the assignment value of the variable $x$.
It follows that the truth values of closed formulas are independent of the variable assignments, hence determined by $M = (U,I)$ only.



\end{page}

%%%%%%%%%% output/526--4-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:526}

\begin{dfn}
One says that a structure $M$ \emph{satisfies} a closed formula $\phi$, if $\phi$ evaluates to true under $M$.
In this case $M$ is also called a \emph{model} of $\phi$, and one writes $M \vDash \phi$.
\end{dfn}

\end{page}

%%%%%%%%%% output/529--4-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:529}

\begin{dfn}
A closed formula $\phi$ is called \emph{satisfiable} if it has at least one model.
A closed formula $\phi$ is called \emph{valid} if $M \vDash \phi$ for every structure~$M$.
\end{dfn}

\end{page}

%%%%%%%%%% output/531--4-1-5-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{14}
\label{portion:531}

\subsection{Propositional logic inside predicate logic}
Consider the signature without functions and with nullary predicates only:
\begin{equation}
\label{eqn:NullSign}
\cP:\ P_1(), P_2(), \ldots
\end{equation}
Formulas in this signature contain no terms, because a term must occur as an argument of a predicate, but nullary predicates have no arguments.
We can introduce variables in the formulas only with quantifiers by writing something like $\forall x \exists y P \wedge Q$,
but in any structure this formula evaluates in the same way as $P \wedge Q$.
Thus the formulas in signature \eqref{eqn:NullSign} look like propositional formulas with variable symbols $P_i$.

How does $P \wedge Q$ actually evaluate?
By definition, a first-order structure $(U,I)$ assigns to each nullary predicate $P$ a truth value $I(P)$.
Then the truth value of $P \wedge Q$ is $I(P) \wedge I(Q)$ (and the universe $U$ has no significance).
Similarly for every other formula: an evaluation with respect to interpretation $I$ is the same as evaluation of a propositional formula
with $I$ viewed as valuation $v$.

Thus signature \eqref{eqn:NullSign} realizes the propositional logic as a special case of the predicate logic.
One can state this as follows.


\end{page}

%%%%%%%%%% output/533--4-1-5-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{15}
\label{portion:533}

\begin{thm}
A predicate formula in signature \eqref{eqn:NullSign} is valid if and only if the corresponding propositional formula is a tautology.
\end{thm}

\end{page}

%%%%%%%%%% output/535--4-2-0-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:535}

\section{Proof theory}
We will present Gentzen's sequent calculus for predicate logic.
There are other proof theories, for example Hilbert-style systems.
These theories are equivalent to each other, which means that a formula provable within one of them is also provable within any other.
(Postfactum it follows from their soundness and completeness,
but one can describe a transformation of Hilbert proof into a Gentzen proof and vice versa in a direct way.)



\end{page}

%%%%%%%%%% output/536--4-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:536}

\subsection{Substitutions}
Let $A$ be a predicate formula, and let $t$ be a term.
Assume that $A$ contains a free variable $x$.
Then we can define a new formula $A[t/x]$ obtained by substitution of $t$ for all free occurrences of $x$.
(We don't exclude the possibility that $x$ also occurs bound in $A$.)

Intuitively, $A[t/x]$ should represent a special case of $A$, so that for example if $A$ is true for all $x$, then $A[t/x]$ is valid.
However, some substitutions do not have this property.
Take for example
\[
A = \exists y (y < x), \quad t = y.
\]
Then we have $A[t/x] = \exists y (y < y)$.
This is false in the universe of integers, although $A$ was true.


\end{page}

%%%%%%%%%% output/538--4-2-1-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:538}

\begin{dfn}
A term $t$ is \emph{free for} $x$ in $A$ if no variable of $t$ becomes bound after the substitution of $t$ for all free occurrences of $x$.
\end{dfn}

\end{page}

%%%%%%%%%% output/539--4-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:539}

In other words, when we are substituting $t$ for free occurrences of $x$,
we also want all parts of $t$ to remain free in the resulting formula.

In order to determine the truth value of a non-closed formula $A$, one needs a structure $M = (U,I)$
and an assignment $\mu$ for all free variables in $A$ (that is, $\mu$ is a map from the set of free variables in $A$ to $U$).
A formula $A$ is called satisfiable (respectively, falsifiable) in $M$
if there is an assignment $\mu$ such that $(M, \mu) \vDash A$ (respectively, $(M, \mu) \not\vDash A$).


\end{page}

%%%%%%%%%% output/541--4-2-1-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:541}

\begin{lem}
\label{lem:TermSubstitute}
Assume that a term $t$ is free for $x$ in $A$.
Then for every structure $M$ the following holds:
\begin{itemize}
\item
If $\forall x A$ is satisfiable in $M$, then $A[t/x]$ is satisfiable in $M$.
\item
If $\exists x A$ is falsifiable in $M$, then $A[t/x]$ is falsifiable in $M$.
\end{itemize}
\end{lem}

\end{page}

%%%%%%%%%% output/542--4-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:542}


More specifically, consider substitution in $A$ of a variable $y$ for a free variable $x$.


\end{page}

%%%%%%%%%% output/544--4-2-1-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:544}

\begin{lem}
\label{lem:VarSubstitute}
Assume that a variable $y$ is free for $x$ in $A$ and is not occurring freely in $A$.
Then for every structure $M$ the following holds:
\begin{itemize}
\item
If $\exists x A$ is satisfiable in $M$, then $A[y/x]$ is satisfiable in $M$.
\item
If $\forall x A$ is falsifiable in $M$, then $A[y/x]$ is falsifiable in $M$.
\end{itemize}
\end{lem}

\end{page}

%%%%%%%%%% output/545--4-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:545}


An example showing that $y$ should not occur freely in $A$:
\[
\exists x A = \exists x (P(x) \wedge \neg P(y)).
\]
This formula is satisfiable in any structure where the interpretation of predicate $P$ is not constant.
But after the quantifier removal and substitution of $y$ for $x$ it becomes
\[
A[y/x] = P(y) \wedge \neg P(y),
\]
which is not satisfiable.








\end{page}

%%%%%%%%%% output/546--4-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:546}

\subsection{Inference rules}
Gentzen system for the predicate logic without equality operates with sequents $\Gamma \vdash \Delta$,
where $\Gamma$ and $\Delta$ are sets of predicate formulas.
As before, we are building a deduction tree with the root $\vdash A$,
where $A$ is a formula which we are trying to prove or disprove.


\end{page}

%%%%%%%%%% output/548--4-2-2-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:548}

\begin{dfn}
A sequent $A_1, \ldots, A_m \vdash B_1, \ldots, B_n$ is called \emph{falsifiable}
if there exists a first-order structure $M$ and a variable assignment $\mu$ which simultaneously make all $A_i$ true and all $B_j$ false:
\[
(M,\mu) \vDash A_i \text{ for all }i, \quad (M,\mu) \not\vDash B_j \text{ for all }j.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/549--4-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:549}

% Note that among the formulas $A_i$, $B_j$ some can be non-closed, so it is necessary to define variable assignments for their free variables.
% (We are not taking the universal closure of non-closed formulas in this case.)

Axioms of the Gentzen system are, as before, sequents $\Gamma \vdash \Delta$
with some formula occurring on both sides: $A \in \Gamma \cap \Delta$.
Axioms are valid (because no structure can at the same time satisfy and falsify $A$).
Inference rules for the logical connectives $\wedge, \vee, \to, \neg$ are the same.
There are four new rules involving the quantifiers.

\begin{align*}
&(\forall\text{ \rm left}):\
\AxiomC{$A[t/x], \forall x A, \Gamma \vdash \Delta$}
\UnaryInfC{$\forall x A, \Gamma \vdash \Delta$}
\DisplayProof
&&(\forall\text{ \rm right}):\
\AxiomC{$\Gamma \vdash A[y/x], \Delta$}
\UnaryInfC{$\Gamma \vdash \forall x A, \Delta$}
\DisplayProof
\\
&(\exists\text{ \rm left}):\
\AxiomC{$A[y/x], \Gamma \vdash \Delta$}
\UnaryInfC{$\exists x A, \Gamma \vdash \Delta$}
\DisplayProof
&&(\exists\text{ \rm right}):\
\AxiomC{$\Gamma \vdash A[t/x], \exists x A, \Delta$}
\UnaryInfC{$\Gamma \vdash \exists x A, \Delta$}
\DisplayProof
\end{align*}

Here $t$ is any term free for $x$ in $A$, and $y$ is any variable free for $x$ in $A$ and not occurring freely in the conclusion
(that is, in the sequent $\Gamma \vdash \forall x A, \Delta$ for the right $\forall$ rule and in the sequent $\exists x A, \Gamma \vdash \Delta$
for the left $\exists$ rule).
In particular, one can substitute $x$ for itself if $x$ does not occur freely in $\Gamma$ and~$\Delta$.

As before, a deduction tree is a proof tree if all of its leaves are axioms.
A sequent is called provable if there is a proof tree with this sequent as a root.


\end{page}

%%%%%%%%%% output/551--4-2-2-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:551}

\begin{lem}
\label{lem:InferencePred}
For each of the inference rules, the conclusion is falsifiable in some structure $M$
if and only if at least one of the premises is falsifiable in the structure $M$.
\end{lem}

\end{page}

%%%%%%%%%% output/552--4-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:552}

\begin{proof}
For the inference rules for $\wedge, \vee, \to, \neg$ the argument simply invokes the truth tables.

For the inference rules involving quantifiers this follows from Lemmas \ref{lem:TermSubstitute} and \ref{lem:VarSubstitute}.
\end{proof}


\end{page}

%%%%%%%%%% output/554--4-2-2-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:554}

\begin{thm}
Provable formulas are valid.
In other words, sequent calculus for predicate logic is sound.
\end{thm}

\end{page}

%%%%%%%%%% output/555--4-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:555}

\begin{proof}
Follows from Lemma \ref{lem:InferencePred}.
\end{proof}



\end{page}

%%%%%%%%%% output/557--4-2-2-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:557}

\begin{exl}
Let us prove the formula $\exists x(P \to Q(x)) \to (P \to \exists y Q(y))$.
\begin{prooftree}
\AxiomC{$P \vdash P, \exists y Q(y)$}
\AxiomC{$Q(z_1), P \vdash Q(z_1), \exists y Q(y)$}
\UnaryInfC{$Q(z_1), P \vdash \exists y Q(y)$}
\BinaryInfC{$P, P \to Q(z_1) \vdash \exists y Q(y)$}
\UnaryInfC{$P \to Q(z_1) \vdash P \to \exists y Q(y)$}
\UnaryInfC{$\exists x(P \to Q(x)) \vdash P \to \exists y Q(y)$}
\UnaryInfC{$\vdash \exists x(P \to Q(x)) \to (P \to \exists y Q(y))$}
\end{prooftree}

At the second step we are introducing a new variable $z_1$ while applying the left $\exists$ inference rule.
After application of the branching left $\to$ rule we obtain an axiom on the left (with $P$ on both sides of $\vdash$).
On the right we are applying the right $\exists$ rule, where we can make a clever choice of a term to substitute for $y$.
Choosing $z_1/y$ as the substitution, we obtain two equal terms on both sides of $\vdash$, and the proof tree is finished.
\end{exl}

\end{page}

%%%%%%%%%% output/560--4-2-2-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{8}
\label{portion:560}

\begin{exl}
\label{exl:CounterexamplePred}
The following deduction tree allows us to find a counterexample for $\exists x(P \to Q(x)) \to (P \to \forall y Q(y))$.
\begin{prooftree}
\AxiomC{$P \vdash P, \forall y Q(y)$}
\AxiomC{$Q(z_1), P \vdash Q(z_2)$}
\UnaryInfC{$Q(z_1), P \vdash \forall y Q(y)$}
\BinaryInfC{$P, P \to Q(z_1) \vdash \forall y Q(y)$}
\UnaryInfC{$P \to Q(z_1) \vdash P \to \forall y Q(y)$}
\UnaryInfC{$\exists x(P \to Q(x)) \vdash P \to \forall y Q(y)$}
\UnaryInfC{$\vdash \exists x(P \to Q(x)) \to (P \to \forall y Q(y))$}
\end{prooftree}
\end{exl}

\end{page}

%%%%%%%%%% output/561--4-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:561}


The leaf on the right has a counterexample: take a universe with two elements $z_1$ and $z_2$,
evaluate the nullary predicate $P$ to true, and the unary predicate $Q$ to true on $z_1$ and false on $z_2$.





\end{page}

%%%%%%%%%% output/562--4-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:562}

\subsection{Completeness of the sequent calculus}

\end{page}

%%%%%%%%%% output/564--4-2-3-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:564}

\begin{thm}
\label{thm:SequentCalcComplete}
The sequent calculus is complete: every valid formula is provable.
\end{thm}

\end{page}

%%%%%%%%%% output/565--4-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:565}

We will not give a detailed proof of this theorem, but will describe a procedure which, for every closed predicate formula $A$,
constructs a deduction tree with the root $\vdash A$ with one of the following two properties.
The algorithm assumes that the signature does not contain functions; if it does, some modifications are needed.

\begin{itemize}
\item
It is finite and each of its leaves is an axiom.
\item
It is finite and has a leaf from which a counterexample can be read off.
\item
It contains an infinite path producing a counterexample.
\end{itemize}

One cannot exclude the possibility of an infinite tree because there are formulas satisfied by all finite structures
but falsified by some infinite structure (see the exercises).

\begin{proof}[Algorithm description]
We assume that the signature contains no function symbols.
Given a closed formula $A$, put the sequent $\vdash A$ at the root.

From the set of variables choose an infinite sequence of symbols $u_1, u_2, \ldots, $ not occurring in $A$.
(The set $U = \{u_1, u_2, \ldots\}$ or a finite initial segment of it will later be our universe.)
During the algorithm, the variables from the sequence $u_1, u_2, \ldots$ will be \emph{activated} in their natural order.
At the beginning, only $u_1$ is active, the rest is not.

At every step of the algorithm we have some previously constructed deduction tree
and are going to apply an inference rule to one of its leaves.
Each inference rule consists in copying most of the formulas of the sequent and changing only one of them, called the \emph{principal formula}.
The type of the inference rule is determined uniquely by the principal formula: this is the rule for the outermost connective in the principal formula,
and it is the left or the right rule according on which side of $\vdash$ the principal formula is situated.
For the connectives of propositional logic the inference rules are unique.
For the quantifiers, one needs to specify the substitutions.
This is done as follows.

\begin{itemize}
\item
Right $\forall$ or left $\exists$ rule:
substitute the first inactive variable from the sequence $\{u_1, u_2, \ldots\}$.
\item
Left $\forall$ or right $\exists$ rule:
substitute all active $u_i$ not previously substituted into this formula.
\end{itemize}
That means, if there are several active variables not yet substituted into $\forall \vdash$ or $\vdash \exists$,
then the corresponding inference rule is applied several times.
We abbreviate this sequence of steps as
\begin{prooftree}
\AxiomC{$A[u_{k+1}/x], \ldots, A[u_l/x], \forall x A, \Gamma \vdash \Delta$}
\UnaryInfC{$\forall x A, \Gamma \vdash \Delta$}
\end{prooftree}
(and similarly for $\exists x A$ on the right),
where $u_{k+1}, \ldots, u_l$ are all active variables not substituted into $\forall x A$ at some previous step.

Let us note that if the signature contains function symbols, then in the left $\forall$ and right $\exists$ case one
must substitute all possible terms with currently active variables (this is a finite but possibly quite large set).

The inference rules are applied in a breadth-first way:
in one round we go over all leaves, and every non-atomic formula inside a leaf is used as a principal formula.
More exactly, if $A_1, \ldots, A_m \vdash B_1, \ldots, B_n$ is a leaf sequent,
then we first apply the inference rule with $A_1$ as the principal formula.
After that we apply the inference rule with $A_2$ as the principal formula to all ``newly grown'' leaves.
Then we work with $A_3$ in all new leaves, and so on.
Only after we finished the work with the formula $B_n$, we proceed to the next leaf sequent.

A leaf is called closed if one of the following occurs:
\begin{itemize}
\item
its sequent contains only atomic formulas;
\item
its sequent contains only atomic formulas and $\forall x$-formulas on the left or $\exists x$-formulas on the right
for which all substitutions (of active variables, see the instructions above) have already been performed.
\end{itemize}

A closed leaf with only atomic formulas is either an axiom leaf or a counterexample leaf (see Example \ref{exl:CounterexamplePred}).

Example \ref{exl:CounterexampleFinite} illustrates the second possibility.

It can happen that after every round there are non-closed leaves.
In this case the algorithm never terminates, and the sequent at its root is falsifiable.
Namely, the tree will contain an infinite path, and a counterexample can be read off this path as illustrated in Example \ref{exl:CounterexampleInfinite} below.
\end{proof}


\end{page}

%%%%%%%%%% output/567--4-2-3-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:567}

\begin{exl}
\label{exl:CounterexampleFinite}
Disprove the formula $\forall x \exists y P(x,y)$.
\begin{prooftree}
\AxiomC{$\vdash P(u_2,u_1), P(u_2, u_2), \exists y P(u_2,y)$}
\UnaryInfC{$\vdash \exists y P(u_2,y)$}
\UnaryInfC{$\vdash \forall x \exists y P(x,y)$}
\end{prooftree}

This tree is closed, and its only leaf describes a counterexample:
\[
U = \{u_1, u_2\}, \quad I(P)(u_2,u_1) = I(P)(u_2,u_2) = \text{false}
\]
(the value of $I(P)$ on $(u_1, u_2)$ and $(u_1, u_1)$ is inessential).
\end{exl}

\end{page}

%%%%%%%%%% output/570--4-2-3-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{11}
\label{portion:570}

\begin{exl}
\label{exl:CounterexampleInfinite}
Disprove the formula $\exists x \forall y P(x,y)$.
\begin{prooftree}
\AxiomC{$\cdots$}
\UnaryInfC{$\vdash P(u_1,u_2), P(u_2,u_3), \exists x \forall y P(x,y)$}
\UnaryInfC{$\vdash P(u_1,u_2), \forall y P(u_2,y), \exists x \forall y P(x,y)$}
\UnaryInfC{$\vdash P(u_1,u_2), \exists x \forall y P(x,y)$}
\UnaryInfC{$\vdash \forall y P(u_1,y), \exists x \forall y P(x,y)$}
\UnaryInfC{$\vdash \exists x \forall y P(x,y)$}
\end{prooftree}

The tree is infinite, and it is easy to read off a counterexample:
\[
U = \{u_1, u_2, \ldots\}, \quad I(P)(u_i, u_{i+1}) = \text{false for all }i.
\]
(Again, the values of $I(P)$ on other pairs of arguments do not matter.)
\end{exl}

\end{page}

%%%%%%%%%% output/573--4-2-3-remark.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{12}
\label{portion:573}

\begin{rem}
It is important to work in a breadth-first way, otherwise one can obtain an infinite tree for a valid formula.
This happens, for example, in the following deduction tree for the formula $(P \vee \neg P) \vee \exists x \forall y P(x,y)$:
\begin{prooftree}
\AxiomC{$\cdots$}
\UnaryInfC{$\vdash Q, \neg Q, P(u_1,u_2), P(u_2,u_3), \exists x \forall y P(x,y)$}
\UnaryInfC{$\cdots$}
\UnaryInfC{$\vdash Q, \neg Q, \exists x \forall y P(x,y)$}
\UnaryInfC{$\vdash (Q \vee \neg Q) \vee \exists x \forall y P(x,y)$}
\UnaryInfC{$\vdash (Q \vee \neg Q) \vee \exists x \forall y P(x,y)$}
\end{prooftree}
Here we are neglecting the non-atomic formula $\neg Q$ and working with $\exists x \forall y P(x,y)$ only,
which produces an infinite path from Example \ref{exl:CounterexampleInfinite}.
Constructed in a breadth-first way, this tree will close with an axiom leaf with $Q$ on both sides of $\vdash$.
\end{rem}

\end{page}

%%%%%%%%%% output/574--4-2-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{dfn}{12}
\label{portion:574}


For details see \cite{Kleene}.




\end{page}

%%%%%%%%%% output/575--4-3-0-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:575}

\section{G\"odel's incompleteness theorems}

\end{page}

%%%%%%%%%% output/576--4-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:576}

\subsection{Theories and models}
Very often, we will deal with closed predicate formulas.
For brevity, a closed formula will be called a \emph{sentence}.


\end{page}

%%%%%%%%%% output/578--4-3-1-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:578}

\begin{dfn}
A (first-order) \emph{theory} is any set of sentences of the predicate logic.
These sentences are called \emph{axioms} of the theory.
\end{dfn}

\end{page}

%%%%%%%%%% output/581--4-3-1-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:581}

\begin{exl}
\label{exl:FirstGraphTheory}
The theory consisting of the following two sentences:
\begin{gather*}
\forall x (\neg P(x,x))\\
\forall x \forall y (P(x,y) \to P(y,x))
\end{gather*}
is called \emph{first-order graph theory}.
Any model of this theory is a graph:
the universe corresponds to the vertex set, and the predicate interpretation to the edge set.

\end{exl}

\end{page}

%%%%%%%%%% output/584--4-3-1-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:584}

\begin{dfn}
A theory $T$ is called \emph{satisfiable} if it has at least one model,
that is a structure which satisfies all sentences of the theory:
\[
M \vDash T \stackrel{\mathrm{def}}{\Longleftrightarrow} M \vDash A \text{ for all }A \in T.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/587--4-3-1-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:587}

\begin{exl}
The theory $\{A, \neg A\}$, where $A$ is any sentence, is unsatisfiable.
Indeed, in every model one of the sentences $A, \neg A$ takes the true value, the other one the false value.
\end{exl}

\end{page}

%%%%%%%%%% output/590--4-3-1-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{5}
\label{portion:590}

\begin{exl}
Every model $M$ defines a theory $T_M = \{A \mid M \vDash A\}$, the set of all sentences satisfied by the model $M$.
\end{exl}

\end{page}

%%%%%%%%%% output/591--4-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{5}
\label{portion:591}

The theory $T_M$ has the property that for every $A$ either $A \in T_M$ or $\neg A \in T_M$.



\end{page}

%%%%%%%%%% output/593--4-3-1-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{6}
\label{portion:593}

\begin{dfn}
Two models $M$ and $M'$ are called \emph{elementarily equivalent} (denoted $M \sim M'$) if $T_M = T_{M'}$, that is
every sentence satisfied by one is also satisfied by the other.
\end{dfn}

\end{page}

%%%%%%%%%% output/596--4-3-1-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{7}
\label{portion:596}

\begin{dfn}
A theory $T$ is called \emph{complete} if it is satisfiable and any two models satisfying it are elementarily equivalent:
\[
M \vDash T, M' \vDash T' \Rightarrow M \sim M'.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/599--4-3-1-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{8}
\label{portion:599}

\begin{exl}
First-order graph theory \ref{exl:FirstGraphTheory} is satisfiable but incomplete.
The sentence $\exists x \exists y (x \ne y)$ is false in the model with one-element universe (graph with a single vertex) and true in any other model.
\end{exl}

\end{page}

%%%%%%%%%% output/602--4-3-1-remark.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{9}
\label{portion:602}

\begin{rem}
One can show that two finite models are elementarily equivalent if and only if the corresponding graphs are isomorphic.
But this is false for infinite models: a doubly infinite path is elementarily equivalent to the union of two of its copies.

The connectivity property of a graph cannot be written as a sentence of the first-order logic.
One can express it with the help of quantifiers over subsets of the universe, which belong already to the second-order logic.
\end{rem}

\end{page}

%%%%%%%%%% output/605--4-3-1-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{10}
\label{portion:605}

\begin{dfn}
A sentence $A$ is said to be a \emph{semantic consequence} of a theory $T$ (denoted $T \vDash A$) if every model of $T$ satisfies $A$:
\[
T \vDash A \stackrel{\mathrm{def}}{\Longleftrightarrow} \text{if } M \vDash T, \text{ then } M \vDash A.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/608--4-3-1-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{11}
\label{portion:608}

\begin{exl}
A semantic consequence of an empty theory is a valid sentence (one which is true in all structures).
If $A$ is valid, then we write $\vDash A$.
\end{exl}

\end{page}

%%%%%%%%%% output/611--4-3-1-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{12}
\label{portion:611}

\begin{lem}
A theory $T$ is complete if and only if it is satisfiable and for every sentence $A$ one has either $T \vDash A$ or $T \vDash \neg A$.
\end{lem}

\end{page}

%%%%%%%%%% output/612--4-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{12}
\label{portion:612}

\begin{proof}
Assume $T$ is complete, and let $M$ be any of its models.
For every sentence $A$, we have either $M \vDash A$ or $M \vDash \neg A$.
Due to the completeness of $T$ this implies $T \vDash A$, respectively $T \vDash \neg A$.

Assume $T$ is incomplete. Then there are two models $M$ and $M'$ whose sets of satisfied formulas differ.
Without loss of generality, let $M \vDash A$ and $M' \not\vDash A$.
Then neither $T \vDash A$ nor $T \vDash \neg A$ hold.
\end{proof}


\end{page}

%%%%%%%%%% output/614--4-3-1-exercise.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{13}
\label{portion:614}

\begin{exc}
For every model $M$, the theory $T_M$ is complete.
\end{exc}

\end{page}

%%%%%%%%%% output/615--4-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{13}
\label{portion:615}


The first-order theory of graphs is incomplete, and this is good: one has many different graphs with different properties.
But if we want to know everything about the arithmetics of $\N$, the natural numbers, then we need a complete theory.
The theory should contain some axioms for addition and multiplication and maybe some other axioms,
so that every sentence has a well-defined truth value
(that is, all structures that satisfy our axioms assign to it the same value).
For example, the question ``Can every integer greater than $2$ be expressed as the sum of two primes?'' should have a definite answer.
%If our set of axioms is incomplete, then there is a sentence which is true in one model and false in another model.

One might use the result of the above exercise: if we assume that $\N$ is something objectively real,
then there is the corresponding complete theory $T_{\N}$.
But this theory has ``too many'' axioms, and we have no explicit description for it, apart from saying ``what is true, is true''.
%An obvious requirement to a theory is that it should be possible to describe its axioms in some explicit way.
G\"odel's incompleteness theorem says that there is no complete theory of $\N$
such that for every sentence one can decide in finite time is it an axiom of the theory or not.

Before we say more about the incompleteness theorem, we must discuss proofs, a constructive way to derive consequences of a theory.




\end{page}

%%%%%%%%%% output/616--4-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{13}
\label{portion:616}

\subsection{Models and proofs: G\"odel's completeness theorem}

\end{page}

%%%%%%%%%% output/618--4-3-2-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{14}
\label{portion:618}

\begin{dfn}
A sentence $A$ is said to be a \emph{syntactic consequence} of a theory $T$ (denoted $T \vdash A$) if
there is a deduction tree with $\Gamma \vdash A$ at the root, where $\Gamma \subset T$ is any finite subset, and logical axioms at the leaves.
One says that $A$ is a \emph{theorem} of theory $T$.
The set of all theorems of $T$ is denoted by $\Th(T)$.
\end{dfn}

\end{page}

%%%%%%%%%% output/621--4-3-2-exercise.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{15}
\label{portion:621}

\begin{exc}
If $T \vdash A$ and $T' \supset T$, then $T' \vdash A$.
\end{exc}

\end{page}

%%%%%%%%%% output/624--4-3-2-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{16}
\label{portion:624}

\begin{exl}
The definition of a group can easily be formulated as a first-order theory.
It is also possible to state and to prove the following theorem:
If all elements of a group have order two, then the group is commutative.
\end{exl}

\end{page}

%%%%%%%%%% output/627--4-3-2-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{17}
\label{portion:627}

\begin{lem}
A syntactic consequence is a semantic consequence.
That is, $T \vdash A$ implies $T \vDash A$.
\end{lem}

\end{page}

%%%%%%%%%% output/628--4-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{17}
\label{portion:628}

\begin{proof}
Proof by contradiction.
Assume that there is a model $M$ of $T$ which does not satisfy $A$.
Then this model falsifies the sequent $\Gamma \vdash A$, and therefore falsifies one of the leaves of the deduction tree.
But this is impossible, thus every model that satisfies $T$ also satisfies $A$.
\end{proof}



\end{page}

%%%%%%%%%% output/630--4-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{18}
\label{portion:630}

\begin{thm}[G\"odel's completeness theorem]
\label{thm:Complete1}
A semantic consequence of a theory is also its syntactic consequence:
\[
T \vDash A \Rightarrow T \vdash A.
\]
In other words, every sentence which is true in a given theory has a formal proof.
\end{thm}

\end{page}

%%%%%%%%%% output/631--4-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{18}
\label{portion:631}


A special case of this theorem is Theorem \ref{thm:SequentCalcComplete}, which says that $\vDash A$ implies $\vdash A$
(here the theory is empty).


The completeness theorem can be formulated in a different way.


\end{page}

%%%%%%%%%% output/633--4-3-2-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{19}
\label{portion:633}

\begin{dfn}
A theory $T$ is called (syntactically) \emph{consistent} if there is no sentence $A$ such that $T \vdash A$ and $T \vdash \neg A$.
\end{dfn}

\end{page}

%%%%%%%%%% output/636--4-3-2-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{20}
\label{portion:636}

\begin{lem}
\label{lem:SatConsist}
Every satisfiable theory is consistent.
\end{lem}

\end{page}

%%%%%%%%%% output/637--4-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{20}
\label{portion:637}

\begin{proof}
Proof by contraposition.
Let $T$ be an inconsistent theory: there is a sentence $A$ such that $T \vdash A$ and $T \vdash \neg A$.
Assume that $T$ is satisfiable: $M \vDash T$.
Then the soundness of the proof system implies that $M \vDash A$ and $M \vDash \neg A$, which is a contradiction.
Thus every inconsistent theory is unsatisfiable.
\end{proof}

The inverse implication is also correct; it can be derived from the completeness theorem and vice versa by some manipulations with deduction trees.
Therefore it is also called the completeness theorem.


\end{page}

%%%%%%%%%% output/639--4-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{21}
\label{portion:639}

\begin{thm}[G\"odel's completeness theorem, second version]
\label{thm:Complete2}
Every consistent theory is satisfiable.
\end{thm}

\end{page}

%%%%%%%%%% output/640--4-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{21}
\label{portion:640}

% 

\end{page}

%%%%%%%%%% output/642--4-3-2-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{22}
\label{portion:642}

% \begin{lem}[Deduction theorem]
% \label{lem:DedThm}
% For any theory $T$ and sentences $A$ and $B$, if $T \cup \{A\} \vdash B$, then $T \vdash (A \to B)$.
% \end{lem}

\end{page}

%%%%%%%%%% output/643--4-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{22}
\label{portion:643}

% \begin{proof}
% If $T \cup A \vdash B$, then there is a deduction tree with $\Gamma \vdash B$ at the root.
% If $\Gamma \ni A$, then we can attach under the root the sequent $\Gamma \setminus \{A\} \vdash A \to B$, which produces a deduction tree
% for $A \to B$ from $\Gamma \setminus \{A\}$.
% If $\Gamma \not\ni A$, then add $A$ to the antecedent of all sequents in the tree.
% Inference rules remain inference rules, axiom leaves remain axiom leaves, and we are in the situation considered earlier.
% \end{proof}
% 
% 
% 

\end{page}

%%%%%%%%%% output/645--4-3-2-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{23}
\label{portion:645}

% \begin{lem}
% \label{lem:ThmNotConsist}
% A sentence $A$ is a theorem of $T$ if and only if $T \cup \{\neg A\}$ is not consistent.
% \end{lem}

\end{page}

%%%%%%%%%% output/646--4-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{23}
\label{portion:646}

% \begin{proof}
% If $T \vdash A$, then $T \cup \{\neg A\} \vdash A$ and $T \cup \{\neg A\} \vdash \neg A$.
% Thus $T \cup \{\neg A\}$ is not consistent.
% 
% In the opposite direction, if $T \cup \{\neg A\}$ is not consistent, then we have
% \[
% T \cup \{\neg A\} \vdash B, \neg B \vdash A
% \]
% (because $(B \wedge \neg B) \to A$ is a tautology).
% By Lemma \ref{lem:DedThm}, $T \cup \{\neg A\} \vdash A$ implies $T \vdash (\neg A \to A)$.
% Since $(\neg A \to A) \to A$ is a tautology, we get $T \vdash A$.
% \end{proof}
% 
% \begin{proof}[Proof of equivalence Theorem \ref{thm:Complete1} $\Leftrightarrow$ \ref{thm:Complete2}]
% If $A$ is not a theorem of $T$, then $T \cup \{\neg A\}$ is consistent (Lemma \ref{lem:ThmNotConsist}), thus satisfiable (Lemma \ref{lem:SatConsist}).
% Thus there is a model for $T$ in which $A$ is false.
% \end{proof}




\end{page}

%%%%%%%%%% output/648--4-3-2-corollary.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{24}
\label{portion:648}

\begin{cor}
A theory $T$ is complete if and only if it is consistent and syntactically complete,
that is for every sentence $A$ either $T \vdash A$ or $T \vdash \neg A$ but not both.
\end{cor}

\end{page}

%%%%%%%%%% output/650--4-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{24}
\label{portion:650}

\subsection{Peano arithmetic}
The signature consists of a nullary function $0$, a unary function $s$ (successor), two binary functions $+$ and $\cdot$.
The equality predicate $=$.
Axioms of $PA$:
\begin{enumerate}
\item
$\forall x \neg(s(x) = 0)$
\item
$\forall x \forall y (s(x)=s(y) \to x=y)$
\item
$\forall x (x = 0 \vee \exists y (s(y)=x))$
\item
$\forall x (x+0=x)$
\item
$\forall x \forall y (x+s(y) = s(x+y))$
\item
$\forall x (x \cdot 0 = 0)$
\item
$\forall x \forall y (x \cdot s(y) = x \cdot y + x)$
\item
$\forall \bar y \left((A(0, \bar y) \wedge \forall x (A(x, \bar y) \to A(s(x), \bar y)) \to \forall x A(x, \bar y)\right)$
\end{enumerate}
The last item is the \emph{induction schema}, that is it encodes infinitely many sentences.
Here $\bar y$ denotes $y_1, \ldots, y_n$, and $\forall \bar y$ denotes $\forall y_1 \ldots \forall y_n$.
The number $n$ can be any non-negative integer, and $A$ can be any formula with $n+1$ free variables $x, y_1, \ldots, y_n$.


The theory consisting of the first seven axioms is called Robinson arithmetic, we will denote it by $PA_0$.
It does not imply that the addition is commutative.

Since we assume that $\N$ is an objective reality and the operations with positive integers satisfy the above properties,
theory $PA$ is satisfiable and hence consistent.






\end{page}

%%%%%%%%%% output/651--4-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{24}
\label{portion:651}

\subsection{Recursive functions and recursive sets}

\end{page}

%%%%%%%%%% output/653--4-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{25}
\label{portion:653}

\begin{dfn}
A \emph{recursive function} is a function $f \colon X \to \N$ defined on a subset $X$ of $\N^p$ for some $p$
which can be computed in finite time.
\end{dfn}

\end{page}

%%%%%%%%%% output/654--4-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{25}
\label{portion:654}

The above definition is highly informal.
The formal definition says that a recursive function is one that can be obtained from basic functions
(constants, $x \mapsto x+1$, projections) by finitely many operations such as composition,
primitive recursion (which allows to construct $f(x,y) = x+y$, for example), and minimization.
(Functions which can be obtained without using minimization are called primitive recursive functions.)
It can be shown that recursive functions are exactly those which can be computed by Turing machines.
The input of a machine is a $p$-tuple of positive integers $x = (x_1, \ldots, x_p)$;
if $x \in X$, then the machine outputs $f(x)$, if $x \notin X$, then the machine outputs an error or never stops.
(Without loss of generality, one can assume that for $x \notin X$ the machine never stops: if one has a machine that outputs an error,
attach to it a machine which reacts on this error by starting an infinite loop.)



\end{page}

%%%%%%%%%% output/656--4-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{26}
\label{portion:656}

\begin{dfn}
A set $X \subset \N^p$ is called \emph{recursive} if the function
\[
\mathbf{1}_X \colon \N^p \to \N, \quad \mathbf{1}_X(x) =
\begin{cases}
1, \text{ if }x \in X\\
0, \text{ if }x \notin X
\end{cases}
\]
is recursive.
\end{dfn}

\end{page}

%%%%%%%%%% output/657--4-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{26}
\label{portion:657}

In other words, a set $X$ is recursive if there is an algorithm which for every input $x$ terminates in finite time and tells whether $x$ belongs to $X$ or not.


\end{page}

%%%%%%%%%% output/659--4-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{27}
\label{portion:659}

\begin{dfn}
A set $X \subset \N^p$ is called \emph{recursively enumerable} if the function
\[
\overline{\mathbf{1}}_X \colon X \to \N, \quad \overline{\mathbf{1}}_X(x) = 1 \text{ for all }x \in X
\]
is recursive.
\end{dfn}

\end{page}

%%%%%%%%%% output/660--4-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{27}
\label{portion:660}

In other words, $X$ is recursively enumerable if there is an algorithm which terminates only when the input $x$ belongs to $X$.

Recursive sets are also called decidable, and recursively enumerable are called semi-decidable.


\end{page}

%%%%%%%%%% output/662--4-3-4-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{28}
\label{portion:662}

\begin{exl}
The set of prime numbers is recursive.
It is not easy to give an example of a recursively enumerable but not recursive set.
\end{exl}

\end{page}

%%%%%%%%%% output/665--4-3-4-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{29}
\label{portion:665}

\begin{lem}
A set $X \subset \N^p$ is recursive if and only if both $X$ and $\N^p \setminus X$ are recursively enumerable.
\end{lem}

\end{page}

%%%%%%%%%% output/666--4-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{29}
\label{portion:666}

\begin{proof}
If we have an algorithm for computing $\mathbf{1}_X$, then modify it by going into an endless loop if the output is $0$.
This computes the function $\overline{\mathbf{1}}_X$. The function $\overline{\mathbf{1}}_{\N^p \setminus X}$ is computed similarly.

If one has an algorithm which stops only for inputs $x \in X$ and an algorithm which stops only for inputs $x \notin X$, then run them parallelly
and interpret the output of the first algorithm as $1$, and the output of the second algorithm as $0$.
This allows us to compute the function $\mathbf{1}_X$.
\end{proof}



\end{page}

%%%%%%%%%% output/668--4-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{30}
\label{portion:668}

\begin{dfn}
Let $B$ be a formula of the predicate logic in the signature of Peano arithmetic with exactly one free variable $y$.
One says that $B$ \emph{represents} a subset $Y \subset \N$ if
\[
n \in Y \text{ if and only if }PA_0 \vdash B[\underline{n}/y],
\]
where $\underline{n} = \underbrace{s \circ s \circ \cdots \circ s}_{n\text{ times}}(0)$.
\end{dfn}

\end{page}

%%%%%%%%%% output/671--4-3-4-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{31}
\label{portion:671}

\begin{thm}
\label{thm:ReprThm}
Every recursive subset of $\N$ is representable.
\end{thm}

\end{page}

%%%%%%%%%% output/673--4-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{31}
\label{portion:673}

\subsection{G\"odel's incompleteness theorems}
Denote by $L$ the language of $PA$ (that is, the set of all formulas in the signature of $PA$).
There is an injective map
\[
\sharp \colon L \to \N
\]
called \emph{G\"odel numbering} such that its image $\sharp(L) \subset \N$ is a recursive set.
(Such a map can be constructed by associating to every symbol a number (similarly ASCII encoding) and then encoding a sequence of numbers by a single number
for example through $(k_1, \ldots, k_n) \mapsto p_1^{k_1} \cdots p_n^{k_n}$, where $p_i$ is the $i$-th prime number.)
That the image of this map is recursive means that there is an algorithm which determines for any number $m \in \N$ whether it encodes a well-formed formula.
It is clear how to reconstruct a sequence of symbols from its G\"odel number, and it is clear how to check whether a sequence of symbols is a formula.

The G\"odel numbering can be extended to sequences of formulas:
\[
\sharp\sharp \colon L^* \to \N.
\]
A proof of a formula can be represented as a sequence of formulas
(this is so in the Hilbert proof system, for the Gentzen system one has to agree how to transform a tree into a list; this is doable).
Thus every proof has a G\"odel number as well, with different numbers corresponding to different proofs (and some numbers not corresponding to any proof).

G\"odel numbering allows to speak about recursive sets of formulas.

Let $\Th(T)$ denote the set of all theorems of theory $T$.


\end{page}

%%%%%%%%%% output/675--4-3-5-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{32}
\label{portion:675}

\begin{dfn}
A theory $T$ is called \emph{recursive} if the set
\[
\sharp T = \{\sharp A \mid A \in T\} \subset \N
\]
is recursive.

A theory $T$ is called \emph{decidable} if the set
\[
\sharp \Th(T) = \{\sharp A \mid A \in \Th(T)\} \subset \N
\]
is recursive.
\end{dfn}

\end{page}

%%%%%%%%%% output/676--4-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{32}
\label{portion:676}


For a recursive theory there is an algorithm which, for any given formula, tells us in finite time whether this formula is an axiom of our theory or not.
This is a very reasonable condition for a theory: one should be able to distinguish axioms from non-axioms.
For a decidable theory there is an algorithm which, for any given formula, tells us whether this formula can be derived from the axioms or not.


\end{page}

%%%%%%%%%% output/678--4-3-5-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{33}
\label{portion:678}

\begin{lem}
If theory $T$ is recursive, then its set of theorems $\Th(T)$ is recursively enumerable.
\end{lem}

\end{page}

%%%%%%%%%% output/679--4-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{33}
\label{portion:679}

Intuitively, if we can distinguish axioms from non-axioms, then the set of proofs is recursive.
From certain properties of recursive sets it follows that the set of last terms of proofs is recursively enumerable.



\end{page}

%%%%%%%%%% output/681--4-3-5-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{34}
\label{portion:681}

\begin{thm}
A complete and recursive theory is decidable.
\end{thm}

\end{page}

%%%%%%%%%% output/682--4-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{34}
\label{portion:682}

\begin{proof}
By the lemma, the set $\Th(T)$ is recursively enumerable, so that it remains to show that its complement is recursively enumerable.
But in a complete theory, $T \not\vdash A$ if and only if $T \vdash \neg A$.
Thus, if we use $\neg A$ as an input for the algorithm which stops if and only if its input is a theorem, then this algorithm will stop if and only if
$A$ is not a theorem.
\end{proof}



\end{page}

%%%%%%%%%% output/684--4-3-5-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{35}
\label{portion:684}

\begin{thm}
\label{thm:ConstUndecid}
Let $T \supset PA_0$ be a consistent theory. Then $T$ is undecidable.
In particular, $PA_0$ and $PA$ are undecidable.
\end{thm}

\end{page}

%%%%%%%%%% output/685--4-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{35}
\label{portion:685}


Observe that this theorem together with the previous one immediately imply


\end{page}

%%%%%%%%%% output/687--4-3-5-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{36}
\label{portion:687}

\begin{thm}[First G\"odel's incompleteness theorem]
Every consistent and recursive theory which contains $PA_0$ is incomplete.
\end{thm}

\end{page}

%%%%%%%%%% output/688--4-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{36}
\label{portion:688}


\begin{proof}[Proof of Theorem \ref{thm:ConstUndecid}]
Assume that $T$ is decidable.
Consider the set
\[
X = \{(m,n) \mid m = \sharp A(\cdot), T \vdash A[\underline{n}/\cdot]\} \subset \N^2.
\]
(Here $A(\cdot)$ means that $A$ has one free variable, and $A[t/\cdot]$ means substitution of term $t$ for this variable.)
Since $T$ is decidable, the set $X$ is recursive (there is an algorithm which decides whether $(m,n) \in X$ or not).

Now let
\[
Y = \{n \mid (n,n) \notin X\}.
\]
Clearly, since $X$ is recursive, so is $Y$.
By Theorem \ref{thm:ReprThm}, the set $Y$ is representable.
That is, there is a formula $B(y)$ such that
\begin{gather*}
n \in Y \Rightarrow PA_0 \vdash B[\underline{n}/y]\\
n \notin Y \Rightarrow PA_0 \vdash \neg B[\underline{n}/y]
\end{gather*}
Since $T \supset PA_0$, for every $C$ such that $PA_0 \vdash C$ we also have $T \vdash C$.

Now let $n = \sharp B(y)$.
Let us ask ourselves if $n \in Y$ or not. Assume $n \in Y$.
By definition of $Y$, $X$, and $n$ we have
\[
n \in Y \Rightarrow (n,n) \notin X \Rightarrow T \not\vdash B[\underline{n}/y] \Rightarrow T \vdash \neg B[\underline{n}/y].
\]
On the other hand, by definition of $B(y)$ we have
\[
n \in Y \Rightarrow T \vdash B[\underline{n}/y],
\]
which implies that $T$ is incoherent.
If we assume that $n \notin Y$, then we derive similarly that $T \vdash B[\underline{n}/y]$ and $T \vdash \neg B[\underline{n}/y]$.

Thus the decidability assumption contradicts the consistency assumption, and the theorem is proved.
\end{proof}



\end{page}

%%%%%%%%%% output/690--4-3-5-corollary.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{37}
\label{portion:690}

\begin{cor}
The set $V = \{A \mid A \text{ is valid}\}$ is recursively enumerable but not recursive.
\end{cor}

\end{page}

%%%%%%%%%% output/691--4-3-5-other.tex
\begin{page}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{dfn}{37}
\label{portion:691}

\begin{proof}
The set $V$ is the set of theorems of the empty theory. Since the empty theory is recursive, $V$ is recursively enumerable.

Denote by $B$ the conjunction of all seven axioms of $PA_0$.
If $V$ is recursive, then for any sentence $A$ one can decide in finite time whether $B \to A$ belongs to $V$ or not.
But an answer to this question is equivalent to an answer to the question whether $A \in \Th(PA_0)$ or not.
This contradicts the fact that $PA_0$ is undecidable, thus $V$ cannot be recursive.
\end{proof}




For details see \cite{CL1,CL2}.





\end{page}

%%%%%%%%%% output/692--5-0-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{0}
\setcounter{dfn}{37}
\label{portion:692}

\chapter{Combinatorics II}
Additional reading for this part of the course: \cite{Aig07}.


\end{page}

%%%%%%%%%% output/693--5-1-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:693}

\section{Linear recursive sequences}

\end{page}

%%%%%%%%%% output/694--5-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:694}

\subsection{Fibonacci sequence and Binet formula}
The \emph{Fibonacci sequence} is defined as follows:
\[
a_1 = 1, \quad a_2 = 1, \quad a_n = a_{n-1} + a_{n-2} \text{ for }n > 2.
\]
Here are the first several terms:
\[
1,\ 1,\ 2,\ 3,\ 5,\ 8,\ 13,\ 21,\ 34,\ 55,\ 89,\ \ldots
\]


\end{page}

%%%%%%%%%% output/696--5-1-1-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:696}

\begin{thm}[Binet]
\label{thm:Binet}
The $n$-th Fibonacci number is equal to
\[
a_n = \frac{\left(\frac{1+\sqrt{5}}2\right)^n - \left(\frac{1-\sqrt{5}}2\right)^n}{\sqrt{5}}
\]
\end{thm}

\end{page}

%%%%%%%%%% output/697--5-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:697}


At the first sight it is not clear at all that this formula produces a rational and even integer number.

A direct consequence of the Binet formula is that the Fibonacci sequence grows as a geometric progression with ratio
equal to the golden ratio
\[
\tau = \frac{1+\sqrt{5}}2 = 1,618\ldots.
\]
Indeed, the second summand is
\[
\left(\frac{1-\sqrt{5}}2\right)^n = (-0,618\ldots)^n \to 0 \text{ as } n \to \infty,
\]
which implies that
\[
a_n \sim \frac{\tau^n}{\sqrt{5}},
\]
and even more exactly
\[
a_n = \left\lfloor \frac{\tau^n}{\sqrt{5}} \right\rceil,
\]
where $\lfloor x \rceil$ denotes the closest integer to the number $x$.



\end{page}

%%%%%%%%%% output/698--5-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:698}

\subsection{Linear recursive sequences of order 2}
We will prove not only the Binet formula, but show that similar formulas hold for all sequences defined similarly to the Fibonacci sequence.


\end{page}

%%%%%%%%%% output/700--5-1-2-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:700}

\begin{dfn}
A sequence $a_n$, $n = 0, 1, \ldots$, is called a \emph{linear recursive sequence of order 2} if
it satisfies the relation
\begin{equation}
\label{eqn:LinRec2}
a_n = ra_{n-1} + sa_{n-2} \text{ for all } n \ge 2
\end{equation}
for some constant coefficients $r$ and $s$, where $s \ne 0$.
\end{dfn}

\end{page}

%%%%%%%%%% output/701--5-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:701}

Such a sequence is uniquely determined by the values of $a_0$ and $a_1$.


\end{page}

%%%%%%%%%% output/703--5-1-2-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:703}

\begin{dfn}
The \emph{characteristic polynomial} of the sequence \eqref{eqn:LinRec2} is the quadratic polynomial
\[
x^2 - rx - s.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/706--5-1-2-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:706}

\begin{thm}
\begin{enumerate}
\item
If the characteristic polynomial of \eqref{eqn:LinRec2} has two different roots $\lambda_1$, $\lambda_2$,
then one has
\[
a_n = c_1 \lambda_1^n + c_2 \lambda_2^n
\]
for some $c_1, c_2 \in \R$.
\item
If the characteristic polynomial of \eqref{eqn:LinRec2} has one (double) root $\lambda$,
then one has
\[
a_n = c_1 \lambda^n + c_2 n \lambda^n
\]
for some $c_1, c_2 \in \R$.
\end{enumerate}
\end{thm}

\end{page}

%%%%%%%%%% output/707--5-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:707}

\begin{proof}
Part 1. Let us show that each of the two geometric progressions $\lambda_1^n$ and $\lambda_2^n$ satisfies
the recurrence relation \eqref{eqn:LinRec2}.
Indeed, one has
\[
\lambda_i^n - r \lambda_i^{n-1} - s \lambda_i^{n-2} = \lambda_i^{n-2}(\lambda_i^2 - r \lambda_i - s) = 0,
\]
which implies $\lambda_i^n = r \lambda_i^{n-1} + s \lambda_i^{n-2}$.
It follows that for any constant coefficients $c_1$ and $c_2$ the sequence $c_1\lambda_1^n + c_2\lambda_2^n$
also satisfies the relation \eqref{eqn:LinRec2}.

Let us show that the coefficients $c_1$ and $c_2$ can be chosen so that
\begin{gather*}
c_1  + c_2 = a_0\\
c_1 \lambda_1 + c_2 \lambda_2 = a_1.
\end{gather*}
This is a system of two linear equations for two unknowns $c_1$ and $c_2$,
which has a (unique) solution because the matrix of the system has a non-zero determinant:
\[
\begin{vmatrix}
1 & 1\\
\lambda_1 & \lambda_2
\end{vmatrix}
= \lambda_2 - \lambda_1 \ne 0.
\]
Once the sequences $c_1\lambda_1^n + c_2\lambda_2^n$ and $a_n$ coincide in the first two terms,
they coincide everywhere. This proves the first part of the Theorem.


Part 2. The proof is similar, but as the basis sequences we take $\lambda^n$ and $n\lambda^n$.
The first of them satisfies the linear recurrence by the same reason as in Part 1.
To check the recurrence for the second sequence, observe that $\lambda$ being the double root of the characteristic polynomial
means that
\[
x^2 - rx - s = (x-\lambda)^2 \Rightarrow r = 2\lambda,\, s = -\lambda^2.
\]
Thus we have
\begin{multline*}
n\lambda^n - r(n-1)\lambda^{n-1} - s(n-2)\lambda^{n-2} = \lambda^{n-2}(n\lambda^2 - 2\lambda(n-1)\lambda + \lambda^2(n-2))\\
= \lambda^n(n - 2(n-1) + (n-2)) = 0.
\end{multline*}
As next one has to find the coefficients $c_1$ and $c_2$ which make the linear combination $c_1\lambda^n + c_2 n\lambda^n$
to coincide with the sequence $a_n$ in the first two terms:
\begin{gather*}
c_1 = a_0\\
c_1 \lambda + c_2 \lambda = a_1.
\end{gather*}
Clearly, this linear system has a solution.
\end{proof}

Let us apply the algorithm from the above proof to find an explicit formula for Fibonacci numbers.

The characteristic polynomial is $\lambda^2 - \lambda - 1$. Its roots are
\begin{equation}
\label{eqn:FibRoots}
\lambda_1 = \frac{1 + \sqrt{5}}2, \quad \lambda_2 = \frac{1 - \sqrt{5}}2.
\end{equation}
If we start the Fibonacci sequence from the zeroth term so that the recurrence relation holds between $a_0$, $a_1$, $a_2$ as well, 
then we must put $a_0 = 0$.
Thus the coefficients $c_1$ and $c_2$ are found from the system
\begin{gather*}
c_1 + c_2 = 0\\
c_1\lambda_1 + c_2\lambda_2 = 1.
\end{gather*}
From the first equation one has $c_2 = -c_1$. Substituting this into the second equation one obtains
\[
c_1 = \frac{1}{\lambda_1 - \lambda_2} = \frac{1}{\sqrt{5}}.
\]
The result is the formula of Binet:
\[
a_n = \frac{1}{\sqrt{5}} \lambda_1^n - \frac{1}{\sqrt{5}} \lambda_2^n.
\]



\end{page}

%%%%%%%%%% output/708--5-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:708}

\subsection{Linear recursive sequences of higher order}
Linear recursive sequences of higher orders are defined similarly and can be handled in a similar way.

\end{page}

%%%%%%%%%% output/710--5-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:710}

\begin{dfn}
A sequence $a_0, a_1, a_2, \ldots$ is called a \emph{linear recursive sequence of order} $k$
if it satisfies the relation
\begin{equation}
\label{eqn:LinReck}
a_n = r_1 a_{n-1} + r_2 a_{n-2} + \cdots + r_k a_{n-k}
\end{equation}
for all $n \ge k$ for some constants $r_1, \ldots, r_k$, where $r_k \ne 0$.
\end{dfn}

\end{page}

%%%%%%%%%% output/711--5-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:711}

A linear recursive sequence of order $k$ is completely determined by the values of its first $k$ terms.


\end{page}

%%%%%%%%%% output/713--5-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:713}

\begin{dfn}
The \emph{characteristic polynomial} of the sequence \eqref{eqn:LinReck} is
\[
P(x) = x^k - r_1 x^{k-1} - r_2 x^{k-2} - \cdots - r_k.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/716--5-1-3-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:716}

\begin{thm}
Let $P(x) = (x-\lambda_1)^{k_1} \cdots (x-\lambda_m)^{k_m}$ be the complete factorization of the characteristic polynomial
of a linear recursive sequence $a_n$.
Then the sequence $a_n$ is a linear combination of the sequences $n^j \lambda_i^n$, $0 \le j \le k_i - 1$.
\end{thm}

\end{page}

%%%%%%%%%% output/717--5-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{7}
\label{portion:717}

We don't give the proof, which is similar to the case of sequences of order 2.
If all roots of $P(x)$ are distinct, then the coefficients $c_i$ in
\[
a_n = c_1 \lambda_1^n + \cdots + c_k \lambda_k^n
\]
are found by solving a system of $k$ linear equations with $k$ unknowns.
If there are multiple roots, some work should be done in order to prove that the sequences $n^j \lambda_i^n$ satisfy
the recursive relation \eqref{eqn:LinReck}.



\end{page}

%%%%%%%%%% output/718--5-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{7}
\label{portion:718}

\subsection{The case of complex roots}
Back to the case of a recursive sequence of order 2, what happens if the characteristic polynomial
\[
x^2 - rx - s
\]
has no real roots? This happens when the discriminant $D = r^2 + 4s$ is negative.
Then it has two complex roots
\[
\lambda_{1,2} = \frac{r \pm \sqrt{D}}2,
\]
and all arguments remain valid: the geometric progressions $\lambda_i^n$ solve the recurrence and every sequence satisfying the recurrence
can be written as a linear combination of these two progressions.

The two complex roots are conjugate to each other:
\[
\lambda_1 = \lambda, \quad \lambda_2 = \overline{\lambda}.
\]
A linear combination of the progressions $\lambda^n$ and $\overline{\lambda}^n$ takes real values only if the coefficients are conjugate:
\[
a_n = c \lambda^n + \overline{c} \overline{\lambda}^n.
\]

Let us write the roots in the exponential form:
\[
\lambda = \rho e^{i\phi} = \rho (\cos\phi + i \sin\phi), \quad \overline{\lambda} = \rho e^{-i\phi} = \rho (\cos\phi - i\sin\phi).
\]
Then we have
\begin{multline*}
c \lambda^n + \overline{c} \overline{\lambda}^n
= \rho^n(ce^{in\phi} + \overline{c}e^{-in\phi})\\
= \rho^n(c(\cos n\phi + i\sin n\phi) + \overline{c}(\cos n\phi - i\sin n\phi))\\
= \rho^n((c+\overline{c}) \cos n\phi + i(c-\overline{c}) \sin n\phi)
\end{multline*}
That is, every recursive sequence is a linear combination with real coefficients of the following two sequences:
\[
\rho^n \cos n\phi \quad \text{and} \quad \rho^n \sin n\phi.
\]

Consider the special case $\rho = 1$.
Then the recursive relation has the form
\[
a_n = r a_{n-1} - a_{n-2},
\]
where $r = 2 \cos\phi$.
If $\phi$ is a rational multiple of $\pi$, then this sequence will be periodic independently of the initial values $a_0, a_1$.
For example, this is the case of the recurrence
\[
a_n = a_{n-1} - a_{n-2}
\]
Here $\phi = \frac{\pi}{3}$, and the sequence will have period $6$.
(This is also easy to check by iterating the recurrence relation and writing $a_2, a_3, \ldots$ in terms of $a_0$ and $a_1$.)
If $\phi$ is not a rational multiple of $\pi$, then the sequence will not be periodic.
It will fill densely some interval.
This is the case of the recurrence
\[
a_n = \frac{a_{n-1}}2 - a_{n-2}.
\]




\end{page}

%%%%%%%%%% output/719--5-1-5-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{7}
\label{portion:719}

\subsection{An application of the Binet formula}

\end{page}

%%%%%%%%%% output/721--5-1-5-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{5}
\setcounter{dfn}{8}
\label{portion:721}

\begin{thm}
The square of every Fibonacci number differs from the product of its left and right neighbors by $1$.
For example,
\[
3^2 = 2 \cdot 5 - 1, \quad 5^2 = 3 \cdot 8 + 1, \quad 8^2 = 5 \cdot 13 - 1.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/722--5-1-5-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{0}
\setcounter{dfn}{8}
\label{portion:722}

This and many other relations between Fibonacci numbers can be proved by induction,
sometimes in a not very straightforward way.
When the Binet formula is used, the proof consists of simple algebraic manipulations.

\begin{proof}
We have $a_n = \frac{1}{\sqrt{5}}(\lambda_1^n - \lambda_2^n)$ with $\lambda_i$ as in \eqref{eqn:FibRoots}.
Taking into account that $\lambda_1\lambda_2 = -1$, one computes
\[
a_n^2 = \frac15(\lambda_1^{2n} - 2 \lambda_1^n \lambda_2^n + \lambda_2^{2n}) = \frac15(\lambda_1^{2n} + \lambda_2^{2n} - 2(-1)^n).
\]
On the other hand,
\begin{multline*}
a_{n-1} a_{n+1} = \frac15(\lambda_1^{n-1} - \lambda_2^{n-1})(\lambda_1^{n+1} - \lambda_2^{n+1})\\
= \frac15(\lambda_1^{2n} - \lambda_1^{n-1}\lambda_2^{n+1} - \lambda_2^{n-1}\lambda_1^{n+1} + \lambda_2^{2n})\\
= \frac15(\lambda_1^{2n} + \lambda_2^{2n} - \lambda_1^{n-1}\lambda_2^{n-1}(\lambda_1^2 + \lambda_2^2))
\end{multline*}
One computes
\[
\lambda_1^2 + \lambda_2^2 = \frac{1 + 2\sqrt{5} + 5}4 + \frac{1 - 2\sqrt{5} + 5}4 = 3,
\]
which implies
\[
a_{n-1} a_{n+1} = \frac15(\lambda_1^{2n} + \lambda_2^{2n} - 3(-1)^{n-1}) = a_n^2 + (-1)^n.
\]
\end{proof}





\end{page}

%%%%%%%%%% output/723--5-2-0-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:723}

\section{Generating functions}


\end{page}

%%%%%%%%%% output/724--5-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{0}
\label{portion:724}

\subsection{Fibonacci again}
\label{sec:FibAgain}
Take the Fibonacci sequence
\[
(a_0, a_1, a_2, a_3, \ldots) = (0, 1, 1, 2, \ldots)
\]
and write a power series
\[
A(x) = \sum_{k=0}^\infty a_k x^k = a_0 + a_1 x + a_2 x^2 + \cdots.
\]

Because of
\begin{align*}
xA(x)  = a_0x + & a_1x^2 + a_2x^3 + \cdots\\
x^2A(x)  = & a_0x^2 + a_1x^3 + \cdots
\end{align*}
we have
\begin{multline*}
xA(x) + x^2A(x) = a_0 x + (a_1+a_0)x^2 + (a_2+a_1)x^3 + \cdots\\
= a_2x^2 + a_3x^3 + \cdots = A(x) - a_0 - a_1 x = A(x) - x.
\end{multline*}
This implies
\[
A(x)(1-x-x^2) = x \Rightarrow A(x) = \frac{x}{1-x-x^2}.
\]
(At the moment it is not clear what this equation means and why can we perform with the power series $A(x)$ the above algebraic manipulations.
A justification will be given later. Now let us continue to do whatever looks reasonable.)

We claim that there are real numbers $A, B$ such that
\[
\frac{x}{1-x-x^2} = \frac{x}{(1-\lambda_1x)(1-\lambda_2x)} = \frac{A}{1-\lambda_1x} + \frac{B}{1-\lambda_2x}
\]
Here $\lambda_1 = \frac{1+\sqrt{5}}2$, and $\lambda_2 = \frac{1-\sqrt{5}}2$.

The numbers $A$ and $B$ can be found by a smart guess:
\begin{multline*}
\frac{x}{(1-\lambda_1x)(1-\lambda_2x)} =
\frac{1}{\lambda_1 - \lambda_2} \frac{(1-\lambda_2x) - (1-\lambda_1x)}{(1-\lambda_1x)(1-\lambda_2x)}\\
= \frac{1}{\sqrt{5}} \left( \frac{1}{1-\lambda_1x} - \frac{1}{1-\lambda_2x} \right)
\end{multline*}
Or they can be found by writing down a system of linear equations:
\begin{multline*}
\frac{x}{(1-\lambda_1x)(1-\lambda_2x)} = \frac{A}{1-\lambda_1x} + \frac{B}{1-\lambda_2x}\\
= \frac{A(1-\lambda_2x) + B(1-\lambda_1x)}{(1-\lambda_1x)(1-\lambda_2x)}
= \frac{(A+B) - (A\lambda_2 + B\lambda_1)x}{(1-\lambda_1x)(1-\lambda_2x)}\\
\Rightarrow \begin{cases} A+B = 0\\ A\lambda_2 + B\lambda_1 = -1 \end{cases}
\end{multline*}

Anyway, we have
\[
A(x) = \frac{x}{1-x-x^2} = \frac{1}{\sqrt{5}} \left( \frac{1}{1-\lambda_1x} - \frac{1}{1-\lambda_2x} \right).
\]
Now, from the formula for geometric progression
\[
\frac{1}{1-y} = 1 + y + y^2 + y^3 + \cdots
\]
by substituting $y = \lambda x$ we get
\[
\frac{1}{1 - \lambda x} = 1 + \lambda x + \lambda^2 x^2 + \lambda^3 x^3 + \cdots.
\]
Thus we have
\[
A(x) = \frac{1}{\sqrt{5}} \left( \sum_{k=0}^\infty \lambda_1^kx^k - \sum_{k=0}^\infty \lambda_2^kx^k \right)
= \sum_{k=0}^\infty \frac{\lambda_1^k - \lambda_2^k}{\sqrt{5}} x^k,
\]
which means that
\[
a_k = \frac{\lambda_1^k - \lambda_2^k}{\sqrt{5}},
\]
the Binet formula again.



\end{page}

%%%%%%%%%% output/725--5-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{0}
\label{portion:725}

\subsection{Operations with formal power series}
\label{sec:OperFPS}
There are two ways of interpreting the calculations we made above.
The first approach is by viewing $x$ as a real number close to $0$ such that the power series $\sum_{k=0}^\infty a_k x^k$ converges.
Then some theorems from calculus ensure that all our operations were correct.
The second approach is to deal with $\sum_{k=0}^\infty a_k x^k$ as a formal expression,
to define operations with such expressions, and to show that these operations satisfy all of the usual algebraic properties.
We choose the second approach: \emph{formal power series}.


\end{page}

%%%%%%%%%% output/727--5-2-2-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:727}

\begin{dfn}
Let $A(x) = \sum_{k=0}^\infty a_k x^k$ and $B(x) = \sum_{k=0}^\infty b_k x^k$ be two formal power series.
Their \emph{sum} is the formal power series
\[
A(x) + B(x) = \sum_{k=0}^\infty (a_k + b_k)x^k,
\]
and their \emph{product} is the formal power series
\[
A(x)B(x) = \sum_{k=0}^\infty c_k x^k,
\]
where
\[
c_k = a_0b_k + a_1b_{k-1} + \cdots + a_kb_0 = \sum_{i=0}^k a_i b_{k-i}.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/728--5-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:728}

The formula for the product comes from expanding the brackets in
\[
(a_0 + a_1 x + a_2 x^2 + \cdots)(b_0 + b_1 x + b_2 x^2 + \cdots).
\]
It can easily be checked that the sum and the product defined above satisfy the usual rules, such as
\[
A(x)(B(x) + C(x)) = A(x)B(x) + A(x)C(x)
\]
etc.


\end{page}

%%%%%%%%%% output/730--5-2-2-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:730}

\begin{dfn}
A formal power series $B(x)$ is called (a multiplicative) \emph{inverse} of $A(x)$ if
\[
A(x)B(x) = 1 = 1 + 0 \cdot x + 0 \cdot x^2 + \cdots.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/733--5-2-2-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:733}

\begin{lem}
\label{lem:MultInverse}
Every formal power series $A(x) = \sum_{k=0}^\infty a_k x^k$ such that $a_0 \ne 0$
has a unique multiplicative inverse.
\end{lem}

\end{page}

%%%%%%%%%% output/734--5-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:734}

\begin{proof}
The equation $A(x)B(x) = 1$ consists of an infinite sequence of equations
\begin{gather*}
a_0b_0 = 1\\
a_0b_1 + a_1b_0 = 0\\
a_0b_2 + a_1b_1 + a_2b_0 = 0\\
\cdots
\end{gather*}
with unknowns $b_0, b_1, \ldots$.
The first equation implies $b_0 = \frac{1}{a_0}$ (which is defined because $a_0 \ne 0$).
Knowing $b_0$ we can express $b_1$ from the second equation:
\[
b_1 = -\frac{a_1b_0}{a_0}
\]
and continue in the same spirit, because $(k+1)$-st equation can be solved for $b_k$:
\[
b_k = -\frac{1}{a_0}\sum_{i=1}^k a_ib_{k-i}.
\]
This shows that the inverse series $B(x)$ exists and is unique.
\end{proof}

We denote the inverse series to $A(x)$ by $(A(x))^{-1}$ or $\frac{1}{A(x)}$.


\end{page}

%%%%%%%%%% output/736--5-2-2-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:736}

\begin{exl}
One has
\[
(1-x)^{-1} = 1 + x + x^2 + \cdots,
\]
as the brackets expansion
\[
(1-x)(1+x+x^2+\cdots) = 1 + (x - x) + (x^2 - x^2) + \cdots = 1
\]
shows.
\end{exl}

\end{page}

%%%%%%%%%% output/739--5-2-2-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:739}

\begin{dfn}
For $A(x) = \sum_{k=0}^\infty a_k x^k$ and $B(x) = \sum_{k=0}^\infty b_k x^k$ define the \emph{composition} as
\[
A(B(x)) = \sum_{k=0}^\infty a_k (B(x))^k.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/740--5-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:740}

The composition is not always well-defined. Indeed, we have
\[
A(B(x)) = a_0 + a_1(b_0 + b_1 x + b_2 x^2 + \cdots) + a_2(b_0 + b_1 x + b_2 x^2 + \cdots)^2 + \cdots.
\]
In particular, the constant term is an infinite sum $a_0 + a_1 b_0 + a_2 b_0^2 + \cdots$, which is not good.
However, if $b_0 = 0$, then we have
\[
a_k (B(x))^k = a_k (b_1 x + b_2 x^2 + \cdots)^k = a_k b_1^k x^k + \text{ higher order terms},
\]
so that the coefficient at $x^k$ in $A(B(x))$ is a finite expression in $a_i, b_j$.
Thus we arrive to the following conclusion.


\end{page}

%%%%%%%%%% output/742--5-2-2-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:742}

\begin{lem}
The composition $A(B(x))$ is well-defined if $b_0 = 0$.
\end{lem}

\end{page}

%%%%%%%%%% output/743--5-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:743}

(Note that it is also well-defined for any value of $b_0$ provided that $A(x)$ is a polynomial, but we are not going to need this.)

The composition is compatible with sum and product:
\begin{gather*}
\text{if } C(x) = A(x) + B(x), \text{ then } C(D(x)) = A(D(x)) + B(D(x)),\\
\text{if } C(x) = A(x)B(x), \text{ then } C(D(x)) = A(D(x))B(D(x)).
\end{gather*}

This implies in particular that for every formal power series $B(x)$ with $b_0 = 0$ one has
\[
(1 - B(x))^{-1} = 1 + B(x) + (B(x))^2 + \cdots.
\]


\end{page}

%%%%%%%%%% output/745--5-2-2-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:745}

\begin{exl}
One has
\[
(1 + x)^{-1} = 1 - x + x^2 - \cdots,
\]
\[
(1 - \lambda x)^{-1} = 1 + \lambda x + \lambda^2 x^2 + \cdots,
\]
\[
(1 - x^2)^{-1} = 1 + x^2 + x^4 + \cdots.
\]
\end{exl}

\end{page}

%%%%%%%%%% output/746--5-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:746}



The definition and lemmas of this section give a meaning to the manipulations done in Section \ref{sec:FibAgain}.




\end{page}

%%%%%%%%%% output/747--5-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:747}

\subsection{Linear recursive sequences and partial fraction decomposition}

\end{page}

%%%%%%%%%% output/749--5-2-3-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:749}

\begin{dfn}
The formal power series
\[
a_0 + a_1 x + a_2 x^2 + \cdots = \sum_{k=0}^\infty a_k x^k
\]
is called the \emph{generating function} of the sequence $a_0, a_1, a_2, \ldots$.
\end{dfn}

\end{page}

%%%%%%%%%% output/752--5-2-3-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:752}

% \begin{dfn}
% A \emph{rational function} is a quotient of two polynomials $\frac{P(x)}{Q(x)}$.
% \end{dfn}

\end{page}

%%%%%%%%%% output/753--5-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:753}

% By Lemma \ref{lem:MultInverse} if the constant coefficient of $Q(x)$ is non-zero,
% then $\frac{P(x)}{Q(x)}$ can be written as a formal power series.


\end{page}

%%%%%%%%%% output/755--5-2-3-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:755}

\begin{thm}
The generating function of a sequence satisfying the linear recursion
\[
a_n = r_1 a_{n-1} + r_2 a_{n-2} + \cdots + r_k a_{n-k}, \quad n \ge k
\]
can be written in the form $\frac{B(x)}{\overline{P}(x)}$, where $B(x)$ is some polynomial, and
\[
\overline{P}(x) = 1 - r_1 x - r_2 x^2 - \cdots - r_k x^k.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/756--5-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:756}


\begin{proof}
Exercise.
% One computes
% \begin{multline*}
% \overline{P}(x)A(x) =
% (1 - r_1 x - r_2 x^2 - \cdots - r_k x^k)(a_0 + a_1x + a_2x^2 + \cdots)\\
% \sum_{i=0}^{k-1} (a_i - r_1 a_{i-1} - \cdots - r_i a_0) x^i + \sum_{n=k}^\infty x^n(a_n - r_1 a_{n-1} - \cdots - r_k a_{n-k}) = B(x).
% \end{multline*}
% Thus we have
% \[
% A(x) = \frac{B(x)}{\overline{P}(x)},
% \]
% where $B(x)$ is a polynomial of degree at most $k-1$.
\end{proof}

Note that $\overline{P}(x)$ is related to the characteristic polynomial $P(x)$ through
\[
\overline{P}(x) = x^k P\left(\frac1x\right).
\]
It follows that the roots of the polynomial $\overline{P}(x)$ are reciprocals of the roots of $P(x)$.
More exactly, if $P(x) = (x - \lambda_1)^{k_1} \cdots (x - \lambda_m)^{k_m}$, then
\[
\overline{P}(x) = (1 - \lambda_1 x)^{k_1} \cdots (1- \lambda_m)^{k_m}.
\]

The following theorem generalizes our representation of the fraction $\frac{x}{1-x-x^2}$ as a sum of two simpler fractions.

\end{page}

%%%%%%%%%% output/758--5-2-3-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{11}
\label{portion:758}

\begin{thm}[Partial fraction decomposition]
\begin{enumerate}
\item
Let $\lambda_1, \ldots, \lambda_k \in \R$, and let $B(x)$ be a polynomial of degree $< k$.
Then there are real numbers $c_1, \ldots, c_k$ such that
\[
\frac{B(x)}{(1-\lambda_1 x) \cdots (1-\lambda_k x)} = \frac{c_1}{1-\lambda_1 x} + \cdots + \frac{c_k}{1-\lambda_k x}
\]
\item
Let $\lambda_1, \ldots, \lambda_m \in \R$, $k_1, \ldots, k_m \in \N$ such that $k_1 + \cdots + k_m = k$,
and let $B(x)$ be a polynomial of degree $< k$.
Then there are real numbers $c_{ij}$, $1 \le i \le m$, $1 \le j \le k_i$ such that
\[
\frac{B(x)}{(1-\lambda_1 x)^{k_1} \cdots (1-\lambda_m x)^{k_m}} = \sum_{i=1}^m \sum_{j=1}^{k_i} \frac{c_{ij}}{(1-\lambda_i x)^j}
\]
\end{enumerate}
\end{thm}

\end{page}

%%%%%%%%%% output/761--5-2-3-remark.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{dfn}{12}
\label{portion:761}

\begin{rem}
Note that if $\deg B \ge k$, then we can divide $B(x)$ by the denominator with remainder:
\[
B(x) = Q(x) (x-\lambda_1)^{k_1} \cdots (x-\lambda_m)^{k_m} + R(x), \quad \deg R < k,
\]
and then apply the theorem to a fraction with $R$ in place of $B$.
\end{rem}

\end{page}

%%%%%%%%%% output/762--5-2-3-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:762}


We don't give a proof of the above theorem, but there is a simple algorithm that allows to compute the coefficients $c_i$, respectively $c_{ij}$.
Bring the equation to a common denominator, it becomes an equation between the numerators.
The numerators are polynomials; they are equal if and only if their corresponding coefficients are equal.
This yields a system of linear equations on the unknowns $c_i$ (respectively $c_{ij}$).

Because of
\[
\frac{1}{1-\lambda_i x} = 1 + \lambda_i x + \lambda_i^2 x^2 + \cdots
\]
the first part of the above theorem implies that every linear recursive sequence has the form $a_n = \sum_i c_i \lambda_i^n$,
if the characteristic polynomial has only simple roots $\lambda_i$.
In the case of multiple roots we need to represent the quotient $\frac{1}{(1-\lambda x)^j}$ as a formal power series.



\end{page}

%%%%%%%%%% output/763--5-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:763}

\subsection{Generalized binomial theorem}


\end{page}

%%%%%%%%%% output/765--5-2-4-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:765}

\begin{thm}
\label{thm:BinomNeg}
For every positive integer $n$ one has
\begin{equation}
\label{eqn:BinomNeg}
(1+x)^{-n} = \sum_{k=0}^\infty \binom{-n}{k} x^k,
\end{equation}
where
\[
\binom{-n}{k} = \frac{-n \cdot (-n-1) \cdot \ldots \cdot (-n-k+1)}{k!}.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/766--5-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:766}


What do we mean by $(1+x)^{-n}$?
This is a power series $A(x)$ such that $A(x) (1+x)^n = 1$.


\end{page}

%%%%%%%%%% output/768--5-2-4-remark.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:768}

\begin{rem}
Observe that
\begin{multline*}
\binom{-n}{k} = (-1)^k \frac{(n+k-1) (n+k-2) \cdot \ldots \cdot n}{k!}\\
= (-1)^k \binom{n+k-1}{k} = (-1)^k \binom{n+k-1}{n-1}.
\end{multline*}
Therefore, by substituting $-x$ instead of $x$ one can rewrite \eqref{eqn:BinomNeg} as
\[
(1-x)^{-n} = \sum_{k=0}^\infty \binom{n+k-1}{n-1} x^k.
\]
\end{rem}

\end{page}

%%%%%%%%%% output/769--5-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:769}


Theorem \ref{thm:BinomNeg} will follow from a more general theorem below.
Now let us just check it for a special case $n=2$:
\begin{multline*}
(1-x)^{-2} = (1-x)^{-1} (1-x)^{-1} = (1 + x + x^2 + \cdots)(1 + x + x^2 + \cdots)\\
= 1 + (x \cdot 1 + 1 \cdot x) + (x^2 + x \cdot x + x^2) + \cdots
= 1 + 2x + 3x^2 + \cdots
\end{multline*}


\end{page}

%%%%%%%%%% output/771--5-2-4-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{15}
\label{portion:771}

\begin{dfn}
For every $\alpha \in \R$ and every non-negative integer $k$ define the \emph{generalized binomial coefficient} $\binom{\alpha}{k}$ as
\[
\binom{\alpha}{k} = \frac{\alpha(\alpha - 1) \cdots (\alpha - k + 1)}{k!}.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/772--5-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{15}
\label{portion:772}

For $k = 0$ the products in the denominator and in the numerator are empty, therefore we have $\binom{\alpha}{k} = \frac{1}{1} = 1$.


\end{page}

%%%%%%%%%% output/774--5-2-4-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{16}
\label{portion:774}

\begin{thm}[Vandermonde's identity]
\label{thm:Vandermonde}
For every $\alpha, \beta \in \R$ the following holds:
\[
\binom{\alpha + \beta}{k} = \sum_{i=0}^k \binom{\alpha}{i} \binom{\beta}{k-i}.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/775--5-2-4-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{16}
\label{portion:775}


A straightforward consequence of this is the following.


\end{page}

%%%%%%%%%% output/777--5-2-4-corollary.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{dfn}{17}
\label{portion:777}

\begin{cor}
\label{cor:GenBinom}
For every $\alpha \in \R$ put by definition
\[
(1 + x)^\alpha = \sum_{k=0}^\infty \binom{\alpha}{k} x^k
\]
(which for $\alpha \in \N$ agrees with the binomial formula, so that this definition does not override the usual definition of $(1+x)^n$).
Then for every $\alpha, \beta \in \R$ the following identity between formal power series holds:
\[
(1+x)^\alpha (1+x)^\beta = (1+x)^{\alpha + \beta}.
\]
In particular, for every $p \in \Z$ and $q \in \N$ one has
\[
\left( (1+x)^{\frac{p}{q}} \right)^q = (1+x)^p.
\]
\end{cor}

\end{page}

%%%%%%%%%% output/778--5-2-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{dfn}{17}
\label{portion:778}


It follows that one can extract $q$-th root from any formal power series with non-zero constant term.

\begin{proof}[Proof of Theorem \ref{thm:Vandermonde}]
Step 1. If $\alpha = m$ and $\beta = n$ are positive integers, then
\[
\binom{m + n}{k} = \sum_{i=0}^k \binom{m}{i} \binom{n}{k-i}
\]
can be proved by a combinatorial argument: $\binom{m+n}{k}$ is the number of different choices of $k$ elements from the set $\{1, \ldots, m+n\}$.
To choose $k$ elements, one has to choose $i$ elements among $\{1, \ldots, m\}$ and $k-i$ elements among $\{m+1, \ldots, m+n\}$ for some $i$ between $0$ and $k$.
The number of such choices is $\binom{m}{i} \binom{n}{k-i}$.
Summing over $i$ we obtain the desired formula.

Step 2. Let us prove
\[
\binom{\alpha + n}{k} = \sum_{i=0}^k \binom{\alpha}{i} \binom{n}{k-i}
\]
for all $\alpha \in \R$ and all positive integers $n$.
For fixed $n$ and $k$ the left hand side of the above formula is a polynomial in $\alpha$ of degree $k$;
the right hand side is also a polynomial in $\alpha$ of degree $k$.
Both polynomials take equal values at $\alpha = m$ for all positive integers $m$.
It follows that the polynomials are identical
(if two polynomials of degree $k$ coincide at $k+1$ points, then their difference is a polynomial of degree $\le k$ with $> k$ roots, hence identically zero).

Step 3. Finally let us prove
\[
\binom{\alpha + \beta}{k} = \sum_{i=0}^k \binom{\alpha}{i} \binom{\beta}{k-i}
\]
for all $\alpha, \beta \in \R$.
Fix $\alpha \in \R$ and $k \in \Z_{\ge 0}$. Then the left and the right hand sides are polynomials in $\beta$ of degree $k$.
By Step 2, the values of these polynomials coincide whenever $\beta$ is a positive integer.
Thus the polynomials are identically equal.
(In particular, evaluating the left hand side and the right hand side for any values of $\alpha, k, \beta$ leads to the same results.)
\end{proof}






\end{page}

%%%%%%%%%% output/779--5-3-0-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:779}

\section{Partition of integers}
In this section we will count the number of decompositions of a positive integer into summands.
There are different versions of this problem.
Before coming to the most difficult and interesting one in Section \ref{subsec:Partitions} we look at simpler versions.




\end{page}

%%%%%%%%%% output/780--5-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:780}

\subsection{Money changing problem}
Imagine a country where only $9$, $17$, and $31$ dollar banknotes are in circulation.
In how many different ways can one pay $1000$ dollars without change?

The problem can be reformulated as finding the number of integer solutions of the equation
\[
9k + 17l + 31m = 1000, \quad k, l, m \ge 0
\]
More generally, denote by $a_n$ the number of different ways to pay $n$ dollars without change.
What can one say about $a_n$?


\end{page}

%%%%%%%%%% output/782--5-3-1-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:782}

\begin{thm}
\label{thm:Dollars}
The generating function of the sequence $a_n$ has the following form:
\[
\sum_{n=0}^\infty a_n x^n = \frac{1}{(1-x^9)(1-x^{17})(1-x^{31})}.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/783--5-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:783}

\begin{proof}
The right hand side is equal to
\begin{multline*}
(1 + x^9 + x^{18} + \cdots)(1 + x^{17} + x^{34} + \cdots)(1 + x^{31} + x^{62} + \cdots)\\
= \sum_{k,l,m\ge 0} x^{9k+17l+31m}.
\end{multline*}
(We pick $x^{9k}$ from the first brackets, $x^{17l}$ from the second brackets, and $x^{31m}$ from the third brackets).
Thus the coefficient at $x^n$ is the number of solutions of the equation
\[
9k + 17l + 31m = n, \quad k, l, m \ge 0,
\]
that is $a_n$.
\end{proof}

One can represent the quotient $\frac{1}{(1-x^9)(1-x^{17})(1-x^{31})}$ as the sum of partial fractions.
For this one has to factorize $1-x^9$.
Complex roots of unity will appear.
It is ultimately possible (but very time-consuming) to write a closed formula for the number of ways to change $n$ dollars.
What is easier to prove is the asymptotics of the number $a_n$:
\[
a_n \sim \frac{n^2}{9 \cdot 17 \cdot 31} = \frac{n^2}{4743}, \text{ that is } \lim_{n \to 0} \frac{a_n}{n^2} = \frac{1}{4743}.
\]





\end{page}

%%%%%%%%%% output/784--5-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:784}

\subsection{Compositions again}
Recall that a weak composition of a number $n$ is a representation of $n$ as a sum of non-negative integers.
(One counts ordered sums: $5 = 2 + 3$ and $5 = 3 + 2$ are different compositions.)
We have computed the number of weak compositions in Section \ref{sec:Comp} by the ``stones and sticks'' method.
Let us do it again with generating functions.

Fix a positive integer $k$ and denote by $a_n$ the number of weak compositions of $n$ from $k$ parts.

\end{page}

%%%%%%%%%% output/786--5-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:786}

\begin{thm}
The generating function of the sequence $a_n$ has the following form:
\[
\sum_{n=0}^\infty a_n x^n = \frac{1}{(1-x)^k}.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/787--5-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{2}
\label{portion:787}

\begin{proof}
One has
\[
\frac{1}{(1-x)^k} = \underbrace{(1+x+x^2+\cdots)\cdots(1+x+x^2+\cdots)}_{k}
\]
When one expands the brackets in the product on the right hand side,
one picks from the first brackets a monomial $x^{m_1}$, from the second $x^{m_2}$ and so on up to $x^{m_k}$ from the last brackets.
The product of these monomials is $x^{m_1+\cdots+m_k}$.
When one collects the monomials of degree $n$, one obtains a term $a_n x^n$,
where $n$ is the number of solutions of the equation
\[
m_1 + \cdots + m_k = n,
\]
where the unknowns $m_1, \ldots, m_k$ can take only non-negative integer values.
The number of solutions is exactly the number of weak compositions of $n$ from $k$ parts.
\end{proof}

Now, by applying the generalized binomial theorem one obtains
\[
\sum_{n=0}^\infty a_n x^n = \frac{1}{(1-x)^k} = (1-x)^{-k} = \sum_{n=0}^\infty (-1)^n \binom{-k}{n} x^n
\]
Thus we have
\[
a_n = (-1)^n \binom{-k}{n} = \binom{n+k-1}{n} = \binom{n+k-1}{k-1}.
\]




\end{page}

%%%%%%%%%% output/788--5-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{2}
\label{portion:788}

\subsection{Fibonacci once again}
Write the generating function for the Fibonacci sequence in the following way:
\[
\frac{x}{1-x-x^2} = \frac{x}{1-(x+x^2)} = x( 1 + (x+x^2) + (x+x^2)^2 + \cdots)
\]
Multiplying out the power $(x+x^2)^k$, one obtains monomials of the form $x^{m_1+\cdots+m_k}$,
where each of $m_i$ is equal $1$ or $2$.
Summing $(x+x^2)^k$ over all $k$ (and not forgetting the factor $x$ on the right hand side of the above equation) one obtains
\[
\frac{x}{1-x-x^2} = \sum_{n=1}^\infty a_{n-1} x^n,
\]
where $a_n$ is the number of ways to represent $n$ as a sum of ones and twos.
The number of summands is not prescribed, and representations that differ in the order of summands are counted separately.

The above interpretation of Fibonacci numbers is equivalent to the following one:
the $n$-th Fibonacci number is the number of domino tilings of the $2 \times (n-1)$ rectangle.
See Figure \ref{fig:FibDomino}.

\begin{figure}[ht]
\begin{center}
\includegraphics{2nTiling}
\end{center}
\caption{This tiling corresponds to the representation $7 = 1 + 2 + 1 + 1 + 2$.}
\label{fig:FibDomino}
\end{figure}




\end{page}

%%%%%%%%%% output/789--5-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{2}
\label{portion:789}

\subsection{Partitions and their generating function}
\label{subsec:Partitions}

\end{page}

%%%%%%%%%% output/791--5-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{3}
\label{portion:791}

\begin{dfn}
A \emph{partition} of a positive integer $n$ is its representation as a sum of positive integers.
Representations that differ only in the order of summands are considered the same.

The number of partitions of $n$ is denoted by $p_n$.
\end{dfn}

\end{page}

%%%%%%%%%% output/792--5-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{3}
\label{portion:792}

In order to distinguish between different partitions, it is convenient to write the summands in the non-increasing order.
This means, one can also define partitions as compositions with non-increasing summands.


\end{page}

%%%%%%%%%% output/794--5-3-4-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{4}
\label{portion:794}

\begin{exl}
Below is the list of all partitions of the number $5$.
\begin{align*}
5 &= 5\\
&= 4+1\\
&= 3+2\\
&= 3+1+1\\
&= 2+2+1\\
&= 2+1+1+1\\
&= 1+1+1+1+1
\end{align*}
Thus we have $p_5=7$.
\end{exl}

\end{page}

%%%%%%%%%% output/795--5-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{4}
\label{portion:795}


Here are the first few terms of the sequence $p_n$, starting with $p_0$ which we by definition set to be equal to $1$:
\[
1, 1, 2, 3, 5, 7, 11, 15, 22, \ldots.
\]


% Every composition can be turned into a partition by rearranging the summands in the non-increasing order.
% On the other hand, different compositions may correspond to the same partition.
% For example, the partition $5=2+2+1$ corresponds to $3$ different compositions.
% It follows that
% \[
% p_n \le 2^{n-1}
% \]
% (That the number of compositions of $n$ from an arbitrary number of summands is $2^{n-1}$
% can be easily shown by the ``stones and sticks'' method.

Unlike for the money changing problem and for Fibonacci numbers, there is no closed formula for the number of partitions.
But there are a lot of beautiful theorems about partitions, and we will prove some of them.
Often we will be using the generating function method.


\end{page}

%%%%%%%%%% output/797--5-3-4-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:797}

\begin{thm}
The generating function of the sequence $p_n$ is
\[
\sum_{n=0}^\infty p_n x^n = \frac{1}{(1-x)(1-x^2)(1-x^3)\cdots}.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/798--5-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:798}

\begin{proof}
This is similar to the money changing problem, but with banknotes of any denomination available.

The right hand side is equal to
\[
(1+x+x^2+\cdots)(1+x^2+x^4+\cdots)(1+x^3+x^6+\cdots)\cdots = \sum x^{m_1 + 2m_2 + 3m_3 + \cdots + km_k},
\]
where the sum is taken over all $k$ and all collections of non-negative integers $m_1, \ldots, m_k$.
When we collect the like terms, the coefficient at $x^n$ will be equal to the number of representations of $n$ in the form
$m_1 + 2m_2 + \cdots + km_k$.
This corresponds to a unique partition, namely to
\[
n = \underbrace{k+\cdots+k}_{m_k} + \cdots + \underbrace{1+\cdots+1}_{m_1}.
\]
Thus the product on the right hand side is equal to the generating function of the number of partitions.
\end{proof}

In the above proof we met an infinite product of power series.
This product is again a power series because in order to compute the coefficient at $x^n$
only finitely many factors from the infinite product are needed (the first $n$ factors in the above case).
Definitions below formalize this.


\end{page}

%%%%%%%%%% output/800--5-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:800}

\begin{dfn}
One says that a sequence $c_n$ \emph{stabilizes} to $c$ if $c_n = c$ for all sufficiently big $n$:
\[
\exists N \text{ such that } c_n = c \forall n > N.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/803--5-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{7}
\label{portion:803}

\begin{dfn}
Let $B_1(x), B_2(x), \ldots$ be a sequence of formal power series with
\[
B_k(x) = \sum_{n=0}^\infty b_{k,n}(x) x^n
\]
One says that the sequence $B_k(x)$ of power series \emph{converges} to the power series $B(x) = \sum_{n=0}^\infty b_n x^n$:
\[
\lim_{k \to \infty} B_k(x) = B(x)
\]
if the sequence of coefficients at $x^n$ in $B_k(x)$ stabilizes to the coefficient at $x^n$ in $B(x)$:
\[
\forall n \exists K_n \text{ such that } b_{k,n} = b_n \forall k > K_n.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/806--5-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{8}
\label{portion:806}

\begin{dfn}
\[
\prod_{i=1}^\infty A_i(x) = \lim_{k \to \infty} \prod_{i=1}^k A_i(x)
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/809--5-3-4-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{9}
\label{portion:809}

\begin{lem}
Let $A_i(x) = 1 + a_{i,1} x + a_{i,2} x^2 + \cdots$.
The infinite product $\prod_{i=1}^\infty A_i(x)$ is well-defined if and only if $\lim\limits_{k \to \infty} \deg (A_k(x) - 1) = \infty$.
Here the \emph{degree} of a formal power series is the index of the first non-zero coefficient:
\[
\deg (a_m x^m + a_{m+1} x^{m+1} + \cdots) = m \text{ if } a_m \ne 0.
\]
\end{lem}

\end{page}

%%%%%%%%%% output/811--5-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{9}
\label{portion:811}

\subsection{Algebraic and bijective proofs}

% 

\end{page}

%%%%%%%%%% output/813--5-3-5-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{10}
\label{portion:813}

% \begin{thm}
% For all $n \ge 1$, the number of partitions of $n$ without parts of size $1$ is equal to $p_n - p_{n-1}$.
% \end{thm}

\end{page}

%%%%%%%%%% output/814--5-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{10}
\label{portion:814}

% \begin{proof}[Algebraic proof]
% Let $q_n$ be the number of partitions of $n$ without parts of size $1$.
% In the same way as we derived the generating function for the number of partitions,
% we can derive the generating function for the sequence $q_n$:
% \begin{multline*}
% \sum_{n=0}^\infty q_nx^n = (1+x^2+x^4+\cdots)(1+x^3+x^6+\cdots)\\
% = \frac{1}{(1-x^2)(1-x^3)\cdots} = (1-x) \sum_{n=0}^\infty p_nx^n
% \end{multline*}
% Thus we have
% \begin{multline*}
% q_0 + q_1 x + q_2 x^2 + \cdots = (1-x)(p_0 + p_1x + p_2x^2 +\cdots\\
% = p_0 + p_1x + p_2x^2 + \cdots\\
% \quad\quad - p_0x - p_1x_2 - \cdots\\
% = p_0 + (p_1-p_0)x + (p_2-p_1)x^2 + \cdots,
% \end{multline*}
% that is $q_n = p_n - p_{n-1}$ for $n \ge 1$.
% \end{proof}
% \begin{proof}[Bijective proof]
% Let $P_n$ denote the set of all partitions of $n$.
% Consider the map
% \[
% f \colon P_{n-1} \to P_n, \quad f(m_1+\cdots+m_k) = m_1+\cdots+m_k+1.
% \]
% This map is injective: different partitions are sent to different partitions.
% Thus $|f(P_{n-1})| = |P_{n-1}| = p_{n-1}$.
% Observe that
% \[
% P_n \setminus f(P_{n-1}) = \{\text{partitions of }n \text{ without parts of size }1\}.
% \]
% Thus the number of partitions of $n$ without parts of size $1$ is equal to
% \[
% |P_n \setminus f(P_{n-1})| = |P_n| - |f(P_{n-1})| = p_n - p_{n-1},
% \]
% and the theorem is proved.
% \end{proof}

Here is the first amazing fact about partitions.


\end{page}

%%%%%%%%%% output/816--5-3-5-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{11}
\label{portion:816}

\begin{thm}
The number of partitions of $n$ into distinct parts is equal to the number of partitions of $n$ into odd parts.
\end{thm}

\end{page}

%%%%%%%%%% output/817--5-3-5-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{11}
\label{portion:817}

\begin{proof}[Algebraic proof]
The generating function for partitions into distinct parts is
\[
(1+x)(1+x^2)(1+x^3)\cdots
\]
The generating function for partitions into odd parts is
\[
(1+x+x^2+\cdots)(1+x^3+x^6+\cdots)(1+x^5+x^{10}+\cdots)\cdots = \frac1{1-x} \frac1{1-x^3} \frac1{1-x^5} \cdots
\]
Let us show that the first formal power series is equal to the second one.
\begin{equation}
\label{eqn:OddDist}
(1+x)(1+x^2)(1+x^3)\cdots = \frac{1-x^2}{1-x} \frac{1-x^4}{1-x^2} \frac{1-x^6}{1-x^3} \cdots = \frac1{1-x} \frac1{1-x^3} \frac1{1-x^5} \cdots
\end{equation}
This equation is a bit more subtle than it appears.
We have
\[
(1+x) \cdots (1+x^{2k}) = \frac{1-x^2}{1-x} \cdots \frac{1-x^{4k}}{1-x^{2k}} = \frac{(1-x^{2k+2}) \cdots (1-x^{4k})}{(1-x) \cdots (1-x^{2k-1})}
\]
The right hand side has the same coefficient at $x^i$ for $i \le 2k$ as the infinite product on the right hand side of \eqref{eqn:OddDist}.
And the left hand side has the same coefficient at $x^i$ for $i \le 2k$ as the infinite product on the left hand side of \eqref{eqn:OddDist}.
\end{proof}

\begin{proof}[Bijective proof]
Take a partition of $n$ into odd parts:
\[
n = 1 \cdot m_1 + 3 \cdot m_3 + 5 \cdot m_5 + \cdots
\]
It can be transformed into a partition into distinct parts as follows.
Write $m_{2k+1}$ in the binary system:
\[
m_{2k+1} = 2^{d_1} + \cdots + 2^{d_s}, \quad d_i \ne d_j.
\]
Then replace $(2k+1) \cdot m_{2k+1}$ by
\[
(2k+1)(2^{d_1} + \cdots + 2^{d_s}) = 2^{d_1}(2k+1) + \cdots + 2^{d_s}(2k+1).
\]
Being done for all $k$, this gives a new partition of $n$.
The parts of the new partition are different.
Indeed, any two parts that come from the same $k$ are distinct: $2^{d_i}(2k+1) \ne 2^{d_j}(2k+1)$ because $d_i \ne d_j$.
Any two parts that come from different $k$ are also distinct: $2^{d_i(k)}(2k+1) \ne 2^{d_j(l)}(2l+1)$
because they have different greatest odd divisors $2k+1 \ne 2l+1$.

In the opposite direction, we transform every partition of $n$ into distinct parts as follows:
\begin{multline*}
n = k_1 + \cdots + k_t, \quad k_i \ne k_j\\
= o_1 2^{d_1} + \cdots + o_t 2^{d_t}, \quad o_i \text{ odd}\\
= 1 \cdot m_1 + 3 \cdot m_3 + \cdots, \quad m_{2k+1} = \sum_{o_i = 2k+1} 2^{d_i}
\end{multline*}
It can easily be shown that this transformation is inverse to the first one.
Thus we have a bijection between partitions into odd and partitions into distinct parts.
\end{proof}


\end{page}

%%%%%%%%%% output/819--5-3-5-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{dfn}{12}
\label{portion:819}

\begin{exl}
Turning a partition with odd parts into a partition with distinct parts:
\[
42 = 7+7+7+\underbrace{3+ \cdots +3}_{7} = 7 (2+1) + 3(4+2+1) = 14 + 7 + 12 + 6 + 3 = 14 + 12 + 7 + 6 + 3
\]
Turning a partition with distinct parts into a partition with odd parts:
\[
42 = 20 + 12 + 10 = 4\cdot 5 + 4 \cdot 3 + 2 \cdot 5 = (4+2)5 + 4\cdot 3 = \underbrace{5+\cdots+5}_{6} + 3 + 3 + 3
\]
\end{exl}

\end{page}

%%%%%%%%%% output/821--5-3-6-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{12}
\label{portion:821}

\subsection{Ferrers diagrams}
Ferrers diagram is a method to visualize a partition.
A partition
\[
n = k_1 + \cdots + k_m, \quad k_1 \ge \cdots \ge k_m
\]
is represented by by $m$ rows of dots, aligned on the left, with $k_1$ dots in the first row, $k_2$ dots in the second row etc.
Instead of dots one can use squares, see Figure \ref{fig:Ferrers}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.5\textwidth]{Ferrers}
\end{center}
\caption{Ferrers diagram for the partition $8 = 4 + 3 + 1$.}
\label{fig:Ferrers}
\end{figure}

With the help of Ferrers diagrams one can define the following transformation of a partition.


\end{page}

%%%%%%%%%% output/823--5-3-6-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{13}
\label{portion:823}

\begin{dfn}
The \emph{conjugate} of a partition $\lambda$ is
obtained by reflecting the diagram of $\lambda$ in the northwest-southeast diagonal.
\end{dfn}

\end{page}

%%%%%%%%%% output/826--5-3-6-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{14}
\label{portion:826}

\begin{exl}
The conjugate of $8 = 4 + 3 + 1$ is $8 = 3 + 2 + 2 + 1$, see Figure \ref{fig:FerrersConj}.
\end{exl}

\end{page}

%%%%%%%%%% output/827--5-3-6-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{14}
\label{portion:827}


\begin{figure}[ht]
\begin{center}
\includegraphics[width=.5\textwidth]{FerrersConj}
\end{center}
\caption{Conjugation of Ferrers diagrams.}
\label{fig:FerrersConj}
\end{figure}

Conjugation provides an immediate proof of the following theorem.


\end{page}

%%%%%%%%%% output/829--5-3-6-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{15}
\label{portion:829}

\begin{thm}
\label{thm:RestrPart}
The number of partitions of $n$ into exactly $k$ parts is equal to the number of partitions of $n$ with the largest part equal to $k$.

The number of partitions of $n$ into at most $k$ parts is equal to the number of partitions of $n$ into parts that do not exceed $k$.
\end{thm}

\end{page}

%%%%%%%%%% output/830--5-3-6-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{15}
\label{portion:830}

\begin{proof}
Conjugation defines a self-bijection of the set of all partitions of $n$.
Diagrams with $k$ rows are conjugate to diagrams with $k$ columns,
thus the number of diagrams of the first kind is equal to the number of diagrams of the second kind.
On the other hand, diagrams with $k$ rows correspond to partitions into $k$ parts,
and diagrams with $k$ columns correspond to partitions whose largest part is equal to $k$.
Thus we have as many partitions of the first kind as partitions of the second kind.

For partitions into at most $k$ parts and partitions with all parts $\le k$ the argument is similar.
\end{proof}


Let us present another elegant statement about partitions.
First, call a partition $\lambda$ \emph{self-conjugate} if it is conjugate to itself:
the Ferrers diagram of $\lambda$ is symmetric with respect to the northwest-southeast diagonal.
Figure \ref{fig:FerrersSelfConj} shows all self-conjugate partitions of $12$.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{ConjPart12}
\end{center}
\caption{Self-conjugate partitions of $12$.}
\label{fig:FerrersSelfConj}
\end{figure}



\end{page}

%%%%%%%%%% output/832--5-3-6-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{6}
\setcounter{dfn}{16}
\label{portion:832}

\begin{thm}
The number of self-conjugate partitions of $n$ is equal to the number of partitions of $n$ into distinct odd parts.
\end{thm}

\end{page}

%%%%%%%%%% output/833--5-3-6-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{7}
\setcounter{dfn}{16}
\label{portion:833}

\begin{proof}
In a self-conjugate partition join the dots as shown on Figure \ref{fig:OddDist}.
That is, take the union of the first row with the first column,
then the union of what remained of the second row with what remained of the second column, etc.
Each of these sets has an odd number of elements due to the symmetry of the diagram.
Besides, every set has less elements than the previous one.
This transforms a self-conjugate partition into a partition with distinct odd parts.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.8\textwidth]{OddDist}
\end{center}
\caption{From self-conjugate partitions to partitions into distinct odd parts: $12 = 11 + 1 = 9 + 3 = 7 + 5$.}
\label{fig:OddDist}
\end{figure}

The above transformation has an inverse.
Take any partition with distinct odd parts and ``bend'' each part in the middle.
Packing these hooks one inside another in the decreasing order produces the diagram of a self-conjugate partition.
Thus we have a bijection between the set of self-conjugate partitions and the set of partitions into distinct odd parts.
\end{proof}






\end{page}

%%%%%%%%%% output/834--5-3-7-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{7}
\setcounter{dfn}{16}
\label{portion:834}

\subsection{Recursive formulas for the number of partitions}
Denote by $p(n, \le k)$ the number of partitions of $n$ into at most $k$ parts.
(As we know from Theorem \ref{thm:RestrPart}, this is also the number of partitions into parts that do not exceed $k$.)
Clearly, one has $p_n = p(n, \le n)$.



\end{page}

%%%%%%%%%% output/836--5-3-7-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{7}
\setcounter{dfn}{17}
\label{portion:836}

\begin{thm}
One has
\begin{equation}
\label{eqn:RecNK}
p(n, \le k) = p(n, \le k-1) + p(n-k, \le k).
\end{equation}
\end{thm}

\end{page}

%%%%%%%%%% output/837--5-3-7-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{7}
\setcounter{dfn}{17}
\label{portion:837}

\begin{proof}
Partitions of $n$ into $\le k$ parts can be split into two classes:
partitions into $\le k-1$ parts and partitions into exactly $k$ parts.
By definition, the first class contains $p(n, \le k-1)$ partitions.
Take a partition from the second class and subtract $1$ from each of its parts.
Since it had exactly $k$ parts, the sum of the parts becomes $n-k$,
and their number becomes $\le k$ (it will be strictly less than $k$ if the smallest part was of size $1$,
and exactly $k$ if the smallest part was larger than $1$).
This establishes a bijection between the second class and partitions of $n-k$ into $\le k$ parts
and proves the theorem.
\end{proof}

\end{page}

%%%%%%%%%% output/839--5-3-7-remark.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{7}
\setcounter{dfn}{18}
\label{portion:839}

\begin{rem}
In terms of the Ferrers diagrams, the above transition from a partition of $n$ into $k$ parts to a partition of $n-k$ into $\le k$ parts
consists in removing the leftmost column of the diagram.
\end{rem}

\end{page}

%%%%%%%%%% output/840--5-3-7-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{7}
\setcounter{dfn}{18}
\label{portion:840}



Using relation \eqref{eqn:RecNK}, one can compute the entries of the table $p(n, \le k)$ recursively.
First, one fills the row $k=1$ and the column $n=0$ with ones.
Then, one can fill the table row after row or column after column.
One should observe that for $k > n$ one has $p(n, \le k) = p(n, \le n)$.
If one goes column after column, then in order to fill the column for $n = i$
one marks the diagonal $n+k = i$ and computes the $(n,k)$-entry as the sum of the entry immediately above it
and the entry on the intersection of the current row and the marked diagonal.

\begin{center}
\begin{tabular}{r|rrrrrrr}
$k^{\scalebox{1}{$n$}}$\hspace{-.2cm} & 0 & 1 & 2 & 3 & 4 & 5 & 6\\
\hline
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
2 & 1 & 1 & 2 & 2 & 3 & 3 & 4\\
3 & 1 & 1 & 2 & 3 & 4 & 5 & 7\\
4 & 1 & 1 & 2 & 3 & 5 & 6 & 9\\
5 & 1 & 1 & 2 & 3 & 5 & 7 & 10\\
6 & 1 & 1 & 2 & 3 & 5 & 7 & 11
\end{tabular}
\end{center}

There is a recurrence which allows a much faster computation of the number of partitions.


\end{page}

%%%%%%%%%% output/842--5-3-7-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{7}
\setcounter{dfn}{19}
\label{portion:842}

\begin{thm}[MacMahon's recurrence]
The number of partitions satisfies the following recurrence relation:
\[
p(n) =  \sum_{k=1}^\infty (-1)^{k-1} \left(p\left(n - \frac{3k^2 - k}{2}\right) + p\left(n - \frac{3k^2 + k}{2}\right)\right)
\]
Here the notation $p(n)$ is used instead of $p_n$.
\end{thm}

\end{page}

%%%%%%%%%% output/843--5-3-7-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{8}
\setcounter{dfn}{19}
\label{portion:843}


Computing the first few terms of the sequences $\frac{3k^2 \pm k}2$, one rewrites MacMahon's recurrence as
\[
p_n = p_{n-1} + p_{n-2} - p_{n-5} - p_{n-7} + p_{n-12} + p_{n-15} - \cdots
\]
The sum on the right hand side looks infinite but is in fact finite: $p(n-i)$ becomes zero as soon as $i$ exceeds $n$.
Thus the sum contains about $2\sqrt{\frac{2}{3} n}$ summands and allows a very fast computation of $p_n$.

MacMahon, more than a hundred years ago, has computed $p(n)$ for $n$ up to $200$.
In particular, he found that
\[
p(200) = 3 972 999 029 388.
\]

We will prove MacMahon's recurrence in the next section.




\end{page}

%%%%%%%%%% output/844--5-3-8-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{8}
\setcounter{dfn}{19}
\label{portion:844}

\subsection{Pentagonal numbers and the Euler identity}
MacMahon's recurrence is a consequence of the following remarkable theorem.

\end{page}

%%%%%%%%%% output/846--5-3-8-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{8}
\setcounter{dfn}{20}
\label{portion:846}

\begin{thm}[Euler]
\label{thm:EulerPenta}
One has
\begin{multline*}
(1-x)(1-x^2)(1-x^3) \cdots = 1 - x - x^2 + x^5 + x^7 - x^{12} - x^{15} + x^{22} + x^{26} - \cdots\\
= 1 + \sum_{k=1}^{\infty} (-1)^k (x^{(3k^2-k)/2} + x^{(3k^2+k)/2})
\end{multline*}
\end{thm}

\end{page}

%%%%%%%%%% output/847--5-3-8-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{8}
\setcounter{dfn}{20}
\label{portion:847}


This result is remarkable for two reasons.
Firstly, the coefficients on the right hand side take only values $0$ and $\pm 1$ (and more often $0$ than $\pm 1$).
This means that there is a lot of cancellation happening during this bracket expansion.
Secondly, we know the values of these coefficients exactly.

The proof of the Euler identity is very elegant.
Before learning it, let us see how this identity implies MacMahon's recurrence.

\begin{proof}[Proof of MacMahon's recurrence]
The formula for generating function of the number of partitions
\[
\sum_{n=0}^\infty p_n x^n = \frac{1}{(1-x)(1-x^2)(1-x^3) \cdots}
\]
and the Euler identity imply that
\[
(p_0 + p_1 x + p_2 x^2 + \cdots)(1 - x - x^2 + x^5 + x^7 - x^{12} - x^{15} + \cdots) = 1.
\]
The coefficient at $x^n$, $n \ge 1$, on the left hand side is
\[
p_n - p_{n-1} - p_{n-2} + p_{n-5} + p_{n-7} - \cdots,
\]
while on the right hand side the term $x^n$ is missing.
This implies the MacMahon recurrence.
\end{proof}


The numbers $\frac{3k^2 \pm k}{2}$ are called \emph{pentagonal numbers}
because they count the numbers of dots on Figure \ref{fig:PentaNumber}.
If one substitutes $-k$ for $k$, then $\frac{3k^2 - k}{2}$ becomes $\frac{3k^2 + k}{2}$,
this is why the latter number is also called pentagonal.

\begin{figure}[ht]
\begin{center}
\input{Fig/PentaNumbers.pdf_t}
\end{center}
\caption{Pentagonal numbers $\frac{3k^2 - k}{2}$.}
\label{fig:PentaNumber}
\end{figure}

What is of importance for us is not the arrangement of dots on Figure~\ref{fig:PentaNumber}
but Ferrers diagrams on Figure \ref{fig:FerrersPenta}.
They show that pentagonal numbers have partitions of the following form:
\begin{gather*}
\frac{3k^2-k}{2} = k + (k+1) + \cdots + (2k-1),\\
\frac{3k^2+k}{2} = (k+1) + (k+2) + \cdots + 2k.
\end{gather*}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.7\textwidth]{FerrersPenta}
\end{center}
\caption{Arrangements of $\frac{3k^2 - k}{2}$ or $\frac{3k^2 + k}{2}$ dots.}
\label{fig:FerrersPenta}
\end{figure}


Let us proceed to the proof of Theorem \ref{thm:EulerPenta}.
Denote by $q_n$ the number of partitions of $n$ into distinct parts.
Also, denote by $q_{n,\mathrm{even}}$ and $q_{n,\mathrm{odd}}$ the number of partitions of $n$ into an even,
respectively odd number of distinct parts.


\end{page}

%%%%%%%%%% output/849--5-3-8-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{8}
\setcounter{dfn}{21}
\label{portion:849}

\begin{lem}
We have
\[
(1-x)(1-x^2)(1-x^3) \cdots = 1 + \sum_{n=1}^\infty (q_{n,\mathrm{even}} - q_{n,\mathrm{odd}})x^n.
\]
\end{lem}

\end{page}

%%%%%%%%%% output/850--5-3-8-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{8}
\setcounter{dfn}{21}
\label{portion:850}

\begin{proof}
Recall that
\[
(1+x)(1+x^2)(1+x^3) \cdots = 1 + \sum_{n=1}^\infty q_n x^n,
\]
because the general term of the brackets expansion on the left hand side is $x^{k_1 + \cdots + k_m}$, where all $k_i$ are different.
Now, when we expand the product $(1-x)(1-x^2)(1-x^3)\cdots$, the general term has the form $(-1)^m x^{k_1 + \cdots + k_m}$.
Therefore the coefficient at $x^n$ is the number of partitions into an even number of distinct parts
minus the number of partitions into an odd number of distinct parts.
\end{proof}

We have reduced Theorem \ref{thm:EulerPenta} to the following lemma.

\end{page}

%%%%%%%%%% output/852--5-3-8-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{8}
\setcounter{dfn}{22}
\label{portion:852}

\begin{lem}
For every $n \ge 1$ we have
\[
q_{n,\mathrm{even}} - q_{n, \mathrm{odd}} =
\begin{cases}
(-1)^k, &\text{if } n = \frac{3k^2 \pm k}2;\\
0, &\text{otherwise.}
\end{cases}
\]
\end{lem}

\end{page}

%%%%%%%%%% output/853--5-3-8-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{9}
\setcounter{dfn}{22}
\label{portion:853}

\begin{proof}
We will construct a matching on the set of partitions into distinct summands
such that partitions with an even number of summands are matched to partitions with an odd number of summands.
For $n$ different from $\frac{3k^2 \pm k}2$ this matching will be perfect,
and for $n = \frac{3k^2 \pm k}2$ exactly one partition will remain unmatched.

For every partition $\lambda$ denote by $i(\lambda)$ the size of its smallest part.
By $j(\lambda)$ denote the number of consecutive parts of $\lambda$ starting with the largest part.
On the Ferrers diagram, $i(\lambda)$ is the length of the last row,
and $j(\lambda)$ is the length of the diagonal starting from the top right dot (let us call it the rightmost diagonal),
see Figure \ref{fig:IandJ}.

\begin{figure}[ht]
\begin{center}
\input{Fig/IandJ.pdf_t}
\end{center}
\caption{Definition of $i(\lambda)$ and $j(\lambda)$.}
\label{fig:IandJ}
\end{figure}

Take a partition $\lambda$ with $i(\lambda) \le j(\lambda)$ and look at its diagram.
Construct a new diagram by removing the last row and adding a dot to each of the first $i$ rows.
(In other words, move the last row so that it becomes a new rightmost diagonal.)
This yields a new partition $\lambda'$ with $j(\lambda') = i(\lambda)$ (because the new rightmost diagonal is the old last row)
and $i(\lambda') > i(\lambda)$ (because the new last row is longer than the old one).
In particular, $i(\lambda') > j(\lambda')$.
See Figure~\ref{fig:PentaMatching}.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.7\textwidth]{PentaMatching}
\end{center}
\caption{Transforming a $(i \le j)$-partition into a $(i > j)$-partition.}
\label{fig:PentaMatching}
\end{figure}

This is an (almost) perfect matching between partitions with $i \le j$ and partitions with $i > j$.
Indeed, the above transformation is (almost always) invertible:
take a partition with $i > j$ and move its rightmost diagonal to the bottom so that it becomes the new last row.

Also, this matching fulfills our needs: when the last row is moved, the number of parts changes by $1$,
so that partitions with an even number of parts are matched to partitions with an odd number of parts.

Why is it only almost perfect?
The transformation of a $(i \le j)$-partition into a $(i > j)$-partition is not going to work if $i=j$ and the last row intersects the rightmost diagonal
(in this case the new rightmost diagonal will ``stick out'').
A partition with this property looks as shown on Figure \ref{fig:FerrersPenta}, left.
Thus it is only possible for $n = \frac{3k^2 - k}2$.
The inverse transformation of a $(i > j)$-partition into a $(i \le j)$-partition will not work
if $i = j+1$ and the last row intersects the rightmost diagonal
(in this case after the transformation the two last rows have equal length, so the partition does not have distinct parts).
Such partitions look as shown on Figure \ref{fig:FerrersPenta}, right, and are only possible for $n = \frac{3k^2 + k}2$.

The ``lonely'' partition has $k$ rows.
Thus if $k$ is even, then there is one even partition more than odd,
and if $k$ is odd, then there is one odd partition more than even.
\end{proof}






\end{page}

%%%%%%%%%% output/854--5-3-9-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{dfn}{22}
\label{portion:854}

\subsection{More about partitions}
\begin{itemize}
\item 
An infinite (but fast convergent) series that computes $p_n$ was found by Ramanujan and Hardy and later improved by Rademacher.
A consequences of the latter is the asymptotics for the number of partitions:
\[
p_n \sim \frac{1}{4n\sqrt{3}} e^{\pi\sqrt{2n/3}}.
\]
\item
Ramanujan observed and later proved that
\begin{gather*}
p_{5n+4} \text{ is divisible by } 5,\\
p_{7n+5} \text{ is divisible by } 7,\\
p_{11n+6} \text{ is divisible by } 11.
\end{gather*}
\item
Erd\"os and Lehner proved that a ``random'' partition of $n$ has $\frac{2\pi}{\sqrt{6}} \sqrt{n} \log n$ summands.
\end{itemize}

Both Ramanujan and Erd\"os were extraordinary figures.
For the biography of Ramanujan see, for example,
\url{http://www-history.mcs.st-andrews.ac.uk/Biographies/Ramanujan.html}.

Further reading about partitions: \cite{AE04}.


\newpage


\end{page}

%%%%%%%%%% output/855--5-4-0-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:855}

\section{Catalan numbers}

\end{page}

%%%%%%%%%% output/856--5-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:856}

\subsection{Rooted binary trees}
Recall that a \emph{rooted tree} is a tree with a marked vertex, the root.
We have used rooted trees (with labels at the vertices) as parse trees of propositional formulas
and as proof structures in the sequent calculus.
Edges or a rooted tree have a natural orientation such that the path from the root to every vertex goes in the direction of edges.
The \emph{out-degree} of a vertex in a rooted tree is the number of outward-directed edges incident to this vertex.
Similarly, the \emph{in-degree} is the number of inward-directed edges; the in-degree of the root is zero, and the in-degrees of all other vertices are one.

A \emph{binary rooted tree} is a rooted tree where the out-degrees of all vertices are at most two.
A \emph{full binary rooted tree} is a rooted tree where the out-degree of each vertex is either two or zero.
(Vertices with out-degree zero are the leaves of the tree.)

With the help of the handshake lemma and the relation $|V| = |E| + 1$ one can show that
a full binary rooted tree with $n+1$ leaves has $2n+1$ vertices and $2n$ edges.


\end{page}

%%%%%%%%%% output/858--5-4-1-definition.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:858}

\begin{dfn}
The number of different full binary rooted trees with $n+1$ leaves is called the $n$-th Catalan number and is denoted by $c_n$.
\end{dfn}

\end{page}

%%%%%%%%%% output/859--5-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:859}

We count trees not up to isomorphism, but take into account also the way they are drawn in the plane.
For example, Figure \ref{fig:Catalan3Trees} shows all binary trees with $4$ leaves.
(When there is no risk of confusion, by ``binary tree'' we mean ``full binary rooted tree''.)

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\textwidth]{Catalan3Trees}
\end{center}
\caption{All $5$ binary trees with $4$ leaves.}
\label{fig:Catalan3Trees}
\end{figure}

One has $c_1 = 1, c_2 = 2, c_3 = 5, c_4 = 14, \ldots$.


\end{page}

%%%%%%%%%% output/861--5-4-1-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:861}

\begin{thm}
The sequence of Catalan numbers satisfies the recurrence
\begin{equation}
\label{eqn:CatalanRecursion}
c_{n+1} = \sum_{k=0}^n c_k c_{n-k} = c_0c_n + c_1c_{n-1} + \cdots + c_nc_0,
\end{equation}
where one puts $c_0 = 1$.
\end{thm}

\end{page}

%%%%%%%%%% output/862--5-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:862}


\begin{figure}[ht]
\begin{center}
\includegraphics[width=.62\textwidth]{InductionCatalanTrees}
\end{center}
\caption{Proof of the recurrence for Catalan numbers.}
\label{fig:InductionCatalanTrees}
\end{figure}

\begin{proof}
Take a binary tree with $n+2$ leaves and remove its root.
This splits the tree into two parts: the left subtree and the right subtree, see Figure~\ref{fig:InductionCatalanTrees}.
Each of them is either a full binary tree or a single vertex.
If the left subtree contains $k+1$ leaves, then the right subtree contains $n-k+1$ leaves
(if the subtree has only one vertex, then the number of leaves is one).
For every $k$, there are $c_k$ possible left subtrees and $c_{n-k}$ possible right subtrees.
This leads to the formula.
\end{proof}



\end{page}

%%%%%%%%%% output/863--5-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:863}

\subsection{Generating function and the formula for $c_n$}
\label{sec:GenFuncCatalan}

\end{page}

%%%%%%%%%% output/865--5-4-2-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:865}

\begin{thm}
\label{thm:CatalanFormula}
The $n$-th Catalan number is equal to
\begin{equation}
\label{eqn:CatalanFormula}
c_n = \frac1{n+1} \binom{2n}{n}.
\end{equation}
\end{thm}

\end{page}

%%%%%%%%%% output/868--5-4-2-lemma.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:868}

\begin{lem}
\label{lem:CatalanGenFunc}
For the generating function $C(x) = c_0 + c_1 x + c_2 x^2 + \cdots$ of the Catalan sequence
the following identity holds:
\[
(C(x))^2 = \frac{C(x) - 1}{x}.
\]
\end{lem}

\end{page}

%%%%%%%%%% output/869--5-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:869}

Observe that on the right hand side we divide a formal power series by $x$.
In Section \ref{sec:OperFPS} we have shown that division by formal power series with non-zero constant term is possible.
In general, it is not allowed to divide by $x$: for example, $\frac{1+x}{x}$ does not correspond to any power series
(there is no series $A(x)$ such that $1+x = xA(x)$).
But in the above case one has $c_0 = 1$, therefore $C(x) - 1 = c_1 x + c_2 x^2 + \cdots$, and we put \emph{by definition}
\[
\frac{C(x) - 1}{x} = c_1 + c_2 x + c_3 x^2 + \cdots.
\]
\begin{proof}
One has
\begin{multline*}
(C(x))^2 = (c_0 + c_1 x + c_2 x^2 + \cdots)(c_0 + c_1 x + c_2 x^2 + \cdots)\\
= c_0^2 + (c_0c_1 + c_1c_0) x + (c_0c_2 + c_1^2 + c_2c_0) x^2 + \cdots\\
= 1 + c_2 x + c_3 x^2 + \cdots = \frac{C(x) - 1}{x}
\end{multline*}
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:CatalanFormula}]
Lemma \ref{lem:CatalanGenFunc} implies that $C(x)$ satisfies a quadratic equation
\[
x (C(x))^2 - C(x) + 1 = 0.
\]
It follows that
\[
C(x) = \frac{1 - \sqrt{1-4x}}{2x}.
\]
(We choose the minus sign before the square root because otherwise the numerator is not divisible by $x$.
One can check that this is a solution of the quadratic equation by direct substitution.)
By the generalized binomial formula one has
\[
\sqrt{1-4x} = \sum_{k=0}^\infty \binom{\frac12}{k} (-4x)^k,
\]
where
\begin{multline*}
\binom{\frac12}{k} = \frac{\frac12 \cdot -\frac12 \cdot \cdots \cdot \left(\frac12 - k + 1\right)}{k!}
= (-1)^{k-1} \frac{(2k-3)!!}{2^k k!}\\
= (-1)^{k-1} \frac{(2k-2)!}{2^{2k-1} k! (k-1)!} = (-1)^{k-1} \frac{1}{2^{2k-1}k} \frac{(2k-2)!}{(k-1)!(k-1)!}\\
= (-1)^{k-1} \frac{1}{2^{2k-1}k} \binom{2k-2}{k-1}
\end{multline*}
for all $k \ge 1$, while $\binom{\frac12}{0} = 1$.
By substituting the expression for $\binom{\frac12}{k}$ into the binomial formula one gets
\[
\sqrt{1-4x} = 1 - \sum_{k=1}^\infty \frac{1}{2^{2k-1}k} \binom{2k-2}{k-1} 4^k x^k
= 1 - \sum_{k=1}^\infty \frac{2}{k} \binom{2k-2}{k-1} x^k.
\]
It follows that
\[
C(x) = \frac{1}{2x} \sum_{k=1}^\infty \frac{2}{k} \binom{2k-2}{k-1} x^k
= \sum_{k=1}^\infty \frac{1}{k} \binom{2k-2}{k-1} x^{k-1} = \sum_{k=0}^\infty \frac{1}{k+1} \binom{2k}{k} x^k,
\]
which proves the theorem.
\end{proof}






\end{page}

%%%%%%%%%% output/870--5-4-3-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:870}

\subsection{Bracket-variable expressions}
Assume that we have to multiply three variables $x$, $y$, and $z$.
Here by ``multiplication'' we mean any binary operation.
If this operation is commutative and associative, then neither the order of variables nor the order of operations is important:
\[
(xy)z = x(yz) = x(zy).
\]
If the operation is associative but not commutative (as multiplication of matrices for example),
then the order of variables matters, but the order of operations does not:
\[
(xy)z = x(yz) \ne x(zy).
\]
Finally, if the operation is neither commutative nor associative (as, for example, $xy := x^y$),
then we must take care both of the order of variables and the order of operations:
\[
(xy)z \ne x(yz).
\]
In how many ways can one multiply a given sequence of variables without changing their order?
For two variables there is only one way, for three variables two: $(xy)z$ and $x(yz)$,
below are all expressions with four variables:
\[
((x_1x_2)x_3)x_4, \quad (x_1(x_2x_3))x_4, \quad (x_1x_2)(x_3x_4), \quad x_1((x_2x_3)x_4), \quad x_1(x_2(x_3x_4)).
\]


\end{page}

%%%%%%%%%% output/872--5-4-3-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:872}

\begin{thm}
The number of different multiplication orders in a sequence of $n+1$ variables is equal to the $n$-th Catalan number.
\end{thm}

\end{page}

%%%%%%%%%% output/873--5-4-3-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:873}

\begin{proof}
We establish a bijection between binary trees with $n+1$ leaves and bracket-variable expressions with $n+1$ variables.

Any tree with $n+1$ leaves is the parse tree of some bracket-variable expression.
Put the variables $x_1, \ldots, x_{n+1}$ at the leaves of the tree, in the order from the left to the right.
Then mark the non-leaf vertices of the tree in the following way:
if the children of a vertex are marked with $A$ and $B$, then mark the vertex with $(AB)$.
The expression which appears at the root is the bracket-variable expression parsed by the tree.
(It has extra brackets around it, which can be removed.)

Thus one has a map from the set of binary trees to the set of bracket-variable expressions.
In order to show that this map is a bijection, one has to show that from \emph{every} bracket-variable expression
one can reconstruct \emph{uniquely} the tree which produces this expression by the above procedure.

This reconstruction (the inverse map from expressions to trees) is described as follows.
Consider the last multiplication to be performed and split the expression at this place.
Draw a root with two children and write the left part of the expression at the left child, and the right part at the right child.
Split in the same way the expressions at the child vertices and continue until all leaves will be marked with variables.
\end{proof}




\end{page}

%%%%%%%%%% output/874--5-4-4-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:874}

\subsection{Triangulations of polygons}
A \emph{triangulation} of a polygon is a subdivision of the polygon into triangles by diagonals.


\end{page}

%%%%%%%%%% output/876--5-4-4-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:876}

\begin{thm}
The number of different triangulations of a convex $(n+2)$-gon is equal to the $n$-th Catalan number.
\end{thm}

\end{page}

%%%%%%%%%% output/877--5-4-4-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{dfn}{6}
\label{portion:877}

In Figure \ref{fig:PentaTriangulations} all triangulations of a regular pentagon are presented.
As one can see, triangulations which differ by rotation or reflection of the polygon are counted separately.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\textwidth]{PentaTriangulations}
\end{center}
\caption{All $5$ triangulations of the pentagon.}
\label{fig:PentaTriangulations}
\end{figure}


\begin{proof}
We establish a bijection between triangulations and binary trees.

Place a dot inside every triangle of the triangulation and place a dot near every edge of the polygon just outside of the polygon.
Then draw a segment between every pair of vertices separated by an edge of the triangulation or by an edge of the polygon.
The result is a tree; every vertex inside of a triangle has degree $3$, and the vertices outside of the polygon are leaves.
Remove the dot at the base edge of the polygon and the edge incident to it.
The result is a rooted binary tree.
See Figure \ref{fig:TriangToTree} for an example.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.9\textwidth]{TriangToTree}
\end{center}
\caption{From a triangulation to a binary tree.}
\label{fig:TriangToTree}
\end{figure}

The base edge is chosen in advance.
For example, if the polygon ``stands'' on a line, one can declare the lowest edge the base edge.
Distinguishing the base edge reflects the fact that the polygon is not allowed to rotate.

From every binary tree one can reconstruct the corresponding triangulation in a unique way.
First, add an edge to the root so that all non-leaf vertices have degree $3$.
Then draw a triangle around each vertex so that each of the sides of the triangle intersects one edge of the tree.
Finally, for every edge of the tree glue the triangles surrounding the incident vertices along their sides intersecting this edge.
\end{proof}




\end{page}

%%%%%%%%%% output/878--5-4-5-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{dfn}{6}
\label{portion:878}

\subsection{Dyck paths}
Recall that a monotone lattice path is a path on the coordinate grid moving only upwards and to the right.
As we know, there are $\binom{2n}{n}$ monotone paths from $(0,0)$ to $(n,n)$.


\end{page}

%%%%%%%%%% output/880--5-4-5-definition.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{dfn}{7}
\label{portion:880}

\begin{dfn}
A \emph{Dyck path} is a monotone lattice path from $(0,0)$ to $(n,n)$ that stays above the diagonal.
The path is allowed to touch the diagonal, but not to cross it.
\end{dfn}

\end{page}

%%%%%%%%%% output/881--5-4-5-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{dfn}{7}
\label{portion:881}


It is convenient to look at a Dyck path on a rotated grid as shown in Figure \ref{fig:DyckPath}.

\begin{figure}[ht]
\begin{center}
\input{Fig/DyckPath.pdf_t}
\end{center}
\caption{A Dyck path.}
\label{fig:DyckPath}
\end{figure}



\end{page}

%%%%%%%%%% output/883--5-4-5-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{dfn}{8}
\label{portion:883}

\begin{thm}
The number of Dyck paths from $(0,0)$ to $(n,n)$ is equal to the $n$-th Catalan number.
\end{thm}

\end{page}

%%%%%%%%%% output/884--5-4-5-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{dfn}{8}
\label{portion:884}

\begin{proof}
Let us show that the sequence $d_n$ of numbers of Dyck paths satisfies the same recurrence relation that the sequence of Catalan numbers.
Take a path from $(0,0)$ to $(n+1,n+1)$ and let $(k+1,k+1)$ be the first point after $(0,0)$ where it touches the diagonal.
The number $k$ can take any value between $0$ and $n$.
The point $(k+1,k+1)$ separates the path into two parts.
The first part never touches the diagonal except at the endpoints.
If we remove from it the initial and the terminal segments, then we get a Dyck path from $(1,0)$ to $(k+1,k)$.
It can be identified by translation with a Dyck path from $(0,0)$ to $(k,k)$.
The part of the path after the point $(k+1,k+1)$ can be identified with a Dyck path from $(0,0)$ to $(n-k,n-k)$.

\begin{figure}[ht]
\begin{center}
\input{Fig/DyckInduction.pdf_t}
\end{center}
\caption{Proving the recursive relation for the number of Dyck paths.}
\label{fig:DyckInduction}
\end{figure}

Conversely, from any $k$-Dyck path and any $(n-k)$-Dyck path one can build a $(n+1)$-Dyck path
by ``lifting up'' the $k$-path and concatenating it with the $(n-k)$-path.
Thus the number of $(n+1)$-Dyck paths whose first contact with the diagonal is at $(k+1, k+1)$ is $d_k d_{n-k}$,
and the total number of $(n+1)$-Dyck paths is
\[
d_{n+1} = \sum_{k=0}^n d_k d_{n-k}.
\]
Thus we have for the sequence $d_n$ the same recursive relation, and also the same starting value $d_0 = 1$.
It follows that $d_n = c_n$ for all $n$.
\end{proof}

% \begin{proof}[Proof by bijection]
% Take a full binary tree with $n+1$ leaves.
% Write its vertices in the depth-first order $v_0v_1\ldots v_{2n}$.
% Associate to it a sequence $a_1a_2 \ldots a_{2n}$ of brackets in the following way:
% \[
% a_i =
% \begin{cases}
% (, &\text{if }v_i\text{ is a child of }v_{i-1},\\
% ), &\text{otherwise}.
% \end{cases}
% \]
% Let us show that this sequence of brackets is balanced.
% Every vertex except the root is either the left child or the right child of its parent.
% The definition of the sequence $a_i$ is equivalent to the following:
% \[
% a_i =
% \begin{cases}
% (, &\text{if }v_i\text{ is the left child of its parent},\\
% ), &\text{if }v_i\text{ is the right child of its parent}.
% \end{cases}
% \]
% In every pair of siblings, the left one precedes in our ordering the right one.
% This implies that the number of closing brackets never exceeds the number of opening brackets.
% \end{proof}



\end{page}

%%%%%%%%%% output/886--5-4-5-corollary.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{dfn}{9}
\label{portion:886}

\begin{cor}
The number of balanced bracket sequences of $n$ opening and $n$ closing brackets is $c_n$.
\end{cor}

\end{page}

%%%%%%%%%% output/887--5-4-5-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{6}
\setcounter{dfn}{9}
\label{portion:887}

Balanced bracket sequences are characterized by the property that,
when reading it from the left to the right, the number of closing brackets never exceeds the number of opening brackets.
This leads to a natural bijection with Dyck paths: interpret an opening bracket as a step upwards, and a closing bracket as a step to the right.



\end{page}

%%%%%%%%%% output/888--5-4-6-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{6}
\setcounter{dfn}{9}
\label{portion:888}

\subsection{A combinatorial proof of the formula for $c_n$}
The proof of the formula for $c_n$ in Section \ref{sec:GenFuncCatalan} was algebraic,
building upon the recursive formula \eqref{eqn:CatalanRecursion}.
Since Catalan numbers have so many combinatorial interpretation, it would be good to have a combinatorial proof of the same formula.

Instead of counting Dyck paths let us count paths that enter the triangle below the diagonal.
They may stay all the time below the diagonal or cross the diagonal one or several times.
We call them non-Dyck paths.

\end{page}

%%%%%%%%%% output/890--5-4-6-lemma.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{6}
\setcounter{dfn}{10}
\label{portion:890}

\begin{lem}
\label{lem:NonDyck}
Among the monotone paths from $(0,0)$ to $(n,n)$ there are exactly $\binom{2n}{n-1}$ non-Dyck paths.
\end{lem}

\end{page}

%%%%%%%%%% output/891--5-4-6-other.tex
\begin{page}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{dfn}{10}
\label{portion:891}


If we prove this lemma, then the formula for $c_n$ follows immediately:
\begin{multline*}
c_n = \binom{2n}{n} - \binom{2n}{n-1} = \frac{(2n)!}{n!n!} - \frac{(2n)!}{(n+1)!(n-1)!}\\
= \frac{(2n)!}{(n+1)!n!}((n+1) - n) = \frac{1}{n+1} \binom{2n}{n}.
\end{multline*}


\begin{proof}[Proof of Lemma \ref{lem:NonDyck}]
Let $(k,k-1)$ be the first point where a non-Dyck path enters the triangle below the diagonal.
The number $k$ can take any value between $1$ (which means that the first step goes below the diagonal) and $n$.
Reflect the part of the path from $(k,k-1)$ to $(n,n)$ as shown in Figure \ref{fig:NonDyckCount}.

\begin{figure}[ht]
\begin{center}
\includegraphics{NonDyckCount}
\end{center}
\caption{Counting non-Dyck paths.}
\label{fig:NonDyckCount}
\end{figure}

This transforms every non-Dyck path to a path from $(0,0)$ to $(n+1,n-1)$.
For every path from $(0,0)$ to $(n+1,n-1)$ there is a unique non-Dyck path that produces it.
To reconstruct this non-Dyck path, apply the same operation: take the first point of the form $(k,k-1)$ on the path to $(n+1,n-1)$
and reflect the part of the path after this point.
\end{proof}




\end{page}

%%%%%%%%%% output/892--6-0-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{0}
\setcounter{dfn}{10}
\label{portion:892}

\chapter{Automata theory}
The main source for this chapter is \cite{HU79}.

\end{page}

%%%%%%%%%% output/893--6-1-0-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:893}

\section{Finite automata}

\end{page}

%%%%%%%%%% output/894--6-1-1-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{0}
\label{portion:894}

\subsection{Alphabets, words, and languages}
An \emph{alphabet} is any finite set of symbols.
Examples:
\begin{itemize}
\item
the binary alphabet $\{0,1\}$;
\item
the alphabet of a single symbol $\{0\}$;
\item
the alphabet $\{p_1, \ldots, p_n\} \cup \{\neg, \wedge, \vee, \to, (, )\}$ of the propositional logic.
\end{itemize}

A \emph{string} or a \emph{word} is a finite sequence of symbols from a given alphabet.
The set of words of length $n$ in the alphabet $\Sigma$ is denoted by $\Sigma^n$:
\[
\Sigma^n = \{x_1 \ldots x_n \mid x_i \in \Sigma\ \forall i\}.
\]
This is the same as the Cartesian power $\Sigma^n$, with only a notational difference: $x_1 \ldots x_n$
instead of $(x_1, \ldots, x_n)$.

The concatenation of two words defines a map $\Sigma^m \times \Sigma^n \to \Sigma^{m+n}$.
Clearly, $uv \ne vu$ in general.
Denote by $\Sigma^* = \Sigma^0 \cup \Sigma^1 \cup \Sigma^2 \cup \cdots$ the set of all words in the alphabet $\Sigma$.

There is a unique element in $\Sigma^0$: the word of zero length; it is denoted by $\epsilon$.
One has
\[
\epsilon w = w = w\epsilon \text{ for all } w \in \Sigma^*.
\]

A \emph{language} is a subset of $\Sigma^*$.
Here are some examples of languages.

\begin{itemize}
\item
The set of all sequences of zeros of prime length:
\[
\{0^p \mid p \text{ is a prime number}\}.
\]
\item
The set of all binary palindromes (binary sequences that read the same forward and backward):
\[
\{\epsilon, 0, 1, 00, 11, 000, 010, \ldots\}.
\]
\item
In the alphabet of propositional logic, the set of all propositional formulas.
\item
In the same alphabet, the set of all propositional formulas which are tautologies.
\end{itemize}






\end{page}

%%%%%%%%%% output/895--6-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{0}
\label{portion:895}

\subsection{Deterministic finite automata}
A \emph{finite automaton} is a machine with finitely many states that changes its states according to the input.


\end{page}

%%%%%%%%%% output/897--6-1-2-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:897}

\begin{dfn}
A \emph{deterministic finite automaton (DFA)} is a quintuple $(Q, \Sigma, \delta, q_0, F)$, where
\begin{itemize}
\item
$Q$ is the set of states;
\item
$\Sigma$ is the input alphabet;
\item
$\delta$ is the transition function, that is, a map $\delta \colon Q \times \Sigma \to Q$;
\item
$q_0$ is the initial state;
\item
$F \subset Q$ is the set of final states.
\end{itemize}
The sets $Q$ and $\Sigma$ are assumed to be finite, and $F$ non-empty.
\end{dfn}

\end{page}

%%%%%%%%%% output/898--6-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{1}
\label{portion:898}


It is convenient to represent a DFA in the form of a \emph{transition diagram}.
A transition diagram is a graph whose vertex set is the set of states, and edges are directed and labeled by the alphabet symbols.
The edges describe the transition function: if $\delta(q_i,a) = q_j$, then we draw a directed edge from $q_i$ to $q_j$ and label it with $a$.
The initial state is indicated by an incoming arrow starting at nowhere.
The final states are indicated by double circles.
Figure \ref{fig:DFAOdd} shows an example of a DFA.

\begin{figure}[htb]
\begin{center}
\input{Fig/DFAOdd.pdf_t}
\caption{A deterministic finite automaton.}
\label{fig:DFAOdd}
\end{center}
\end{figure}

At the beginning the automaton is in the initial state.
When it receives an input word $w \in \Sigma^*$, it reads it from left to right and changes its state after each letter according to the transition function.
If after reading the input the automaton is in one of the final states, then one says that the word $w$ is \emph{accepted} by the automaton.
(Because of this, the final states are sometimes called \emph{accepting states}.)

One describes it formally by extending the transition function $\delta$ to a function $\widehat{\delta} \colon Q \times \Sigma^* \to Q$.
The value of $\widehat{\delta}(q,w)$ is the state in which the automaton ends if it starts at $q$ and reads the word $w$.
The definition is recursive:
\begin{enumerate}
\item
$\widehat{\delta}(q, \epsilon) = q$ for every state $q$;
\item
$\widehat{\delta}(q, wa) = \delta(\widehat{\delta}(q,w), a)$ for every state $q$, every word $w$, and every letter $a$.
\end{enumerate}


\end{page}

%%%%%%%%%% output/900--6-1-2-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:900}

\begin{dfn}
Let $M$ be a DFA.
A word $w \in \Sigma^*$ is called \emph{accepted} by $M$ if $\widehat{\delta}(q_0, w) \in F$.
The \emph{language $L(M)$ accepted by} $M$ is the set of all words accepted by $M$:
\[
L(M) = \{w \in \Sigma^* \mid \widehat{\delta}(q_0,w) \in F\}.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/903--6-1-2-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:903}

\begin{exl}
\par\noindent
\begin{enumerate}
\item[a)]
The language accepted by the automaton on Figure \ref{fig:DFAOdd} consists of all strings with an odd number of ones.
\item[b)]
One has $\epsilon \in L(M)$ if and only if the initial state $q_0$ of $M$ belongs to the set of final states of $M$.
\end{enumerate}
\end{exl}

\end{page}

%%%%%%%%%% output/906--6-1-2-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:906}

\begin{dfn}
A language is called \emph{regular} if it is accepted by some DFA.
\end{dfn}

\end{page}

%%%%%%%%%% output/907--6-1-2-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:907}


The main question which will be studied is:
\begin{center}
\sc{What languages are regular?}
\end{center}

In other words, what decision problems can be solved by DFAs?
For example, is there a DFA with the alphabet of propositional logic, which can tell if a given sequence of symbols is a tautological propositional formula?


\end{page}

%%%%%%%%%% output/909--6-1-2-remark.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:909}

\begin{rem}
Alternatively to a transition diagram, a DFA can be represented by a table.
For the DFA from Figure \ref{fig:DFAOdd} this is
\begin{center}
\begin{tabular}{c|cc}
& $0$ & $1$\\
\hline
$q_0$ & $q_0$ & $q_1$\\
$q_1$ & $q_1$ & $q_0$
\end{tabular}
\end{center}
The table contains the information about the transition function, the set of states, and the input alphabet.
In addition one should specify the set of final states, $F = \{q_1\}$ in our case.
\end{rem}

\end{page}

%%%%%%%%%% output/911--6-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:911}

\subsection{Nondeterministic finite automata}
In a \emph{nondeterministic finite automaton} (NFA) one allows the transition from a given state on a given input to be not uniquely defined or undefined.
In other words, while on the transition diagram of a DFA for every state $q$ and every input symbol $a$ there is a unique outgoing arrow from $q$ labeled with $a$,
on the transition diagram of an NFA there might be several arrows like that or none at all.
Figure \ref{fig:NFA} shows an example of an NFA.

\begin{figure}[htb]
\begin{center}
\input{Fig/NFA.pdf_t}
\end{center}
\caption{A nondeterministic finite automaton.}
\label{fig:NFA}
\end{figure}

A formal definition is as follows.

\end{page}

%%%%%%%%%% output/913--6-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:913}

\begin{dfn}
A \emph{nondeterministic finite automaton (NFA)} is a quintuple $(Q, \Sigma, \delta, q_0, F)$, where as before
$Q$ is a set of states, $\Sigma$ is a finite alphabet, $q_0 \in Q$ is the initial state, $F \subset Q$ is the set of final states.
However, the transition function
\[
\delta \colon Q \times \Sigma \to 2^Q
\]
associates to a pair $(q,a)$ not a state, but a set of states $P \subset Q$, which is allowed to be empty.
\end{dfn}

\end{page}

%%%%%%%%%% output/914--6-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:914}

(Recall that $2^Q$ denotes the set of all subsets of $Q$.)

As next, we need to describe how an NFA works, that is what words does it accept.
Informally speaking, an NFA can be in several states at the same time, and when an input symbol is read, each of these states generates a new set of states.
The number of current states does not necessarily increase with each step, because for some input there may be no transition defined from some states:
some branches die off.
In a more dramatic way this can be imagined as creation of parallel universes when the transition is not uniquely defined
and an apokalypsis in a given universe if the transition for a given input symbol is undefined.

Formally, we define an extended transition function recursively as
\begin{enumerate}
\item
$\widehat{\delta}(q, \epsilon) = \{q\}$;
\item
$\widehat{\delta}(q, wa) = \bigcup\limits_{p \in \widehat{\delta}(q,w)} \delta(p, a)$.
\end{enumerate}

Now, a word $w$ is considered \emph{accepted} by an NFA if there is a sequence of transitions corresponding to the input $w$
that ends up in a final state.
(That is, a word is accepted if it is accepted in at least one of the parallel universes.)

Formally, we define the language accepted by a non-deterministic automaton $M$ as
\begin{equation}
\label{eqn:NFALanguage}
L(M) = \{w \in \Sigma^* \mid \widehat{\delta}(q_0, w) \cap F \ne \emptyset\}.
\end{equation}


\end{page}

%%%%%%%%%% output/916--6-1-3-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:916}

\begin{exl}
\label{exl:NFA}
\par\noindent
\begin{enumerate}
\item[a)]
The automaton in Figure \ref{fig:NFA} accepts all words that contain two consecutive zeros.
Indeed, the state $q_2$ can be reached only if the input contains two consecutive zeros,
and once $q_2$ is reached, one stays there forever.
\item[b)]
The automaton in Figure \ref{fig:NFABroken} accepts all words that do not contain two consecutive zeros.
Indeed, it ``breaks down'' only if the input contains two consecutive zeros, and if it does not break down, then it accepts the input.
\end{enumerate}
\end{exl}

\end{page}

%%%%%%%%%% output/917--6-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:917}


\begin{figure}[htb]
\begin{center}
\input{Fig/NFABroken.pdf_t}
\end{center}
\caption{A nondeterministic finite automaton with $\delta(q_1, 0) = \emptyset$.}
\label{fig:NFABroken}
\end{figure}



\end{page}

%%%%%%%%%% output/919--6-1-3-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:919}

\begin{dfn}
Two finite automata $M$ and $M'$ are called \emph{equivalent} if they accept the same languages: $L(M) = L(M')$.
\end{dfn}

\end{page}

%%%%%%%%%% output/922--6-1-3-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:922}

\begin{thm}
For any NFA there is an equivalent DFA.
\end{thm}

\end{page}

%%%%%%%%%% output/923--6-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:923}

\begin{proof}
The proof is based on the interpretation of an NFA as ``being in several states at the same time''.
One constructs a deterministic automaton whose states correspond to sets of states of NFA.

Take any NFA $M = (Q, \Sigma, \delta, q_0, F)$.
Define a DFA $M' = (Q', \Sigma, \delta', q'_0, F')$ as follows.
\begin{itemize}
\item
$Q' = 2^Q$: the states of $M'$ are all subsets of the set of states of $M$.
\item
$q'_0 = \{q_0\}$, a one-element set.
\item
$F' = \{P \subset Q \mid P \cap F \ne \emptyset\}$, all subsets of $Q$ that contain at least one final state of $M$.
\item
For any $P \subset Q$ put $\delta'(P, a) = \bigcup\limits_{p\in P}\delta(p, a)$.
\end{itemize}

We claim that
\[
\widehat{\delta'}(\{q\}, u) = \widehat{\delta}(q, u)
\]
for all $q \in Q$ and all $u \in \Sigma^*$ and prove it by induction on the length of $u$.
If $u = \epsilon$, then both sides are equal to $\{q\}$.
Take any word of length at least~$1$, and let $a$ be its last letter.
Then this word can be written as $wa$, and we have
\begin{gather*}
\widehat{\delta'}(\{q\}, wa) = \delta'(\widehat{\delta'}(\{q\}, w), a) = \bigcup\limits_{p \in \widehat{\delta'}(\{q\}, w)} \delta(p, a)\\
\widehat{\delta}(q, wa) = \bigcup\limits_{p \in \widehat{\delta}(q,w)} \delta(p, a)
\end{gather*}
By induction hypothesis, $\widehat{\delta'}(\{q\}, w) = \widehat{\delta}(q, w)$, and the induction step is proved.

Definition of $F'$ and definition \eqref{eqn:NFALanguage} imply that $L(M) = L(M')$.
\end{proof}


\end{page}

%%%%%%%%%% output/925--6-1-3-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:925}

\begin{exl}
Let us construct a DFA equivalent to the NFA in Figure~\ref{fig:NFA}.
For convenience, write first the table of our NFA.
\begin{center}
\begin{tabular}{c|cc}
$\delta$ & $0$ & $1$\\
\hline
$q_0$ & $\{q_0, q_1\}$ & $\{q_0\}$\\
$q_1$ & $\{q_2\}$ & $\emptyset$\\
$q_2$ & $\{q_2\}$ & $\{q_2\}$
\end{tabular}
\end{center}
The set $Q = \{q_0, q_1, q_2\}$ has $8$ subsets, so if we follow the construction given in the theorem literally, we must write a table with $8$ rows.
However, not all of the $8$ states will be accessible from the initial state.
The inaccessible states can be removed from the automaton without affecting the language.
Therefore we will introduce new rows in our table for $M'$ only as soon as they are needed.
Also, for a better distinction we will use in $M'$ the $[\ ]$ brackets instead of the set brackets $\{\ \}$.
The result is the following table:
\begin{center}
\begin{tabular}{c|cc}
$\delta'$ & $0$ & $1$\\
\hline
$[q_0]$ & $[q_0,q_1]$ & $[q_0]$\\
$[q_0,q_1]$ & $[q_0,q_1,q_2]$ & $[q_0]$\\
$[q_0, q_1, q_2]$ & $[q_0,q_1,q_2]$ & $[q_0,q_2]$\\
$[q_0,q_2]$ & $[q_0,q_1,q_2]$ & $[q_0,q_2]$
\end{tabular}
\end{center}
For brevity, rename the states so that the table takes the form
\begin{center}
\begin{tabular}{c|cc}
$\delta'$ & $0$ & $1$\\
\hline
$q'_0$ & $q'_1$ & $q'_0$\\
$q'_1$ & $q'_2$ & $q'_0$\\
$q'_2$ & $q'_2$ & $q'_3$\\
$q'_3$ & $q'_2$ & $q'_3$
\end{tabular}
\end{center}
The corresponding transition diagram is shown in Figure \ref{fig:NFAtoDFA}.
The final states are $q'_2$ and $q'_3$ because they correspond to the sets which contain the final state $q_2$ of $M'$.
\end{exl}

\end{page}

%%%%%%%%%% output/926--6-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:926}


\begin{figure}[htb]
\begin{center}
\input{Fig/NFAtoDFA.pdf_t}
\end{center}
\caption{A DFA equivalent to the NFA in Figure \ref{fig:NFA}.}
\label{fig:NFAtoDFA}
\end{figure}


\end{page}

%%%%%%%%%% output/928--6-1-3-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{dfn}{11}
\label{portion:928}

\begin{exl}
Figure \ref{fig:NFAtoDFA2} shows the DFA constructed from the NFA in Figure \ref{fig:NFABroken}.
The state $q'_2$ corresponds to the empty set.
\end{exl}

\end{page}

%%%%%%%%%% output/929--6-1-3-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:929}


\begin{figure}[htb]
\begin{center}
\input{Fig/NFAtoDFA2.pdf_t}
\end{center}
\caption{A DFA equivalent to the NFA in Figure \ref{fig:NFABroken}.}
\label{fig:NFAtoDFA2}
\end{figure}




\end{page}

%%%%%%%%%% output/930--6-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:930}

\subsection{Finite automata with epsilon-transitions}
This is a further extension of the concept of a finite automaton.
In addition to ``branching'' and ``emergency stops'' present in $NFA$
we allow spontaneous transitions between some states.
That is, the transition diagram may contain arrows marked by $\epsilon$, the empty word.
Spontaneous transitions may branch as well: there might be several $\epsilon$-arrows starting from the same state.

Here is a formal definition.

\end{page}

%%%%%%%%%% output/932--6-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:932}

\begin{dfn}
\label{dfn:ENFA}
A \emph{nondeterministic finite automaton with $\epsilon$-transitions ($\epsilon$-NFA)}
is a quintuple $(Q, \Sigma, \delta, q_0, F)$, where $Q$, $\Sigma$, $q_0 \in Q$, and $F \subset Q$
are, as before, the set of states, the input alphabet, the initial state, and the set of final states,
but
\[
\delta \colon Q \times (\Sigma \cup \{\epsilon\}) \to 2^Q.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/933--6-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{12}
\label{portion:933}


A word $w$ is \emph{accepted} by an $\epsilon$-NFA if
there is a path from the initial state to one of the final states which corresponds to the input $w$
with any number of $\epsilon$-transitions inbetween.


\end{page}

%%%%%%%%%% output/935--6-1-4-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:935}

\begin{exl}
\label{exl:ENFA}
The automaton on Figure \ref{fig:ENFA} accepts positive and negative integers: strings of digits,
possibly preceded by the minus sign, the only string beginning with $0$ is $0$.
\begin{figure}[ht]
\begin{center}
\input{Fig/ENFA.pdf_t}
\end{center}
\caption{An $\epsilon$-NFA recognizing decimally represented integers.}
\label{fig:ENFA}
\end{figure}

The transition table of this automaton is given below.

\begin{center}
\begin{tabular}{c|cccc}
& $\epsilon$ & $-$ & $0$ & $1$-$9$\\\hline
$q_0$ & $\{q_1\}$ & $\{q_1\}$ & $\{q_3\}$ & $\emptyset$\\
$q_1$ & $\emptyset$ & $\emptyset$ & $\emptyset$ & $\{q_2\}$\\
$q_2$ & $\{q_3\}$ & $\emptyset$ & $\{q_2\}$ & $\{q_2\}$\\
$q_3$ & $\emptyset$ & $\emptyset$ & $\emptyset$ & $\emptyset$
\end{tabular}
\end{center}
\end{exl}

\end{page}

%%%%%%%%%% output/936--6-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{13}
\label{portion:936}


In order to proceed we need the following notion.

\end{page}

%%%%%%%%%% output/938--6-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:938}

\begin{dfn}
A subset of the set of states $P \subset Q$ is called \emph{$\epsilon$-closed}
if all $\epsilon$-transitions from states of $P$ lead to $P$: for every $q \in P$ one has $\delta(q, \epsilon) \subset P$.

The \emph{$\epsilon$-closure} of a subset $P \subset Q$ is the minimal $\epsilon$-closed subset containing $P$.
We denote the $\epsilon$-closure of $P$ by $\overline{P}$.
\end{dfn}

\end{page}

%%%%%%%%%% output/939--6-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:939}

In other words, $\overline{P}$ is $P$ together with all states that can be reached from $P$ by sequences of $\epsilon$-transitions.

Let us modify and extend the transition function so that it will tell us what states are accessible from a given state
for a given input.
\begin{enumerate}
\item
$\widehat{\delta}(q, \epsilon) = \overline{\{q\}}$
\item
$\widehat{\delta}(q, wa) = \overline{\delta(\widehat{\delta}(q,w), a)} \text{ for all }w \in \Sigma^*$
\end{enumerate}
(Note that $\widehat\delta(q,w)$ is a set, so that $\delta(\widehat{\delta}(q,w), a)$ denotes
the union of $\delta(p,w)$ over all $p \in \widehat{\delta}(q,w)$.)

Observe that, contrarily to the case of DFA and NFA, $\widehat{\delta}(q,a) \ne \delta(q,a)$, but rather
\[
\widehat{\delta}(q,a) = \overline{\delta(\overline{\{q\}}, a)} \supset \delta(q,a).
\]
It is not hard to see that $\widehat{\delta}(q,w)$ consists of all states reachable from $q$ on the input $w$
with arbitrarily many $\epsilon$-transitions before $w$, in the middle of $w$, and after $w$.

In terms of the extended transition function the language accepted by an $\epsilon$-NFA is defined as follows.

\end{page}

%%%%%%%%%% output/941--6-1-4-definition.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{15}
\label{portion:941}

\begin{dfn}
The language accepted by an $\epsilon$-NFA $M$ is
\[
L(M) = \{w \in \Sigma^* \mid \widehat{\delta}(q_0, w) \cap F \ne \emptyset\}.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/942--6-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{15}
\label{portion:942}


We will now show that $\epsilon$-NFAs are not more powerful than DFA: any language accepted by an $\epsilon$-NFA is also accepted by some DFA.


\end{page}

%%%%%%%%%% output/944--6-1-4-theorem.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{16}
\label{portion:944}

\begin{thm}
For every $\epsilon$-NFA there is an equivalent DFA.
\end{thm}

\end{page}

%%%%%%%%%% output/945--6-1-4-other.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{16}
\label{portion:945}

\begin{proof}
Let $M = (Q, \Sigma, \delta, q_0, F)$ be an $\epsilon$-NFA.
Construct a DFA $M' = (2^Q, \Sigma, \delta', q'_0, F')$ by putting
\[
q'_0 = \overline{\{q_0\}}, \quad F' = \{P \subset Q \mid P \cap F \ne \emptyset\},
\]
and defining the transition function by
\[
\delta'(P, a) = \overline{\delta(P, a)}.
%= \overline{\bigcup_{p \in P} \delta(p, a)}.
\]
We claim that for any word $w \in \Sigma^*$ holds
\begin{equation}
\label{eqn:DeltaD=DeltaE}
\widehat{\delta'}(q'_0, w) = \widehat{\delta}(q_0, w),
\end{equation}
where $\widehat{\delta}$ and $\widehat{\delta'}$ are the extended transition functions of $M$ and $M'$.
This is proved by induction on the length of the word $w$.
The base: $|w| = 0$, that is $w = \epsilon$. We have
\[
\widehat{\delta'}(q'_0, \epsilon) = q'_0 = \overline{\{q_0\}} = \widehat{\delta}(q_0, \epsilon).
\]
The induction step: assume \eqref{eqn:DeltaD=DeltaE} holds for all words of length $n$.
Any word of length $n+1$ has the form $wa$, where $|w| = n$ and $a \in \Sigma$.
Using the induction hypothesis, we obtain
\[
\widehat{\delta'}(q'_0, wa) = \delta'(\widehat{\delta'}(q'_0, w), a) = \delta'(\widehat{\delta}(q_0, w), a)
= \overline{\delta(\widehat{\delta}(q_0, w), a)} = \widehat{\delta}(q_0, wa).
\]

Now, by definition we have
\begin{gather*}
w \in L(M) \Leftrightarrow \widehat{\delta}(q_0, w) \cap F \ne \emptyset,\\
w \in L(M') \Leftrightarrow \widehat{\delta'}(q'_0, w) \in F' \Leftrightarrow \widehat{\delta'}(q'_0, w) \cap F \ne \emptyset,
\end{gather*}
which implies $L(M) = L(M')$ due to \eqref{eqn:DeltaD=DeltaE}.
\end{proof}


\end{page}

%%%%%%%%%% output/947--6-1-4-example.tex
\begin{page}
\setcounter{section}{1}
\setcounter{subsection}{4}
\setcounter{dfn}{17}
\label{portion:947}

\begin{exl}
Let us construct a DFA equivalent to the $\epsilon$-NFA from Example \ref{exl:ENFA}.

The transition table is obtained by consulting the table from Example \ref{exl:ENFA}
and applying the rule $\delta'(P, a) = \overline{\delta(P, a)}$.
As in the construction of an NFA out of a DFA, it might be not necessary to consider all subsets of $Q$.
We start with the row corresponding to the initial state,
and add a new row for every state which appeared in one of the previous rows.
The construction ends when no new states appear.

The initial state in our case is $\overline{\{q_0\}} = \{q_0, q_1\}$.
\begin{center}
\begin{tabular}{c|ccc}
& $-$ & $0$ & $1$-$9$\\\hline
$\{q_0, q_1\}$ & $\{q_1\}$ & $\{q_3\}$ & $\{q_2, q_3\}$\\
$\{q_1\}$ & $\emptyset$ & $\emptyset$ & $\{q_2, q_3\}$\\
$\{q_3\}$ & $\emptyset$ & $\emptyset$ & $\emptyset$\\
$\{q_2, q_3\}$ & $\emptyset$ & $\{q_2, q_3\}$ & $\{q_2, q_3\}$\\
$\emptyset$ & $\emptyset$ & $\emptyset$ & $\emptyset$
\end{tabular}
\end{center}

\begin{figure}[ht]
\begin{center}
\input{Fig/DFAFromENFA.pdf_t}
\end{center}
\caption{A DFA equivalent to the $\epsilon$-NFA from Example \ref{exl:ENFA}.}
\label{fig:DFAFromENFA}
\end{figure}

The diagram of this automaton is shown in Figure \ref{fig:DFAFromENFA}.
\end{exl}

\end{page}

%%%%%%%%%% output/949--6-2-0-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:949}

\section{Regular expressions}
A regular expression is a formula which describes a language.
We will see that languages represented by regular expressions are regular (i.e. are accepted by a finite automaton)
and that every regular language can be represented by a regular expression.


\end{page}

%%%%%%%%%% output/950--6-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:950}

\subsection{Definition and examples}
A regular expression is defined recursively.
The basic building blocks are the following.
\begin{enumerate}
\item
$\emptyset$ is a regular expression and denotes the language $\emptyset$.
\item
$\epsilon$ is a regular expression and denotes the language $\{\epsilon\}$.
\item
$a$ is a regular expression for every $a \in \Sigma$ and denotes the language $\{a\}$.
\end{enumerate}
Don't confuse the empty language $\emptyset$ and the language $\{\epsilon\}$ consisting of an empty word.


% \begin{figure}[ht]
% \begin{center}
% \includegraphics{EmptyEmpty.pdf}
% \end{center}
% \caption{NFAs for the languages $\emptyset$ and $\{\epsilon\}$.}
% \label{fig:EmptyEmpty}
% \end{figure}

Sometimes one uses the boldface $\bf{a}$ to denote the language $\{a\}$.
We will use the same symbol $a$.

From these building blocks one constructs more complex regular expressions by using the following operations.
If $r$ and $s$ are regular expressions denoting the languages $R$ and $S$ respectively, then
\begin{enumerate}
\item
$(r+s)$ is a regular expression and denotes the language $R \cup S$;
\item
$(rs)$ is a regular expression and denotes the language $RS = \{uv \mid u \in R, v \in S\}$;
\item
$r^*$ is a regular expression and denotes the language $R^* = \cup_{i=0}^\infty R^i$, where $R^i = \underbrace{RR \cdots R}_{i}$
(the \emph{Kleene closure} of language $R$).
\end{enumerate}


\end{page}

%%%%%%%%%% output/952--6-2-1-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:952}

\begin{exl}
The language of all binary words can be represented by the expression $(0+1)^*$.
\end{exl}

\end{page}

%%%%%%%%%% output/953--6-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:953}



One can omit some of the brackets in regular expressions by adopting the convention that $*$ precedes the concatenation, and the concatenation precedes the sum.
For example, $((0(1^*))+0)$ may be written as $01^* + 0$, and we have
\[
01^* + 0 = \{0, 01, 011, 0111, \ldots\}.
\]

Two regular expressions are called equivalent if they describe the same language.
Here are some simple equivalences:
\[
(rs)t \sim rs(t), \quad (r+s)t \sim rs + rt.
\]
Instead of the equivalence sign we will use the equality sign to denote the equivalence of regular expressions.
For example,
\[
01^* + 0 = 01^*, \quad \emptyset r = \emptyset, \quad \epsilon r = r.
\]



\end{page}

%%%%%%%%%% output/955--6-2-1-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:955}

% \begin{exl}
% Any word in $\Sigma$ is a regular expression, because it is formed from the letter symbols (which are regular expressions)
% by concatenation.
% Following the above definitions, a word $w$ denotes the language $\{w\}$.
% \end{exl}

\end{page}

%%%%%%%%%% output/956--6-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:956}



Recall that a language is called regular if there is a finite automaton (DFA, NFA, or $\epsilon$-NFA, which does not matter, as we have shown)
that accepts this language.
The main theorem is the following.

\end{page}

%%%%%%%%%% output/958--6-2-1-theorem.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:958}

\begin{thm}
\label{thm:RegLangExpr}
A language is regular if and only if it can be represented by a regular expression.
\end{thm}

\end{page}

%%%%%%%%%% output/959--6-2-1-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:959}


This theorem will be proved in the next two sections.
Now let us give some examples of regular expressions and languages corresponding to them.


\end{page}

%%%%%%%%%% output/961--6-2-1-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:961}

\begin{exl}
The language of words consisting of alternating $0$'s and $1$'s.
It can be represented by a regular expression $(01)^* + (10)^* + 0(10)^* + 1(01)^*$.
In this expression a case distinction is incorporated: if the word is of even length and starts with $0$,
then it belongs to the language $(01)^*$, etc.

The same language can be described by the expression
\[
(01)^* + 1(01)^* + (01)^*0 + 1(01)^*0 = (\epsilon + 1)(01)^*(\epsilon + 0).
\]
\end{exl}

\end{page}

%%%%%%%%%% output/964--6-2-1-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{5}
\label{portion:964}

\begin{exl}
The set of all binary words whose tenth symbol from the right is $1$ can be described by a regular expression $(0+1)^* 1 (0+1)^9$,
where $(0+1)^9$ denotes $\underbrace{(0+1)\cdots (0+1)}_{9}$.
\end{exl}

\end{page}

%%%%%%%%%% output/967--6-2-1-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{dfn}{6}
\label{portion:967}

\begin{exl}
The set of all binary words that contain at least one $0$ and at least one $1$
can be represented by
\[
(0+1)^*0(0+1)^*1(0+1)^* + (0+1)^*1(0+1)^*0(0+1)^*.
\]
\end{exl}

\end{page}

%%%%%%%%%% output/969--6-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:969}

\subsection{Equivalence of regular expressions and regular languages}
We will now prove Theorem \ref{thm:RegLangExpr}.
It splits in two parts.


\end{page}

%%%%%%%%%% output/971--6-2-2-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:971}

\begin{lem}
The language described by a regular expression is regular.
\end{lem}

\end{page}

%%%%%%%%%% output/972--6-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:972}

\begin{proof}
We describe a construction algorithm of an $\epsilon$-NFA that accepts the language described by a given regular expression $r$.
Moreover, the resulting automaton will have a unique accepting state.
The construction uses the recursive structure of the regular expression.

Let $r$ be any regular expression.
By definition, $r$ is either basic or is obtained from one or two simpler expressions through sum, concatenation or closure.

If $r$ is basic, then the corresponding language is accepted by one of the automata shown in Figure \ref{fig:AutomBasicRegExpr}.

\begin{figure}[ht]
\begin{center}
\input{Fig/AutomBasicRegExpr.pdf_t}
\end{center}
\caption{Automata for basic regular expressions.}
\label{fig:AutomBasicRegExpr}
\end{figure}

If $r = r_1 + r_2$, then by assumption there are $\epsilon$-NFAs $M_1$ and $M_2$,
each with a unique final state, for the languages represented by $r_1$ and $r_2$.
The automaton in Figure \ref{fig:AutomSum} accepts the language of the expression $r_1 + r_2$.

\begin{figure}[ht]
\begin{center}
\input{Fig/AutomSum.pdf_t}
\end{center}
\caption{Automaton realizing the union of two languages.}
\label{fig:AutomSum}
\end{figure}

If $r = r_1r_2$, then we combine the automata for $r_1$ and $r_2$ as shown in Figure \ref{fig:AutomConcat}.

\begin{figure}[ht]
\begin{center}
\input{Fig/AutomConcat.pdf_t}
\end{center}
\caption{Automaton realizing the concatenation of two languages.}
\label{fig:AutomConcat}
\end{figure}

Finally, the automaton in Figure \ref{fig:AutomClosure} accepts the language of $(r_1)^*$.

\begin{figure}[ht]
\begin{center}
\input{Fig/AutomClosure.pdf_t}
\end{center}
\caption{Automaton realizing the Kleene closure of a language.}
\label{fig:AutomClosure}
\end{figure}

In order to show that these automata do what they are meant to do, one has to prove two things:
first, each word from the language $R_1 + R_2$ (respectively, $R_1R_2$, or $R_1^*$) is accepted by the automaton;
second, each word accepted by the automaton belongs to the respective language.
The arguments proving this are rather straightforward, and we omit them.
\end{proof}


\end{page}

%%%%%%%%%% output/974--6-2-2-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{8}
\label{portion:974}

\begin{exl}
Construct an automaton accepting the language $01^* + 1$.

The automaton obtained with the above argument has ten states, see Example 2.12 from the first edition of Hopcroft-Ullman.
One can rather easily construct an $\epsilon$-NFA with three states.
The above algorithm aims not for the smallest number of states, but for the simplicity of the construction.
\end{exl}

\end{page}

%%%%%%%%%% output/975--6-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{8}
\label{portion:975}


Now we proceed to the second part of Theorem \ref{thm:RegLangExpr}.


\end{page}

%%%%%%%%%% output/977--6-2-2-definition.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{9}
\label{portion:977}

\begin{dfn}
A word $x$ is called a \emph{prefix} of a word $w$ if $w = xy$ for some word $y$.
In particular, both $\epsilon$ and $w$ are prefixes of $w$.
A \emph{proper prefix} of $w$ is a prefix distinct from $\epsilon$ and $w$.
\end{dfn}

\end{page}

%%%%%%%%%% output/980--6-2-2-lemma.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{10}
\label{portion:980}

\begin{lem}
Every regular language can be described by a regular expression.
\end{lem}

\end{page}

%%%%%%%%%% output/981--6-2-2-other.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{10}
\label{portion:981}

\begin{proof}
Let $R$ be the language accepted by a DFA with the set of states $Q = \{q_1, \ldots, q_n\}$, where $q_1$ is the initial state.
Our goal is to construct a regular expression $r$ that describes $R$.

Denote by $R_{ij}^k$ the set of all words $w$ such that
\begin{itemize}
\item
$\widehat{\delta}(q_i, w) = q_j$;
\item
$\widehat{\delta}(q_i, x) \in \{q_1, \ldots, q_k\}$ for all proper prefixes $x$ of $w$.
\end{itemize}
In other words, $R_{ij}^k$ is the set of all words that lead you from $q_i$ to $q_j$
through the states with indices less or equal $k$ only.
Note that we allow $i$ or $j$ to be bigger than $k$: one may be in a state with number bigger than $k$ at the beginning or at the end, but not in between.
One has
\[
R = \bigcup_{q_j \in F} R_{1j}^n.
\]
We will prove by induction on $k$ that each of $R_{ij}^k$ can be represented by a regular expression.

\textbf{Base:} $k = 0$.
By definition, $R_{ij}^0$ consists of direct transitions from $q_i$ to $q_j$. Thus
\[
R_{ij}^0 =
\begin{cases}
\{a \mid \delta(q_i, a) = q_j\}, &\text{ if } i \ne j,\\
\{a \mid \delta(q_i, a) = q_j\} \cup \{\epsilon\}, &\text{ if } i = j.
\end{cases}
\]
In both cases, the set $R_{ij}^0$ is finite and therefore described by a regular expression of the following form:
\[
r_{ij}^0 =
\begin{cases}
a_1 + \cdots + a_p, &\text{ if } i \ne j \text{ and } R_{ij}^0 = \{a_1, \ldots, a_p\},\\
\emptyset, &\text{ if } i \ne j \text{ and } R_{ij}^0 = \emptyset,\\
a_1 + \cdots + a_p + \epsilon, &\text{ if } i = j \text{ and } R_{ij}^0 = \{a_1, \ldots, a_p\},\\
\epsilon, &\text{ if } i = j \text{ and }R_{ij}^0 = \{\epsilon\}.
\end{cases}
\]

\textbf{Step.} Let us prove that for $k \ge 1$ and for all $i, j$ one has
\[
R_{ij}^k = R_{ij}^{k-1} \cup R_{ik}^{k-1} (R_{kk}^{k-1})^* R_{kj}^{k-1}.
\]
Indeed, take any $w \in R_{ij}^k$ and look at the corresponding path from $q_i$ to $q_j$.
If this path does not pass through $q_k$, then $w \in R_{ij}^{k-1}$.
Otherwise split the path into the following pieces:
\begin{itemize}
\item
from $q_i$ to $q_k$ without passing through $q_k$;
\item
from $q_k$ to $q_k$ without passing through $q_k$ (there may be several pieces like this);
\item
from $q_k$ to $q_j$ without passing through $q_k$.
\end{itemize}
This represents the word $w$ as a concatenation $w = w_1 \cdots w_m$, where
\[
w_1 \in R_{ik}^{k-1}, \quad w_2, \ldots, w_{m-1} \in R_{kk}^{k-1}, \quad w_m \in R_{kj}^{k-1}.
\]
Thus $w \in R_{ik}^{k-1} (R_{kk}^{k-1})^* R_{kj}^{k-1}$.
It is also clear than any $w \in R_{ij}^{k-1}$ or $R_{ik}^{k-1} (R_{kk}^{k-1})^* R_{kj}^{k-1}$ leads from $q_i$ to $q_j$
without passing through states with the number $>k$.

By induction assumption, for all $i, j$ there is a regular expression $r_{ij}^{k-1}$ which describes the language $R_{ij}^{k-1}$.
The language $R_{ij}^k$ is then described by the regular expression
\[
r_{ij}^k = r_{ij}^{k-1} + r_{ik}^{k-1} (r_{kk}^{k-1})^* r_{kj}^{k-1}.
\]

Finally, the language $R$ is described by the regular expression
\[
r = r_{1,m+1}^n + \cdots + r_{1,n}^n,
\]
where $F = \{q_{m+1}, \ldots, q_n\}$.
\end{proof}



\end{page}

%%%%%%%%%% output/983--6-2-2-example.tex
\begin{page}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{dfn}{11}
\label{portion:983}

\begin{exl}
\label{exl:DFAForReg1}
(Example 2.13. from \cite{HU79}.)
Find a regular expression for the language accepted by the automaton on Figure \ref{fig:DFAForReg1}.
\begin{figure}[ht]
\begin{center}
\input{Fig/DFAForReg1.pdf_t}
\end{center}
\caption{Automaton for Example \ref{exl:DFAForReg1}.}
\label{fig:DFAForReg1}
\end{figure}

One fills the table in Figure \ref{fig:TableRegExp} column after column according to the above algorithm.

\begin{figure}[ht]
\begin{center}
\begin{tabular}{c|ccc}
& $k=0$ & $k=1$ & $k=2$\\
\hline
$r_{11}^k$ & $\epsilon$ & $\epsilon$ & $(00)^*$\\
$r_{12}^k$ & $0$ & $0$ & $0(00)^*$\\
$r_{13}^k$ & $1$ & $1$ & $0^*1$\\
$r_{21}^k$ & $0$ & $0$ & $0(00)^*$\\
$r_{22}^k$ & $\epsilon$ & $\epsilon + 00$ & $(00)^*$\\
$r_{23}^k$ & $1$ & $1 + 01$ & $0^*1$\\
$r_{31}^k$ & $\emptyset$ & $\emptyset$ & $(0+1)(00)^*0$\\
$r_{32}^k$ & $0+1$ & $0+1$ & $(0+1)(00)^*$\\
$r_{33}^k$ & $\epsilon$ & $\epsilon$ & $\epsilon + (0+1)0^*1$
\end{tabular}
\end{center}
\caption{Finding a regular expression for Example \ref{exl:DFAForReg1}.}
\label{fig:TableRegExp}
\end{figure}

The first column is easy.
For the second and the third column use the recursive formula.
Sometimes a regular expression can be simplified, and this was done at several places in this table.
For example,
\[
r_{22}^1 = r_{22}^0 + r_{21}^0(r_{11}^0)^*r_{12}^0 = \epsilon + 0(\epsilon)^*0 = \epsilon + 00.
\]
More interesting things happen to $r_{13}^2$, which by the direct application of the recursive formula is equal to
\[
r_{13}^2 = r_{12}^1(r_{22}^1)^*r_{23}^1 + r_{13}^1 = 1+ 0(\epsilon + 00)^*(1+01).
\]
Because of $(\epsilon + 00)^* = (00)^*$ and $1+01 = (\epsilon + 0)1$ this can be rewritten as
\[
r_{13}^2 = 1 + 0(00)^*(\epsilon + 0)1.
\]
Further, one has $(00)^*(\epsilon + 0) = 0^*$, so that
\[
r_{13}^2 = 1 + 00^*1 = 0^*1.
\]
A regular expression for the language accepted by this automaton is $r_{12}^3 + r_{13}^3$.
Each of the summands is a lengthy expression. After some simplifications one obtains
\[
r = 0^*1((0+1)0^*1)^*(\epsilon+(0+1)(00)^*) + 0(00)^*.
\]

It should be noted that one does not need all of the table \ref{fig:TableRegExp} to compute the expression $r$.
\end{exl}

\end{page}

%%%%%%%%%% output/985--6-3-0-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:985}

\section{Properties of regular languages}

\end{page}

%%%%%%%%%% output/986--6-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:986}

\subsection{Closure under boolean operations}

\end{page}

%%%%%%%%%% output/988--6-3-1-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:988}

\begin{thm}
\label{thm:ClosRegLang}
Let $R, R_1, R_2$ be any regular languages in the alphabet $\Sigma$.
Then the languages $R_1 \cup R_2, R_1 \cap R_2, \Sigma^* \setminus R$ are also regular.
\end{thm}

\end{page}

%%%%%%%%%% output/989--6-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:989}

\begin{proof}
If $r_1$ is a regular expression for $R_1$, and $r_2$ is a regular expression for $R_2$,
then the regular expression $r_1 + r_2$ describes the language $R_1 \cup R_2$, which is therefore regular.

Given regular expressions $r_1$, $r_2$, $r$,
it is very difficult to find regular expressions for the intersection $R_1 \cap R_2$ and the complement $\Sigma^* \setminus R$.
Let us approach the problem from a different direction.

Let $M = (Q, \Sigma, \delta, q_0, F)$ be a DFA accepting the language $R$.
Then $\overline{M} := (Q, \Sigma, \delta, q_0, Q \setminus F)$ accepts the language $\Sigma^* \setminus R$.
Indeed,
\[
w \in \Sigma^* \setminus R \Leftrightarrow w \notin R \Leftrightarrow \widehat{\delta}(q_0, w) \notin F
\Leftrightarrow \widehat{\delta}(q_0, w) \in Q \setminus F \Leftrightarrow w \in L(\overline{M}).
\]
Therefore $\Sigma^* \setminus R$ is regular.

With the intersection we are helped by de Morgan's rule:
\[
R_1 \cap R_2 = \overline{\overline{R_1} \cup \overline{R_2}},
\]
where the overline denotes the complement.
Since the operations applied on the right hand side preserve regularity, the intersection of two regular languages is regular.
\end{proof}



\end{page}

%%%%%%%%%% output/991--6-3-1-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:991}

\begin{thm}
The equivalence of finite automata and the equivalence of regular expressions is decidable.
(That is, there is an algorithm for each of these problems, which gives a correct answer in finite time.)
\end{thm}

\end{page}

%%%%%%%%%% output/992--6-3-1-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:992}

\begin{proof}
We have an algorithm which converts a regular expression into a finite automaton.
Therefore it suffices to find an algorithm for the equivalence of finite automata.

Given two finite automata $M_1$ and $M_2$, let $L_1 = L(M_1)$ and $L_2 = L(M_2)$.
By Theorem \ref{thm:ClosRegLang}, the symmetric difference
\[
L_1 \triangle L_2 = (L_1 \setminus L_2) \cup (L_2 \setminus L_1)
\]
is a regular language.
Thus there is a finite automaton $M$ that accepts the language $L_1 \triangle L_2$.
One has
\[
L_1 = L_2 \Leftrightarrow L_1 \triangle L_2 = \emptyset.
\]
Therefore it suffices to decide whether the language accepted by $M$ is empty.
The language is empty if and only if from the initial state no final state can be reached.
This is easy to check algorithmically.
\end{proof}





\end{page}

%%%%%%%%%% output/993--6-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:993}

\subsection{The pumping lemma}
Let $\Sigma$ be a finite alphabet.
The set $\Sigma^*$ of all words in $\Sigma$ is countably infinite.
The set $2^{\Sigma^*}$ of all languages in $\Sigma$ is uncountable: it has the cardinality of the continuum.
On the other hand, the set of regular languages is countable, because the set of all regular expressions (and of all finite automata) is countable.
(Note that different automata or different regular expressions can define the same language, but this is not a problem.)
It follows that ``most'' languages are non-regular.

The above argument is a pure existence proof.
In this section we prove the \emph{pumping lemma}, a tool that allows to prove the non-regularity of some languages.

Before stating the lemma, let us explain the underlying idea.
Let $M$ be a DFA.
Any word $z$ accepted by $M$ determines a path from the initial state $q_0$ to one of the finals states $q \in F$.
If the word $z$ is long enough (namely if its length is bigger than the number of states of $M$),
then the corresponding path contains a cycle.
This cycle gives rize to infinitely many other words accepted by $M$, because one can run along it several times.
For example, in Figure \ref{fig:LongWord} one has $z = a_1a_2a_3a_4a_5a_6a_7 \in L(M)$.
By repeating or removing the cycle contained in the path, one obtains
\[
a_1a_2(a_3a_4a_5)^ka_6a_7 \in L(M)
\]
for all $k$, including $k = 0$.

\begin{figure}[ht]
\begin{center}
\input{Fig/LongWord.pdf_t}
\end{center}
\caption{Idea of the pumping lemma: a long word contains a subword which can be repeated.}
\label{fig:LongWord}
\end{figure}

The existence of a cycle can be stated as follows.


\end{page}

%%%%%%%%%% output/995--6-3-2-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:995}

\begin{lem}
\label{lem:LongWord}
Any path of length $\ge n$ (the length of a path is the number of edges) in a graph with $n$ vertices contains a cycle.
Besides, there is a cycle within the first $n$ edges of this path.
\end{lem}

\end{page}

%%%%%%%%%% output/996--6-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:996}


We are now ready to state and prove the pumping lemma.


\end{page}

%%%%%%%%%% output/998--6-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:998}

\begin{thm}[Pumping lemma for regular languages]
Let $L$ be a regular language.
Then there is an integer $n$ such that for any word $z \in L$ of length $|z| \ge n$ the word $z$ can be represented as $z = uvw$ in such a way that
\[
|uv| \le n, \quad |v| \ge 1, \quad \text{ and for all }k \ge 0 \text{ one has } uv^kw \in L.
\]
\end{thm}

\end{page}

%%%%%%%%%% output/999--6-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:999}

\begin{proof}
Since $L$ is a regular language, there is a DFA accepting it.
Let $n$ be the number of states of such a DFA.
Then any word $z \in L$ determines a path of length $|z|$ from $q_0$ to one of the final states.
By Lemma \ref{lem:LongWord}, if $|z| \ge n$, then within the first $n$ edges of this path there is a cycle.
Let $u$ be the prefix of $z$ before the beginning of this cycle, let $v$ be the subword corresponding to the cycle, and let $w$ be the remaining suffix.
Then $z = uvw$, and the path determined by the word $uv^kw$ differs from the path determined by $uvw$ in the number of times it runs along the cycle of $v$,
but has the same endpoint, a final state of the DFA.
Thus $uv^kw \in L$ for all $k \ge 0$.
\end{proof}


\end{page}

%%%%%%%%%% output/1001--6-3-2-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:1001}

\begin{exl}
Take the alphabet of one symbol $\Sigma = \{0\}$
and consider the language $L = \{0^{k^2} \mid k \text{ is a positive integer}\}$:
all sequences of $0$'s whose length is a perfect square.
Let us show that $L$ is not regular. Assume the contrary, and let $n$ be the integer in the pumping lemma.
Let $z = 0^{n^2}$. By the pumping lemma, $z = uvw$, where $1 \le |v| \le |uv| \le n$, and $uv^kw \in L$ for all $k$.
For $k=2$ one has $n^2+1 \le |uv^2w| \le n^2 + n < (n+1)^2$, thus $uv^2w \notin L$.
This contradiction shows that our assumption was false, and $L$ is not regular.
\end{exl}

\end{page}

%%%%%%%%%% output/1002--6-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:1002}


The only property of the sequence of perfect squares that was used is the existence of arbitrarily large gaps.
Therefore any language of the form $\{0^{a_k}\}$,
where $a_k$ is a monotone sequence of integers such that for every $n$ there is $k$ such that $a_{k+1} - a_k > n$, is non-regular.
Later we will be able to show that the only regular languages of the form $\{0^{a_k}\}$ are those for which the sequence $a_k$ is periodic.


\end{page}

%%%%%%%%%% output/1004--6-3-2-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1004}

\begin{exl}
The language $L$ consisting of all binary words with an equal number of zeros and ones is not regular.
Indeed, assume the contrary, and let $n$ be the integer in the pumping lemma.
Then $0^n1^n \in L$.
By the pumping lemma, one can write $0^n1^n = uvw$, where $|uv| \le n$, and $uv^kw \in L$ for all $k$.
It follows that the words $u$ and $v$ consist of zeros only.
Since $|v| \ge 1$, the word $uw = uv^0w$ has less zeros than ones, thus it does not belong to $L$.
This contradiction shows that our assumption was false.
\end{exl}

\end{page}

%%%%%%%%%% output/1005--6-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1005}



The pumping lemma can be used to prove the following theorem.


\end{page}

%%%%%%%%%% output/1007--6-3-2-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:1007}

\begin{thm}
A language accepted by a DFA with $n$ states is
\begin{enumerate}
\item
nonempty if and only if the automaton accepts some word of length less than $n$;
\item
infinite if and only if the automaton accepts some word of length $\ell$, where $n \le \ell < 2n$.
\end{enumerate}
\end{thm}

\end{page}

%%%%%%%%%% output/1008--6-3-2-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:1008}

\begin{proof}
1) The path corresponding to the shortest accepted word does not visit any state more than once.
Otherwise it contains a cycle, and by removing this cycle we obtain a shorter accepted word.
Therefore the length of the shortest accepted word is strictly less than $n$.

2) \emph{The ``if'' direction.} If $z$ is an accepted word of length $\ge n$, then its path contains a cycle.
By pumping this cycle, we obtain infinitely many accepted words.

\emph{The ``only if'' direction.} If the language is infinite, then it contains a word $z$ with $|z| \ge n$.
If $|z| < 2n$, then we are done.
If $|z| \ge 2n$, then write $z$ as $z = uvw$ according to the pumping lemma.
Then $uw$ is also accepted, and we have $|uw| = |z| - |v| \ge |z| - n \ge n$.
Thus we can apply the same case distiction to the word $uw$ and proceed until we get a word of length $\ge n$ and $< 2n$.
\end{proof}




\end{page}

%%%%%%%%%% output/1009--6-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:1009}

\subsection{Closure under homomorphisms}
Let $\Sigma$ and $\Delta$ be two finite alphabets.


\end{page}

%%%%%%%%%% output/1011--6-3-3-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:1011}

\begin{dfn}
A \emph{homomorphism} is a map $h \colon \Sigma^* \to \Delta^*$ such that
\[
h(xy) = h(x)h(y) \text{ for all }x, y \in \Sigma^*.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/1014--6-3-3-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:1014}

\begin{lem}
A homomorphism is uniquely determined by the images of the letters of the alphabet $\Sigma$.
That is, any $h \colon \Sigma \to \Delta^*$ extends to a unique homomorphism.
\end{lem}

\end{page}

%%%%%%%%%% output/1015--6-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:1015}

\begin{proof}
Let us prove the uniqueness.
If we know $h(a)$ for all $a \in \Sigma$, then we have no other choice but put
\[
h(a_1 \ldots a_n) = h(a_1) \ldots h(a_n).
\]
Also by definition of a homomorphism one has
\[
h(x) = h(\epsilon x) = h(\epsilon) h(x),
\]
which implies $h(\epsilon) = \epsilon$.
Thus there is no more than one homomorphism with given values on the letters of the alphabet.

On the other hand, putting $h(a_1 \ldots a_n) = h(a_1) \ldots h(a_n)$ and $h(\epsilon) = \epsilon$ defines a homomorphism,
so the extension exists and is unique.
\end{proof}


\end{page}

%%%%%%%%%% output/1017--6-3-3-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{10}
\label{portion:1017}

\begin{dfn}
Let $L \subset \Sigma^*$ be a language. The \emph{homomorphic image} of $L$ is the language
\[
h(L) = \{h(w) \mid w \in L\} \subset \Delta^*,
\]
where $h \colon \Sigma^* \to \Delta^*$ is some homomorphism.
\end{dfn}

\end{page}

%%%%%%%%%% output/1020--6-3-3-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{11}
\label{portion:1020}

\begin{exl}
\begin{itemize}
\item
$\Sigma = \Delta = \{0\}$, $L = \Sigma^*$, $h(0) = 00$. Then $h(L)$ consists of all even length sequences of zeros.
\item
$\Sigma = \Delta = \{0, 1\}$, $L = \Sigma^*$, $h(0) = 0, h(1) = 10$. For every word $w$ its image $h(w)$ is obtained by inserting a $0$ after every $1$.
The language $h(L)$ consists of all words without two consecutive $1$'s and not ending with $1$.
\end{itemize}
\end{exl}

\end{page}

%%%%%%%%%% output/1023--6-3-3-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{12}
\label{portion:1023}

\begin{thm}
A homomorphic image of a regular language is regular.
\end{thm}

\end{page}

%%%%%%%%%% output/1024--6-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{12}
\label{portion:1024}

\begin{proof}
Let $L$ be a regular language, and let $r$ be a regular expression representing $L$.
Make a substitution in $r$, replacing every symbol by its image under the homomorphism.
The result is a regular expression in the alphabet $\Delta$; denote it by $h(r)$.
Then the language defined by $h(r)$ is $h(L)$, thus $h(L)$ is regular.

The claim $L(h(r)) = h(L(r))$ is proved by induction on the complexity of the expression $r$.
Here is the induction step to $h = h_1 + h_2$:
\begin{multline*}
L(h(r_1+r_2)) = L(h(r_1)+h(r_2)) = L(h(r_1)) \cup L(h(r_2))\\
= h(L(r_1)) \cup h(L(r_2)) = h(L(r_1) \cup L(r_2)) = h(L(r_1+r_2)).
\end{multline*}
\end{proof}

For example, if $h(0) = 0$ and $h(1) = 10$, then $h((0+1)^*) = (0+10)^*$.

The next example shows that a homomorphic image of a non-regular language can be regular.


\end{page}

%%%%%%%%%% output/1026--6-3-3-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{13}
\label{portion:1026}

\begin{exl}
The language $\{0^k1^k \mid k \ge 0\}$ is non-regular, as can be shown with the help of pumping lemma.
But under the homomorphism $h(0) = 0, h(1) = 0$ it goes to the language $\{0^{2k} \mid k \ge 0\}$, which is regular.
\end{exl}

\end{page}

%%%%%%%%%% output/1029--6-3-3-lemma.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{dfn}{14}
\label{portion:1029}

\begin{lem}
The language of all propositional formulas is not regular.
\end{lem}

\end{page}

%%%%%%%%%% output/1030--6-3-3-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:1030}

\begin{proof}
Let $\Sigma$ be the alphabet of the propositional logic.
Consider the homomorphism $h \colon \Sigma^* \to \{0, 1\}^*$ defined by
\[
( \mapsto 0, \quad ) \mapsto 1, \quad x \mapsto \epsilon \text{ for all other symbols of }\Sigma.
\]
Every propositional formula is sent to a balanced bracket sequence (now represented by zeros and ones),
and every balanced bracket sequence is the image of some propositional formula.
If the language of all propositional formulas is regular, then the language of all balanced bracket sequences is also regular.
But it is not, by the pumping lemma: if $n$ is the number from the lemma, then $0^n1^n = uvw$ belongs to the language, but $uv^2w$ is unbalanced.
\end{proof}





\end{page}

%%%%%%%%%% output/1031--6-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{14}
\label{portion:1031}

\subsection{Closure under inverse homomorphism}

\end{page}

%%%%%%%%%% output/1033--6-3-4-definition.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{15}
\label{portion:1033}

\begin{dfn}
Let $h \colon \Sigma^* \to \Delta^*$ be a homomorphism, and $L \subset \Delta^*$ be a language in the alphabet $\Delta$.
The \emph{inverse homomorphic image} of $L$ is
\[
h^{-1}(L) = \{x \in \Sigma^* \mid h(x) \in L\}.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/1036--6-3-4-example.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{16}
\label{portion:1036}

\begin{exl}
Let $\Sigma = \{a,b\}$, $\Delta = \{0, 1\}$, and $h(a) = 01$, $h(b) = 10$.
Then
\[
h^{-1}(1001) = \{ba\}, \quad h^{-1}(0011) = \emptyset.
\]
For $L = (00 + 1)^*$ one has $h^{-1}(L) = (ba)^*$ (check this!).
One has $h(h^{-1}(L)) = (1001)^* \ne L$.
\end{exl}

\end{page}

%%%%%%%%%% output/1037--6-3-4-other.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{16}
\label{portion:1037}


One always has
\[
h(h^{-1}(L)) \subset L \subset h^{-1}(h(L)),
\]
and the inclusions can be strict.


\end{page}

%%%%%%%%%% output/1039--6-3-4-theorem.tex
\begin{page}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{dfn}{17}
\label{portion:1039}

\begin{thm}
An inverse homomorphic image of a regular language is regular.
\end{thm}

\end{page}

%%%%%%%%%% output/1040--6-3-4-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{dfn}{17}
\label{portion:1040}

\begin{proof}
Let $L \subset \Delta^*$ be a regular language, and $h \colon \Sigma^* \to \Delta^*$ a homomorphism.
Let $M = (Q, \Delta, \delta, q_0, F)$ be a DFA accepting $L$.
We will construct a DFA accepting $h^{-1}(L)$ thus proving that this language is regular.
The idea is to use the same set of states and the same set of final states, but interpret each symbol $a \in \Sigma$ as $h(a) \in \Delta$.
Then a word $w \in \Sigma^*$ will be accepted by the new automaton if and only if $h(w)$ was accepted by the old one.
Formally, put
\[
M' = (Q, \Sigma, \delta', q_0, F), \text{ where }\delta'(q, a) = \widehat{\delta}(q, h(a)).
\]
It can be shown by induction on the length of a word $w$ that $\widehat{\delta'}(q, w) = \widehat{\delta}(q, h(w))$.
Thus we have
\[
w \in L(M') \Leftrightarrow \widehat{\delta'}(q, w) \in F \Leftrightarrow \widehat{\delta}(q, h(w)) \in F \Leftrightarrow h(w) \in L
\Leftrightarrow w \in h^{-1}(L),
\]
which means $L(M') = h^{-1}(L)$.
\end{proof}






\end{page}

%%%%%%%%%% output/1041--6-4-0-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1041}

\section{The Myhill-Nerode theorem}

\end{page}

%%%%%%%%%% output/1042--6-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1042}

\subsection{Equivalence of words with respect to a language}

\end{page}

%%%%%%%%%% output/1044--6-4-1-definition.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1044}

\begin{dfn}
Let $L \subset \Sigma^*$ be a language.
Two words $u, v \in \Sigma^*$ are called \emph{$L$-equivalent} (written as $u \sim_L v$) if
\[
\forall x \in \Sigma^* \text{ either } ux, vx \in L \text{ or } ux, vx \notin L.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/1045--6-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1045}


In other words, $u$ and $v$ are \emph{not $L$-equivalent} if they have a \emph{distinguishing extension}:
a word $x \in \Sigma^*$ such that one of the words $ux$, $vx$ is in $L$, and the other not.


\end{page}

%%%%%%%%%% output/1047--6-4-1-example.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:1047}

\begin{exl}
\label{exl:LEquiv}
If $u \in L$ and $v \notin L$, then $u \not\sim_L v$.
Indeed, $\epsilon$ is a distinguishing extension for $u$ and $v$.
\end{exl}

\end{page}

%%%%%%%%%% output/1050--6-4-1-lemma.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:1050}

\begin{lem}
The relation $\sim_L$ is an equivalence relation.
\end{lem}

\end{page}

%%%%%%%%%% output/1051--6-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:1051}

\begin{proof}
The reflexivity and the symmetry are obvious.
To prove the transitivity, let $u \sim_L v$, $v \sim_L w$, and assume that $u \not\sim_L w$.
Then there is a distinguishing extension $x$ for $u$ and $w$.
Without loss of generality, $ux \in L$, $wx \notin L$.
Then if $vx \in L$, we have $v \not\sim_L w$, and if $vx \notin L$, we have $u \not\sim_L v$.
\end{proof}

An equivalence relation splits the set $\Sigma^*$ of all words into \emph{equivalence classes}:
\begin{equation}
\label{eqn:SigmaEqClasses}
\Sigma^* = S_0 \cup S_1 \cup S_2 \cup \cdots
\end{equation}
where $u \sim_L v$ if and only if $u$ and $v$ belong to the same class.
As we noticed in Example \ref{exl:LEquiv}, if $u \sim_L v$, then either $u,v \in L$ or $u,v \notin L$.
It follows that every class $S_i$ is either contained in $L$ or disjoint from $L$.


\end{page}

%%%%%%%%%% output/1053--6-4-1-example.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:1053}

\begin{exl}
Let $L$ consist of all binary words with the number of zeros not divisible by $3$.
Then $u \sim_L v$ if and only if the number of zeros in $u$ and $v$ has the same remainder under division by $3$:
\[
\Sigma^* = S_0 \cup S_1 \cup S_2, \quad S_i = \{u \mid \ell_0(u) \equiv i (\mathrm{mod}\, 3)\}.
\]
One has $L = S_1 \cup S_2$.
\end{exl}

\end{page}

%%%%%%%%%% output/1056--6-4-1-example.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{5}
\label{portion:1056}

\begin{exl}
Let $L$ be the set of all binary words with equal numbers of zeros and ones.
Then $u \sim_L v$ if and only if the difference between the numbers of zeros and ones in $u$ and in $v$ is the same:
\[
\Sigma^* = \cdots \cup S_{-2} \cup S_{-1} \cup S_0 \cup S_1 \cup S_2 \cup \cdots, \quad S_i = \{u \mid \ell_0(u) - \ell_1(u) = i\}.
\]
One has $L = S_0$.
\end{exl}

\end{page}

%%%%%%%%%% output/1059--6-4-1-lemma.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{dfn}{6}
\label{portion:1059}

\begin{lem}
\label{lem:LEquivRInvar}
If $u \sim_L v$, then $ux \sim_L vx$ for all $x \in \Sigma^*$.
\end{lem}

\end{page}

%%%%%%%%%% output/1060--6-4-1-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1060}

\begin{proof}
Assume that $ux \not\sim_L vx$, and let $y$ be a distinguishing extension.
Then $xy$ is a distinguishing extension for $u$ and $v$.
\end{proof}




\end{page}

%%%%%%%%%% output/1061--6-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1061}

\subsection{The theorem}

\end{page}

%%%%%%%%%% output/1063--6-4-2-theorem.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:1063}

\begin{thm}
A language $L$ is regular if and only if the number of $L$-equivalence classes is finite.
\end{thm}

\end{page}

%%%%%%%%%% output/1064--6-4-2-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:1064}

\begin{proof}
Assume that $L$ is regular.
Take a DFA that accepts $L$.
Let $q_0, \ldots, q_n$ be its states, and $q_0$ be the initial state.
Denote
\[
T_i = \{w \in \Sigma^* \mid \widehat{\delta}(q_0, w) = q_i\}.
\]
One has
\[
\Sigma^* = T_0 \cup T_1 \cup \cdots T_n.
\]
We claim that every $T_i$ is a subset of some $S_j$ from the decomposition \eqref{eqn:SigmaEqClasses}.
In other words, every $S_j$ is the union of one or several $T_i$,
which means that the number of $\sim_L$-equivalence classes is at most $n+1$ and implies the first part of the theorem.

In order to prove the claim it suffices to show that if $u$ and $v$ belong to the same $T_i$, then $u \sim_L v$.
Then for every $x \in \Sigma^*$ one has
\[
\widehat{\delta}(q_0, ux) = \widehat{\delta}(\widehat{\delta}(q_0,u), x) = \widehat{\delta}(q_i, x)
= \widehat{\delta}(\widehat{\delta}(q_0,v), x) = \widehat{\delta}(q_0, vx).
\]
Since both words $ux$ and $vx$ bring us to the same state, they either both belong to $L$ (if this state is final)
or both not belong to $L$ (if this state is not final).
Thus $u \sim_L v$.


In the opposite direction, let $\Sigma^* = S_0 \cup \cdots \cup S_n$, where $\epsilon \in S_0$.
Construct a DFA with states $q_0, \ldots, q_n$, the initial state $q_0$, and the transition function defined as follows.
To find $\delta(q_i, a)$, take some $u \in S_i$ and look in which class the word $ua$ lies.
If $ua \in S_j$, then put $\delta(q_i, a) = q_j$.
The result is independent of the choice of a representative $u \in S_i$.
Indeed, by Lemma \ref{lem:LEquivRInvar} $u \sim_L v \Rightarrow ua \sim_L va$.
A state $q_i$ is designated as final if and only if $S_i \subset L$.
It is easy to see that the language accepted by this automaton is $L$.
\end{proof}



\end{page}

%%%%%%%%%% output/1065--6-4-3-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:1065}

\subsection{Minimization of a DFA}
Myhill-Nerode theorem implies


\end{page}

%%%%%%%%%% output/1067--6-4-3-corollary.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:1067}

\begin{cor}
The minimum number of states in a DFA accepting a regular language $L$ is equal to the number of $L$-equivalence classes.
The minimal DFA is unique up to renaming the states.
\end{cor}

\end{page}

%%%%%%%%%% output/1068--6-4-3-other.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:1068}



From any DFA one can construct the minimum DFA accepting the same language by merging certain sets of states into one state.
Two states $q_i$, $q_j$ must be merged if the corresponding sets $T_i$, $T_j$ are contained in the same equivalence class $S_k$.
That is, $q_i \sim q_j$ if for all $x \in \Sigma^*$ either both $\widehat{\delta}(q_i, x)$ and $\widehat{\delta}(q_j, x)$ belong to $F$ or both do not.

One can certify non-equivalence of two states by finding a word $x$ such that $\widehat{\delta}(q_i, x) \in F$ and $\widehat{\delta}(q_j, x) \notin F$
or vice versa.
The algorithm marks pairs of distinguishable states recursively.

At the very beginning one removes all inaccessible states.
Obviously, this does not change the accepted language.

Then one draws a table whose rows and columns are marked by the remaining states.
As initialization, one marks all pairs $(q_i,q_j)$ such that $q_i \in F$ and $q_j \notin F$ or vice versa.
At each of the following steps one goes through all pairs $(q_i,q_j)$, and for each of them one considers all alphabet letters $a$.
If the pair $(\delta(q_i,a), \delta(q_j,a))$ was marked at one of the previous steps, then one marks the pair $(q_i, q_j)$.
If at some step no new pairs are marked, then the algorithm stops.
All pairs of states which are unmarked are merged into a single state
(it is also possible that several states are merged into one state).


\end{page}

%%%%%%%%%% output/1070--6-4-3-example.tex
\begin{page}
\setcounter{section}{4}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:1070}

\begin{exl}
Consider the DFA in Figure \ref{fig:DFAMinim}.
It has one inaccessible state~$q_3$.

\begin{figure}[ht]
\begin{center}
\input{Fig/DFAMinim.pdf_t}
\end{center}
\caption{A non-minimal DFA.}
\label{fig:DFAMinim}
\end{figure}

We thus draw a $7 \times 7$ table and fill it step by step.
It suffices to fill only a half of the table, on one side of the diagonal.

\renewcommand{\arraystretch}{1.7}
% \begin{figure}[ht]
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|}
& $q_0$ & $q_1$ & $q_2$ & $q_4$ & $q_5$ & $q_6$ & $q_7$\\\hline
$q_0$ & \cellcolor{lightgray} &&&&&&\\\hline
$q_1$ & $\times_1$ & \cellcolor{lightgray} &&&&&\\\hline
$q_2$ & $\times_0$ & $\times_0$ & \cellcolor{lightgray} &&&&\\\hline
$q_4$ &  & $\times_1$ & $\times_0$ & \cellcolor{lightgray} &&&\\\hline
$q_5$ & $\times_1$ & $\times_1$ & $\times_0$ & $\times_1$ & \cellcolor{lightgray} &&\\\hline
$q_6$ & $\times_2$ & $\times_1$ & $\times_0$ & $\times_2$ & $\times_1$ & \cellcolor{lightgray} &\\\hline
$q_7$ & $\times_1$ &  & $\times_0$ & $\times_1$ & $\times_1$ & $\times_1$ & \cellcolor{lightgray}\\\hline
\end{tabular}
% \caption{Calculation of equivalent states. The mark $\times_n$ means that the corresponding pair of states was recognized as non-equivalent at the step $n$.}
% \label{fig:DFAMinimTable}
\end{center}
% \end{figure}

A pair of states is marked with $\times_n$ if these states were recognized as non-equivalent at Step $n$.
As initialization (Step 0) we mark all pairs $(q_2, q_i)$ because $q_2$ is the only final state.

A lot of cells are marked at Step 1.
These are all pairs $(q_i, q_j)$ such that either the $0$-arrows or the $1$-arrows lead from $q_i$ to a final and from $q_j$ to a non-final state or vice versa.
For example, we mark $(q_0, q_1)$ because $\delta(q_0, 1) = q_5$ is non-final and $\delta(q_1, 1) = q_2$ is final.

At Step 2 two cells are marked.
For example, we mark $(q_4, q_6)$ because $\delta(q_4, 0) = q_7$, $\delta(q_6, 0) = q_6$, and the pair $(q_6, q_7)$ is already marked (it was marked at Step 1).

At Step 3 we check all pairs of unmarked cells, by looking where the $0$- and $1$-arrows lead, but do not find anything that should be marked.
Thus the algorithm stops, and the minimal DFA is obtained from the one in Figure~\ref{fig:DFAMinim} by removing the state $q_3$,
merging $q_0$ with $q_4$ and merging $q_1$ with $q_7$. See Figure \ref{fig:DFAMinimResult}.

\begin{figure}[ht]
\begin{center}
\input{Fig/DFAMinimResult.pdf_t}
\end{center}
\caption{The minimal DFA equivalent to one in Figure \ref{fig:DFAMinim}.}
\label{fig:DFAMinimResult}
\end{figure}
\end{exl}

\end{page}

%%%%%%%%%% output/1071--6-4-3-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{dfn}{9}
\label{portion:1071}





\newpage


\end{page}

%%%%%%%%%% output/1072--6-5-0-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1072}

\section{Context-free grammars and languages}

\end{page}

%%%%%%%%%% output/1073--6-5-1-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1073}

\subsection{Generating a language by a grammar}
\label{sec:LangGram}
Consider the sentence
\begin{center}
\emph{Colorless green ideas sleep furiously.}
\end{center}
Although it does not make any sense, it is syntactically correct.
One distinguishes a noun phrase ``colorless green ideas'' and a verb phrase ``sleep furiously''.
The noun phrase itself consists of a noun preceeded by adjectives, and the verb phrase consists of a verb followed by adverbs.

More generally, one can formulate the following rules of production of simple English sentences:
\begin{align*}
&S \to NV &&\text{a sentence consists of a noun phrase and a verb phrase}\\
&N \to AdjN &&\text{a noun phrase may start with one or more adjectives}\\
&V \to VAdv &&\text{a verb phrase may end with one or more adverbs}
\end{align*}
At any stage one can substitute for $N$, $V$, $Adj$, $Adv$ a word from a dictionary:
\begin{align*}
&N \to \text{list of nouns}\\
&V \to \text{list of verbs}\\
&Adj \to \text{list of adjectives}\\
&Adv \to \text{list of adverbs}
\end{align*}
The result is a syntactically correct (but mostly meaningless) sentence.

A production can be represented linearly:
\begin{multline*}
S \to NV \to AdjNV \to AdjAdjNV \to \text{colorless }AdjNV\\
\to \text{colorless }AdjNVAdv \to \cdots 
\end{multline*}
or by a \emph{derivation tree} or \emph{parse tree}, see Figure \ref{fig:Chomsky}.

\begin{figure}[ht]
\begin{center}
\includegraphics{Chomsky.pdf}
\end{center}
\caption{Derivation tree for the Chomsky example.}
\label{fig:Chomsky}
\end{figure}

The sentence at the beginning of this section is from a book of Noam Chomsky.
In mid-1950's he proposed the above principles as description of the structure of human languages.
(Of course one needs more production rules in order to be able to generate more complicated sentences.)
A couple of years later John Backus, a programming language designer acquainted with Chomsky's ideas,
described the syntax of the ALGOL programming language in a similar way.

Let us now give an exact definition.

\end{page}

%%%%%%%%%% output/1075--6-5-1-definition.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1075}

\begin{dfn}
A \emph{context-free grammar} (or \emph{CFG} or just \emph{grammar}) is a quadruple $G = (V, T, P, S)$, where
\begin{itemize}
\item
$V$ is a finite set of \emph{variables};
\item
$T$ is a finite set of \emph{terminals};
\item
$P$ is a finite set of productions, each production is of the form $A \to \alpha$, where $\alpha \in (V \cup T)^*$;
\item
$S$ is a special variable ($S \in V$) called the \emph{start symbol}.
\end{itemize}
\end{dfn}

\end{page}

%%%%%%%%%% output/1078--6-5-1-example.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:1078}

\begin{exl}
In our introductory example $V = \{S, N, V, Adj, Adv\}$, $T$ is the set of words in a dictionary, and the productions $P$ are as stated above.
\end{exl}

\end{page}

%%%%%%%%%% output/1079--6-5-1-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:1079}


Note that the alphabet $V$ of variables and the alphabet $T$ of terminals must be disjoint.
To avoid confusion, we will use different symbols in the following way.
\begin{itemize}
\item
The capital letters $A, B, C, \ldots$ are variables.
\item
The letters $a, b, c$ and digits are terminals.
\item
The letters $X, Y, Z$ denote symbols that may be variables or terminals.
\item
The letters $u, v, w, x, y, z$ are used to denote strings of terminals, that is elements of $T^*$.
\item
The letters $\alpha, \beta, \gamma$ are used to denote strings of variables and terminals, that is elements of $(V \cup T)^*$.
\end{itemize}


\end{page}

%%%%%%%%%% output/1081--6-5-1-definition.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:1081}

\begin{dfn}
The \emph{language generated by a grammar} is the set of all words in the alphabet $T$ that can be derived from the start symbol $S$
according to the production rules.
\end{dfn}

\end{page}

%%%%%%%%%% output/1082--6-5-1-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:1082}


In order to describe formally what it means that a word can be derived from the start symbol, let us fix some notations and terminology.
If $A \to \beta$ is any production in $P$, and $\alpha, \gamma \in (V \cup T)^*$,
then we write $\alpha A \gamma \xRightarrow[G]{} \alpha \beta \gamma$
and say that $\alpha A \gamma$ \emph{directly derives} $\alpha\beta\gamma$ in grammar~$G$.
If $\alpha_1, \ldots, \alpha_n \in (V \cup T)^*$ are such that
\[
\alpha_1 \xRightarrow[G]{} \alpha_2, \quad \alpha_2 \xRightarrow[G]{} \alpha_3, \quad \ldots, \quad  \alpha_{n-1} \xRightarrow[G]{} \alpha_n,
\]
then we write $\alpha_1 \xRightarrow[G]{*} \alpha_n$ and say that $\alpha_1$ \emph{derives} $\alpha_n$ in $G$.
When it is clear which grammar we use, then we omit $G$ and write $\xRightarrow[]{}$ and $\xRightarrow[]{*}$, respectively.

Now we can describe the language generated by $G$ as
\[
L(G) = \{w \in T^* \mid S \xRightarrow[G]{*} w\}.
\]
A language is called a \emph{context-free language} (CFL) if it is generated by some context-free grammar.


\end{page}

%%%%%%%%%% output/1084--6-5-1-example.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:1084}

\begin{exl}
\label{exl:0n1n}
Let $V = \{S\}$, $T = \{0, 1\}$, $P = \{S \to 0S1, S \to \epsilon\}$.
From
\[
S \Rightarrow 0S1 \Rightarrow 00S11 \Rightarrow \cdots \Rightarrow 0^{n-1}S1^{n-1} \Rightarrow 0^n1^n
\]
we see that $L(G) = \{0^n1^n \mid n \ge 0\}$.
\end{exl}

\end{page}

%%%%%%%%%% output/1085--6-5-1-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:1085}


Note that the language $\{0^n1^n \mid n \ge 1\}$ is not regular, as can be shown with the help of the pumping lemma.
We will later see that every regular language is context-free.


\end{page}

%%%%%%%%%% output/1087--6-5-1-example.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{1}
\setcounter{dfn}{5}
\label{portion:1087}

\begin{exl}
The language of all binary words with equal numbers of $0$'s and $1$'s is context free.
However, the generating grammar is more complicated, see \cite[Example 4.3]{HU79}.
\end{exl}

\end{page}

%%%%%%%%%% output/1089--6-5-2-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:1089}

\subsection{Grammars in Chomsky form}
A grammar is said to be in \emph{Chomsky form} if all of its productions are of the form $A \to BC$ and $A \to a$.
According to our notation convention, $A, B, C$ are variables and $a$ is a terminal.
Note that the ``English grammar'' from the beginning of Section \ref{sec:LangGram} is in Chomsky form.


\end{page}

%%%%%%%%%% output/1091--6-5-2-theorem.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1091}

\begin{thm}
\label{thm:ChomskyForm}
Any context-free language without $\epsilon$ is generated by a grammar in Chomsky form.
\end{thm}

\end{page}

%%%%%%%%%% output/1092--6-5-2-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1092}


Two grammars are called \emph{equivalent} if they generate the same language.
Thus the above theorem can also be stated as
``for every grammar whose language does not contain $\epsilon$ there is an equivalent grammar in Chomsky form''.


\end{page}

%%%%%%%%%% output/1094--6-5-2-definition.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:1094}

\begin{dfn}
An \emph{$\epsilon$-production} is a production of the form $A \to \epsilon$.
A \emph{unit production} is a production of the form $A \to B$.
\end{dfn}

\end{page}

%%%%%%%%%% output/1095--6-5-2-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:1095}


Without $\epsilon$-productions it is impossible to produce the empty word,
so we cannot get rid of them if the language contains $\epsilon$.
On the other hand, if a grammar contains $\epsilon$-productions,
this does not necessarily mean that the generated language contains $\epsilon$.

Unit productions can be helpful, they introduce sort of ``branching''. For example, the grammar
\[
S \to A \mid B, \quad A \to AA \mid 0, \quad B \to BB \mid 1
\]
is a simple grammar generating the language $(0^* \cup 1^*) \setminus \{\epsilon\}$.



\end{page}

%%%%%%%%%% output/1097--6-5-2-lemma.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{8}
\label{portion:1097}

\begin{lem}
\label{lem:NoENoUnit}
Any context-free language without $\epsilon$ is generated by a grammar without $\epsilon$-productions and without unit productions.
\end{lem}

\end{page}

%%%%%%%%%% output/1098--6-5-2-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{8}
\label{portion:1098}

\begin{proof}
Let $G$ be a context-free grammar possibly containing $\epsilon$- and unit productions.
We construct a grammar $G'$ without $\epsilon$-productions such that $L(G') = L(G) \setminus \{\epsilon\}$.

First, identify \emph{nullable} variables, those which derive $\epsilon$.
This is done recursively.
Initialize the set of nullable variables by those $A$ for which there is a production $(A \to \epsilon) \in P$.
The recursion step adds to the set of nullable variables those $B$ for which $(B \to C_1 \cdots C_k) \in P$
and all $C_1, \ldots C_k$ are nullable.
As soon as this recursion does not find new nullable variables, the algorithm stops.

Second, remove from $P$ all $\epsilon$-productions $A \to \epsilon$ and add new productions in the following way.
Let $A \to X_1 \cdots X_n$ be a production with some of $X_i$ nullable variables
(recall that $X_i$ can stand for a variable symbol as well as for a terminal symbol).
We add all productions of the form $A \to X_1 \widehat{\cdots} X_n$,
where $\widehat{\cdots}$ means that we remove any subset of nullable variables
(with one exception: if all $X_1, \ldots, X_n$ are nullable,
then we do not add the production $A \to \epsilon$ obtained by removing all nullable variables).
That is, if $m$ symbols among $X_1, \ldots X_n$ are nullable variables, then the production $A \to X_1 \cdots X_n$
gives rise to $2^m$ productions if $m < n$ and to $2^n - 1$ productions if $m=n$.

It can be checked that the new set of productions allows to derive all words (except $\epsilon$) which were derivable
with the initial set of productions, and only those words.

Now we construct a grammar $G''$ equivalent to $G'$ but without unit productions.
Call a pair $(A, B)$ \emph{unit pair}, if $A \xRightarrow[]{*} B$.
The set of all unit pairs can be found recursively.

Remove all unit productions, and for each unit pair $(A, B)$ and each non-unit production $B \to \alpha$ add the production $A \to \alpha$.
Every word generated by the grammar $G''$ is also generated by $G'$:
the new direct productions $A \xRightarrow[G'']{} \alpha$ are compositions of two old productions
$\alpha A \beta \xRightarrow[G']{} \alpha B \beta \xRightarrow[G']{} \alpha$.
Every word generated by $G'$ is generated by $G''$: a series of unit productions must always end with a non-unit production,
so if we had $\alpha A \gamma \xRightarrow[G']{*} \alpha B \gamma \xRightarrow[G']{} \alpha\beta\gamma$,
then we have $\alpha A \gamma \xRightarrow[G''] \alpha\beta\gamma$.
Thus the new set of productions generates the same language as before.
\end{proof}
It is important to remove first the $\epsilon$-productions and then the unit productions.
If first the unit, and then $\epsilon$-productions are removed, then the result might contain unit productions.


\end{page}

%%%%%%%%%% output/1100--6-5-2-example.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{9}
\label{portion:1100}

\begin{exl}
Let us remove the $\epsilon$-productions from the grammar
\begin{align*}
S &\to ABA\\
A &\to aA \mid \epsilon\\
B &\to bB \mid b.
\end{align*}
Only the variable $A$ is nullable.
Remove the production $A \to \epsilon$.
The nullable variable $A$ appears on the right hand side of the productions $S \to ABA$ and $A \to aA$.
Add new productions by removing any number of $A$'s from the right hand sides of these productions:
\begin{align*}
S &\to ABA \mid AB \mid BA \mid B\\
A &\to aA \mid a\\
B &\to bB \mid b.
\end{align*}
\end{exl}

\end{page}

%%%%%%%%%% output/1103--6-5-2-example.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{10}
\label{portion:1103}

\begin{exl}
Let us remove the unit productions from the grammar
\begin{align*}
S &\to A \mid B\\
A &\to AA \mid 0\\
B &\to BB \mid 1.
\end{align*}
The unit pairs are $(S,A)$ and $(S,B)$.
Remove the unit productions and introduce all $S \to \alpha$ for which $A \to \alpha$ or $B \to \alpha$:
\begin{align*}
S &\to AA \mid BB \mid 0 \mid 1\\
A &\to AA \mid 0\\
B &\to BB \mid 1.
\end{align*}
\end{exl}

\end{page}

%%%%%%%%%% output/1104--6-5-2-other.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{10}
\label{portion:1104}




\begin{proof}[Proof of Theorem \ref{thm:ChomskyForm}]
Let $L$ be a language without $\epsilon$.
By Lemma \ref{lem:NoENoUnit} there is a grammar $G$ without $\epsilon$ and unit productions such that $L = L(G)$.
If a production of $G$ has a single symbol on the right, then this symbol is a terminal, so the production is of the form $A \to a$.

Any other production of $G$ has the form
\[
A \to X_1 X_2 \cdots X_n, \quad n \ge 2,
\]
where every $X_i$ is either a variable or a terminal.
If $X_i = a$ is a terminal, then introduce a new variable $C_a$ and a new production $C_a \to a$.
In the ``long'' ($n \ge 2$) right hand sides of all productions replace $a$ by $C_a$.
Clearly, the new grammar $G'$ generates the same language as the old one.

In the grammar $G'$, all productions are of the form $A \to a$ or $A \to B_1 \cdots B_n$, $n \ge 2$.
Create a new grammar $G''$ by introducing for each production of a ``long'' ($n \ge 3$) word a new set of variables $D_1, \ldots, D_{n-2}$
and replacing this production by a set of productions
\[
A \to B_1D_1, \quad D_1 \to B_2D_2, \ldots, \quad D_{n-3} \to B_{n-2}D_{n-2}, \quad D_{n-2} \to B_{n-1}B_n.
\]
Again, it is not hard to convince yourself that the new grammar generates the same language.
\end{proof}


\end{page}

%%%%%%%%%% output/1106--6-5-2-example.tex
\begin{page}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{dfn}{11}
\label{portion:1106}

\begin{exl}
Let us find a Chomsky form of the grammar
\begin{align*}
S &\to aB \mid bA\\
A &\to a \mid aS \mid bAA\\
B &\to b \mid bS \mid aBB
\end{align*}
By introducing variables $C_a$ and $C_b$ we get rid of terminals in long words:
\begin{align*}
S &\to C_aB \mid C_bA\\
A &\to a \mid C_aS \mid C_bAA\\
B &\to b \mid C_bS \mid C_aBB\\
C_a &\to a\\
C_b &\to b
\end{align*}
There are two productions $A \to C_bAA$ and $B \to C_aBB$ which have words of length $\ge 3$ on the right hand side.
Replace them by two short productions each:
\begin{align*}
S &\to C_aB \mid C_bA\\
A &\to a \mid C_aS \mid C_bD_1\\
B &\to b \mid C_bS \mid C_aD_2\\
C_a &\to a\\
C_b &\to b\\
D_1 &\to AA\\
D_2 &\to BB
\end{align*}
\end{exl}

\end{page}

%%%%%%%%%% output/1108--6-6-0-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1108}

\section{Pushdown automata}

\end{page}

%%%%%%%%%% output/1109--6-6-1-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1109}

\subsection{Definition}
A pushdown automaton is similar to a finite automaton:
it has a finite number of states and changes the state according to the input.
But it has additional memory, in the form of a stack of unbounded depth.
At every step the automaton reads the input symbol and the top symbol of the stack.
According to these data, the automaton changes its state and rewrites the top of the stack.

We proceed to a formal definition.

\end{page}

%%%%%%%%%% output/1111--6-6-1-definition.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1111}

\begin{dfn}
A \emph{pushdown automaton} or a PDA is a system $(Q, \Sigma, \Gamma, \delta, q_0, Z_0, F)$, where
\begin{itemize}
\item
$Q$ is a (finite) set of states, $q_0 \in Q$ the initial state, $F \subset Q$ the set of final states;
\item
$\Sigma$ is the input alphabet, $\Gamma$ is the stack alphabet, $Z_0 \in \Gamma$ is a special symbol called the \emph{start symbol};
\item
$\delta$ is a map from $Q \times (\Sigma \cup \{\epsilon\}) \times \Gamma$ to finite subsets of $Q \times \Gamma^*$.
\end{itemize}
\end{dfn}

\end{page}

%%%%%%%%%% output/1112--6-6-1-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1112}

At the beginning, the automaton is in the state $q_0$, and the stack contains the symbol $Z_0$.

The transition function $\delta$ takes three arguments:
the current state of the automaton, an input symbol or $\epsilon$, and the top symbol of the stack.
The value of the transition function
\[
\delta(q, a, Z) = \{(p_1, \gamma_1), \ldots, (p_m, \gamma_m)\}, \quad \gamma_i \in \Gamma^*,
\]
is interpreted as follows.
If the automaton in the state $q$ reads the input symbol $a$ and sees the symbol $Z$ on the top of the stack,
then, for a random $i$, it enters the state $p_i$ and replaces the symbol $Z$ by the string $\gamma_i$.


\end{page}

%%%%%%%%%% output/1114--6-6-1-example.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:1114}

\begin{exl}
The rule $\delta(q, a, Z) = \{(p_1, BZ), (p_2, \epsilon)\}$ means that the automaton can either go to state $p_1$ and put $B$ into the stack
or go to state $p_2$ and take $Z$ out of the stack.
\end{exl}

\end{page}

%%%%%%%%%% output/1115--6-6-1-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:1115}


By definition, $\epsilon$-transitions are allowed:
\[
\delta(q, \epsilon, Z) = \{(p_1, \gamma_1), \ldots, (p_m, \gamma_m)\}
\]
means that from the state $q$ and with $Z$ on the top of the stack the automaton can do a spontaneous transition
to a state $p_i$ and replace $Z$ with $\gamma_i$.


\end{page}

%%%%%%%%%% output/1117--6-6-1-example.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:1117}

\begin{exl}
The rule $\delta(q, \epsilon, Z) = \emptyset$ means that no spontaneous transition (from the state $q$ with $Z$ on the top) is allowed.
\end{exl}

\end{page}

%%%%%%%%%% output/1118--6-6-1-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:1118}


There are two versions of acceptance criteria.
In the first, a word $w$ is accepted if some sequence of moves corresponding to the input $w$ leads to an empty stack
(which means that the start symbol $Z_0$ on the bottom of the stack should also be removed).
In this case the set of final states is irrelevant, one may put $F = \emptyset$.

In the second version, a word $w$ is accepted if some sequence of moves corresponding to the input $w$ brings the automaton to a final state.

A PDA is \emph{deterministic} if every set $\delta(q, a, Z) \cup \delta(q, \epsilon, Z)$ contains at most one element
(from every state and every input symbol at most one move is possible, taking $\epsilon$-transitions into account).
Note however that $\delta(q, a, Z) = \emptyset$ means that in the state $q$ with top stack symbol $Z$
the input symbol $a$ is rejected (or ``breaks'' the automaton).


\end{page}

%%%%%%%%%% output/1120--6-6-1-example.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:1120}

\begin{exl}
Consider the language of binary palindromes with a symbol $c$ (``center'') in the middle:
\[
L = \{w c \bar{w} \mid w \in \{0,1\}^*\}.
\]
We describe a deterministic PDA accepting $L$ by the empty stack. Put
\[
M = (\{q_1, q_2\}, \{0, 1, c\}, \{A, B, Z_0\}, \delta, q_1, Z_0, \emptyset\}.
\]
While the automaton is in the state $q_1$, it stores the input in the stack encoding $0$ with $A$ and $1$ with $B$:
\[
\delta(q_1, 0, Z) = (q_1, AZ) \quad \delta(q_1, 1, Z) = (q_1, BZ) \text{ for all }Z \in \Gamma.
\]
If the input symbol is $c$, then the automaton switches to the state $q_2$:
\[
\delta(q_1, c, Z) = (q_2, Z) \text{ for all }Z \in \Gamma.
\]
While in the state $q_2$, the automaton compares the input symbol with the top symbol in the stack.
If the symbols agree, then the top symbol is removed; if they disagree, the automaton ``breaks down''.
\[
\delta(q_2, 0, A) = (q_2, \epsilon) \quad \delta(q_2, 1, B) = (q_2, \epsilon)
\]
Finally, when the automaton sees the start symbol at the bottom of the stack, this symbol is removed, and the word is accepted:
\[
\delta(q_2, \epsilon, Z_0) = (q_2, \epsilon).
\]
\end{exl}

\end{page}

%%%%%%%%%% output/1121--6-6-1-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:1121}


Contrarily to DFA, non-deterministic PDAs are stronger than deterministic ones.
The language of binary palindromes of even length cannot be accepted by a deterministic PDA
but is accepted by a non-deterministic one as the next example shows.


\end{page}

%%%%%%%%%% output/1123--6-6-1-example.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{dfn}{5}
\label{portion:1123}

\begin{exl}
\label{exl:Palindromes}
A PDA accepting the language $\{w \bar{w} \mid w \in \{0,1\}^*\}$ by the empty stack.
\[
M = (\{q_1, q_2\}, \{0, 1\}, \{A, B, Z_0\}, \delta, q_1, Z_0, \emptyset\}
\]
The principle is the same: while in the state $q_1$, we encode the input by putting into the stack $A$ for the input symbol $0$ and $B$ for the input $1$:
\begin{gather*}
\delta(q_1, 0, Z_0) = (q_1, AZ_0), \quad \delta(q_1, 0, B) = (q_1, AB)\\
\delta(q_1, 1, Z_0) = (q_1, BZ_0), \quad \delta(q_1, 1, A) = (q_1, BA)
\end{gather*}
However, if the input symbol agrees with the top stack symbol, then this might be the middle of the palindrome (but also might be not).
So, we make a guess and allow a multiple transition:
\[
\delta(q_1, 0, A) = \{(q_1, AA), (q_2, \epsilon)\}, \quad \delta(q_1, 1, B) = \{(q_1, BB), (q_2, \epsilon)\}.
\]
While in the state $q_2$, we compare the input with the content of the stack:
\[
\delta(q_2, 0, A) = (q_2, \epsilon) \quad \delta(q_2, 1, B) = (q_2, \epsilon)
\]
Finally, we have the possibility to empty the stack spontaneously if its top symbol is $Z_0$,
because this can happen only in the case if the input word was a palindrome (including the empty input):
\[
\delta(q_1, \epsilon, Z_0) = (q_2, \epsilon) \quad \delta(q_2, \epsilon, Z_0) = (q_2, \epsilon)
\]
\end{exl}

\end{page}

%%%%%%%%%% output/1124--6-6-1-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:1124}


Note that when the stack is empty, a PDA stops and does not accept any more input.



\end{page}

%%%%%%%%%% output/1125--6-6-2-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:1125}

\subsection{Instantaneous description}
The configuration of a PDA at any given moment can be represented as follows.


\end{page}

%%%%%%%%%% output/1127--6-6-2-definition.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1127}

\begin{dfn}
An \emph{instantaneous description} or \emph{ID} of a PDA is a triple $(q, w, \gamma)$,
where $q \in Q$ is the current state, $w \in \Sigma^*$ is the not yet processed suffix of the input word,
and $\gamma \in \Gamma^*$ is the content of the stack.
\end{dfn}

\end{page}

%%%%%%%%%% output/1128--6-6-2-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1128}


The transition from one configuration to another is denoted by the symbol $\specialvdash{M}{}$.
By definition of the transition function one has
\[
(q, aw, Z\alpha) \specialvdash{M}{} (p, w, \beta\alpha) \Leftrightarrow (p, \beta) \in \delta(q, a, Z),
\]
where $a \in \Sigma \cup \{\epsilon\}$.

We write $I \specialvdash{M}{*} J$ if for some $I_1, \ldots, I_n$ one has
\[
I \specialvdash{M}{} I_1 \specialvdash{M}{} I_2 \specialvdash{M}{} \cdots \specialvdash{M}{} I_n \specialvdash{M}{} J.
\]

In these terms, the acceptance criteria by final state and by empty stack are formulated as follows.


\end{page}

%%%%%%%%%% output/1130--6-6-2-definition.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{2}
\setcounter{dfn}{7}
\label{portion:1130}

\begin{dfn}
For a PDA $M$ the \emph{language accepted by final state} is
\[
L(M) = \{w \mid (q_0, w, Z_0) \specialvdash{M}{*} (p, \epsilon, \gamma) \text{ for some }p \in F \text{ and } \gamma \in \Gamma^*\},
\]
and the \emph{language accepted by empty stack} is
\[
N(M) = \{w \mid (q_0, w, Z_0) \specialvdash{M}{*} (p, \epsilon, \epsilon) \text{ for some }p \in Q\}.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/1132--6-6-3-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:1132}

\subsection{Equivalence of acceptance by final state and empty stack}

\end{page}

%%%%%%%%%% output/1134--6-6-3-theorem.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:1134}

\begin{thm}
If $L = L(M)$ for some PDA $M$, then there is a PDA $M'$ such that $L = N(M')$.
\end{thm}

\end{page}

%%%%%%%%%% output/1135--6-6-3-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:1135}

\begin{proof}[Idea of the proof]
To replace acceptance by final states with acceptance by empty stack, add a new state (erasure state),
allow the automaton to go to this state as soon as it enters a final state,
and while in the erasure state delete the top stack symbols spontaneously.
\end{proof}


\end{page}

%%%%%%%%%% output/1137--6-6-3-theorem.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:1137}

\begin{thm}
If $L = N(M)$ for some PDA $M$, then there is a PDA $M'$ such that $L = L(M')$.
\end{thm}

\end{page}

%%%%%%%%%% output/1138--6-6-3-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{4}
\setcounter{dfn}{9}
\label{portion:1138}

\begin{proof}[Idea of the proof]
Introduce two new states: a final state $q_f$ and a new initial state $q'_0$, and also a new start symbol $X_0$.
As the first step, the automaton $M'$ puts the old start symbol $Z_0$ on the top of the new start symbol and goes to the old initial state:
\[
\delta(q'_0, \epsilon, X_0) = (q_0, Z_0X_0).
\]
Then one lets the old PDA $M$ do its job.
If one sees $X_0$ on the top of the stack, then it means that $M$ has emptied its stack.
One then goes to the final state:
\[
\delta(q, \epsilon, X_0) = (q_f, \epsilon) \text{ for all }q \ne q'_0.
\]
\end{proof}



\end{page}

%%%%%%%%%% output/1139--6-6-4-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{4}
\setcounter{dfn}{9}
\label{portion:1139}

\subsection{Equivalence of PDA's and CFL's}
In this section we will show that the languages accepted by PDAs are exactly those generated by context-free grammars.
This can be compared to the fact that the languages accepted by DFAs are those described by regular expressions.


\end{page}

%%%%%%%%%% output/1141--6-6-4-theorem.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:1141}

\begin{thm}
\label{thm:CFLtoPDA}
For every context-free language $L$ there is a PDA $M$ such that $N(M) = L$.
\end{thm}

\end{page}

%%%%%%%%%% output/1142--6-6-4-other.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{4}
\setcounter{dfn}{10}
\label{portion:1142}


The proof uses the notion of a \emph{leftmost derivation} of a word in a context-free grammar.
A leftmost derivation is characterized by the property that at each step a production rule is applied to the leftmost variable in the current word.
In other words, the derivation
\[
\label{eqn:LMDerivation}
S = \alpha_0 \Rightarrow \alpha_1 \Rightarrow \cdots \Rightarrow \alpha_m \Rightarrow w
\]
is leftmost if, when we represent $\alpha_i$ as
\[
\alpha_i = u_i A_i \gamma_i, \quad u_i \in T^*,\, A_i \in V, \gamma_i \in (T \cup V)^*,
\]
then $\alpha_{i+1}$ comes from a production $A_i \to \beta_i$:
\begin{equation}
\label{eqn:Leftmost}
\alpha_i = u_i A_i \gamma_i \Rightarrow u_i \beta_i \gamma_i = u_{i+1} \gamma_{i+1} = \alpha_{i+1}.
\end{equation}
(This means in particular that $u_i$ is a prefix of $u_{i+1}$.)

A leftmost derivation always exists: it can be obtained from a derivation tree by the depth-first traversal.

\begin{proof}[Proof of Theorem \ref{thm:CFLtoPDA}]
Let $G = (V, T, P, S)$ be a context-free grammar such that $L = L(G)$.
We construct a PDA that simulates leftmost derivations of words in $G$ and accepts them by the empty stack.

The PDA will have the following structure:
\[
M = (\{q\}, T, T \cup V, \delta, q, S, \emptyset).
\]
(
It has only one state, thus everything is about changing the stack content.
The transition rules are as follows:
\[
\delta(q, \epsilon, A) = \{(q, \beta) \mid (A \to \beta) \in P\}, \quad \delta(q, a, a) = (q, \epsilon).
\]
In other words,
\begin{align}
(q, w, A\gamma) &\specialvdash{M}{} (q, w, \beta\gamma) \text{ whenever } (A \to \beta) \in P \label{eqn:FirstRule}\\
(q, aw, a\beta) &\specialvdash{M}{} (q, w, \beta) \label{eqn:SecondRule}
\end{align}
Let us show that $M$ accepts all words generated by $G$.
From the definition of a leftmost derivation of $w$ it is clear that each $u_i$ is a prefix of $w$: $w = u_i v_i$.
The transition rules allow to transform the ID $(q, v_i, A_i\gamma_i)$ to $(q, v_{i+1}, A_{i+1}\gamma_{i+1})$:
\[
(q, v_i, A_i\gamma_i) \specialvdash{M}{} (q, v_i, \beta_i\gamma_i) \specialvdash{M}{*} (q, v_{i+1}, A_{i+1}\gamma_{i+1}).
\]
First, the rule \eqref{eqn:FirstRule} is applied, and then a sequence (maybe empty) of rules \eqref{eqn:SecondRule}.
Going over $i$ from $0$ to $m$ one transforms $(q, w, S)$ to $(q, \epsilon, \epsilon)$.

Conversely, emptying the stack with the help of the rules \eqref{eqn:FirstRule} and \eqref{eqn:SecondRule}
can be interpreted as a derivation of a word in the grammar $G$.
\end{proof}


\end{page}

%%%%%%%%%% output/1144--6-6-4-theorem.tex
\begin{page}
\setcounter{section}{6}
\setcounter{subsection}{4}
\setcounter{dfn}{11}
\label{portion:1144}

\begin{thm}
For every PDA $M$ the language $N(M)$ is context-free.
\end{thm}

\end{page}

%%%%%%%%%% output/1145--6-6-4-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{0}
\setcounter{dfn}{11}
\label{portion:1145}

We do not give a proof, but it is a sort of reversal of the above construction.






\end{page}

%%%%%%%%%% output/1146--6-7-0-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1146}

\section{Properties of context-free languages}

\end{page}

%%%%%%%%%% output/1147--6-7-1-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1147}

\subsection{Closure properties of CFLs}

\end{page}

%%%%%%%%%% output/1149--6-7-1-theorem.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1149}

\begin{thm}
\label{thm:CFLClosed}
Context-free languages are closed under union, concatenation and Kleene closure.
\end{thm}

\end{page}

%%%%%%%%%% output/1150--6-7-1-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1150}


\begin{proof}
Let $L$ be a context-free language, and let $G = (V, T, P, S)$ be a grammar that generates $L$.
We construct a grammar $G'$ by adding to $V$ a new symbol $S'$ (which will be the new start symbol) and a new production rule:
\[
V' = V \cup \{S'\}, \quad P' = P \cup \{S' \to SS' \mid \epsilon\}.
\]
In $G'$ one can derive all words of the Kleene closure $L^*$:
\[
S' \xRightarrow[G']{} SS' \xRightarrow[G']{} SSS' \xRightarrow[G']{} \cdots \xRightarrow[G']{} SS \ldots S \xRightarrow[G']{} w_1 w_2 \ldots w_n.
\]
Vice versa, every word derived from $S'$ belongs to $L^*$.

Let now $L_1$ and $L_2$ be languages generated by context-free grammars
\[
G_1 = (V_1, T_1, P_1, S_1), \quad G_2 = (V_2, T_2, P_2, S_2).
\]
Put $T = T_1 \cup T_2$.
We want to show that the languages $L_1 \cup L_2 \subset T^*$ and $L_1L_2 \subset T^*$ are context-free.
In both cases rename the variables so that $V_1 \cap V_2 = \emptyset$
and construct a new grammar $G' = (V', T, P', S')$ with $V' = V_1 \cup V_2 \cup \{S'\}$.
It is easy to see that the production rules
\[
P' = P_1 \cup P_2 \cup \{S' \to S_1 \mid S_2\}
\]
generate the language $L_1 \cup L_2$,
and the production rules
\[
P' = P_1 \cup P_2 \cup \{S'' \to S_1S_2\}.
\]
generate $L_1L_2$.
\end{proof}


\end{page}

%%%%%%%%%% output/1152--6-7-1-corollary.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:1152}

\begin{cor}
Every regular language is context-free.
\end{cor}

\end{page}

%%%%%%%%%% output/1153--6-7-1-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:1153}

\begin{proof}[First proof]
A regular language can be constructed from the basic languages $\emptyset$, $\{\epsilon\}$, $\{a\}$
by operations of union, concatenation and Kleene closure.
Therefore it suffices to show that the basic languages are context-free.
Each of them is generated by the grammars with a single variable $S$ and the following production sets:
\[
P = \emptyset \text{ for }L = \emptyset, \quad P = \{S \to \epsilon\} \text{ for }L = \{\epsilon\}, \quad P = \{S \to a\} \text{ for }L = \{a\}.
\]
\end{proof}

\begin{proof}[Second proof]
A regular language is accepted by some DFA. 
Every DFA can be viewed as a PDA with stack playing no role and word acceptance by final states.
\end{proof}



\end{page}

%%%%%%%%%% output/1154--6-7-2-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{2}
\label{portion:1154}

\subsection{The pumping lemma for CFLs}
For any finite alphabet $\Sigma$ the set of all words $\Sigma^*$ is countably infinite.
The set of all languages over $\Sigma$ is the set of all subsets of $\Sigma^*$ and therefore uncountably infinite.
On the other hand, the set of all pushdown automata with the input language $\Sigma$ is countably infinite
(as is the set of all context-free grammars).
It follows that there are languages which are not context-free.

The following theorem provides a tool to prove non-context-freeness of some languages.


\end{page}

%%%%%%%%%% output/1156--6-7-2-theorem.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:1156}

\begin{thm}
\label{thm:PumpCFL}
Let $L$ be a context-free language.
Then there is a positive integer $n$ such that every $z \in L$ of length $|z| \ge n$ can be split into subwords $z = uvwxy$ in such a way that
\begin{enumerate}
\item
$|vx| \ge 1$, that is the words $v$ and $x$ are not both empty;
\item
$|vwx| \le n$;
\item
for all $i \ge 0$ we have $uv^iwx^iy \in L$.
\end{enumerate}
\end{thm}

\end{page}

%%%%%%%%%% output/1157--6-7-2-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{3}
\label{portion:1157}

That is to say, every sufficiently long word contains two subwords within a bounded region
(the bound $n$ depends on the language, but not on the choice of a word from the language)
that can be removed or repeated several times.


\end{page}

%%%%%%%%%% output/1159--6-7-2-example.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:1159}

\begin{exl}
Let us prove that the language $L = \{a^ib^ic^i \mid i \ge 1\}$ is not context-free.
Assume it is, and let $n$ be a constant from the pumping lemma corresponding to this language.
Take $z = a^nb^nc^n$ and write it as $z = uvwxy$ so that the properties of the pumping lemma are satisfied.
We claim that $z' = uwy = uv^0wx^0y \notin L$.
Indeed, since $|vwx| \le n$, the subword $vwx$ is eiter contained in the prefix $a^nb^n$ or in the suffix $b^nc^n$ of $z$.
In the first case the word $z'$ has $n$ letters $c$ at the end, but less than $2n$ $a$'s and $b$'s in total, so that $z' \notin L$.
Similarly, in the second case $z'$ has $n$ $a$'s, but not enough $b$'s and $c$'s.
\end{exl}

\end{page}

%%%%%%%%%% output/1160--6-7-2-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:1160}


Recall that the language $\{a^ib^i \mid i \ge 1\}$ is context-free (Example \ref{exl:0n1n}, slightly modified to exclude the empty word).
The language from the previous example does not look more complicated, however it is no more context-free.

Similarly, we know from Example \ref{exl:Palindromes} that the set of even-length palindromes $\{w \bar{w} \mid w \in \{0,1\}^*\}$
is a context-free language.
The language of the next example does not look much different, but it is not context-free.
(Althoug it could be accepted by an automaton with a queue memory.)


\end{page}

%%%%%%%%%% output/1162--6-7-2-example.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:1162}

\begin{exl}
\label{exl:WW}
The language $L = \{ww \mid w \in \{0,1\}^*\}$ is not context-free.
Assume it is, and let $n$ be a pumping bound for this language.
Take the word $z = 0^n1^n0^n1^n \in L$ and write it as $z = uvwxy$ according to the pumping lemma.
By the lemma, the word $z' = uwy$ is in $L$, we will however show that this is impossible.

The subword $vwx$ is contained within some two consecutive blocks (inside $0^n1^n$ or $1^n0^n$).
% If it is contained in the first half, then look at the word $uv^2wx^2y$.
% It is longer than $z$ by not more than $n$ symbols,
% therefore its middle ``moved to the left'' by not more than $n/2$ symbols with respect to the middle of $z$.
% Then the right half of $z'$ begins with $1$ (the pumping changes the structure of the word but does not decrease the length of the block $1^n$).
% But the left half of $z'$ begins as before with $0$, which is a contradiction.
% A similar argument works if $vwx$ is contained in the second half of $z$.
If $vwx$ is contained in the first half, then while transforming $z$ to $z'$ we removed some symbols from the first half.
It follows that $uwy = 0^k1^l0^n1^n$, where $k, l \le n$ and at least one of them is $<n$.
This word does not belong to $L$.
Similarly, for the other situations of $vwx$ the depumped word has the form $0^n1^k0^l1^n$ or $0^n1^n0^k1^l$ and does not belong to $L$ either.
\end{exl}

\end{page}

%%%%%%%%%% output/1163--6-7-2-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{5}
\label{portion:1163}




Recall that a derivation of a word in a context-free grammar can be represented in the form of a derivation tree,
see Figure \ref{fig:Chomsky}.
(Sometimes different trees can derive the same word, but this is not a problem.)
If the grammar has Chomsky form, then its derivation trees are binary trees.
More exactly, a derivation tree of a Chomsky form grammar is a full binary tree (variable nodes)
together with an additional edge hanging down from every leaf (terminal nodes).


\end{page}

%%%%%%%%%% output/1165--6-7-2-lemma.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{2}
\setcounter{dfn}{6}
\label{portion:1165}

\begin{lem}
\label{lem:ChomskyTree}
Let $G$ be a grammar in Chomsky form, and $T$ be a parse tree for a word $z \in G$.
If $T$ contains no path of length greater than $i$, then $|z| \le 2^{i-1}$.
\end{lem}

\end{page}

%%%%%%%%%% output/1166--6-7-2-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:1166}

\begin{proof}
The length of $z$ is the number of leaves of the derivation tree.
Deleting the terminal nodes does not change the number of leaves but decreases the depth of the tree by one.
A full binary tree of depth $i-1$ has at most $2^{i-1}$ leaves.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:PumpCFL}]
Let $G$ be a grammar in Chomsky form generating the language $L \setminus \{\epsilon\}$.
Put $n = 2^{k-1}+1$, where $k$ is the number of variables in $G$.
We will show that this $n$ satisfies the conditions of the pumping lemma.

Let $z \in L(G)$ be such that $|z| \ge n$, and let $T$ be any derivation tree for $z$.
By Lemma \ref{lem:ChomskyTree}, $T$ contains a path of length at least $k+1$.
This path contains at least $k+2$ vertices.
The last vertex of the path is labeled by a terminal, all of the other vertices are labeled by variables.
Hence there is a variable that appears on this path at least twice.

Take any of the longest paths in $T$ and, ascending it from the leaf, find the first repetition of variables.
This gives us two vertices $v_1$ and $v_2$ such that
\begin{enumerate}
\item
$v_1$ and $v_2$ have the same label, say $A$;
\item
$v_1$ is closer to the root than $v_2$;
\item
the portion of the path from $v_1$ to the leaf has length at most $k+1$.
\end{enumerate}

The subtree $T_1$ with the root $v_1$ represents a derivation from $A$ of a word $z'$ which is a subword of $z$ and has length at most $2^k$.
Indeed, the portion of our path from $v_1$ down to the leaf has length at most $k+1$, and there is no longer path starting from $v_1$
because we have chosen a longest path.


\begin{figure}[ht]
\begin{center}
\input{Fig/PumpCFL1.pdf_t}
\end{center}
\caption{Proof of the pumping lemma for CFLs: structuring a word.}
\label{fig:PumpCFL1}
\end{figure}

The tree $T_1$ contains a subtree $T_2$ growing from $v_2$.
This subtree derives from $A$ a word $w$ which is a subword of $z'$:
\[
z' = vwx,
\]
see Figure \ref{fig:PumpCFL1}.
The words $v$ and $x$ cannot both be $\epsilon$, because $v_1$ has two children, only one of which is an ancestor of $v_2$,
so that the other child generates a subword of $z'$ disjoint from $w$.

To summarize, we have
\[
S \xRightarrow[]{*} uAy, \quad \xRightarrow[]{*} vAx, \quad A \xRightarrow[]{*} w.
\]
It follows that $A \xRightarrow[]{*} v^iwx^i$ and $S \xRightarrow[]{*} uv^iwx^iy$ for all $i \ge 0$.
A derivation tree for $uv^iwx^iy$ can be obtained from the tree $T$ by cut, copy, and paste, see Figure \ref{fig:PumpCFL2}.
\end{proof}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=\textwidth]{PumpCFL2.pdf}
\end{center}
\caption{Proof of the pumping lemma for CFLs: pumping a word. These trees represent the words $uv^3wx^3y$ and $uwy$ respectively.}
\label{fig:PumpCFL2}
\end{figure}



\end{page}

%%%%%%%%%% output/1167--6-7-3-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{3}
\setcounter{dfn}{6}
\label{portion:1167}

\subsection{Non-closure properties of CFLs}
We have already proved that context-free languages are closed under union, concatenation and Kleene closure.


\end{page}

%%%%%%%%%% output/1169--6-7-3-theorem.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:1169}

\begin{thm}
\label{thm:CFLIntClosed}
The set of context-free languages is not closed under intersection.
\end{thm}

\end{page}

%%%%%%%%%% output/1170--6-7-3-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{3}
\setcounter{dfn}{7}
\label{portion:1170}

\begin{proof}
Consider the languages
\begin{gather*}
L = \{a^ib^ic^i \mid i \ge 1\},\\
L_1 = \{a^ib^ic^j \mid i, j \ge 1\},\\
L_2 = \{a^ib^jc^j \mid i, j \ge 1\}.
\end{gather*}
We have $L = L_1 \cap L_2$.
As we know, $L$ is not context-free.
On the other hand $L_1$ is generated by the grammar
\[
S \to AB, \quad A \to aAb \mid ab, \quad B \to cB \mid c.
\]
The language $L_2$ is generated by a similar grammar.
Thus $L_1$ and $L_2$ are context-free languages whose intersection is not context-free.
\end{proof}


\end{page}

%%%%%%%%%% output/1172--6-7-3-theorem.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:1172}

\begin{thm}
The set of context-free languages is not closed under complementation.
\end{thm}

\end{page}

%%%%%%%%%% output/1173--6-7-3-other.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{3}
\setcounter{dfn}{8}
\label{portion:1173}

\begin{proof}
We know that CFL's are closed under union.
If they would be closed under complementation, then due to
\[
L_1 \cap L_2 = \overline{\overline{L_1} \cup \overline{L_2}}
\]
they would also be closed under intersection, which is not the case.
\end{proof}

The above argument is non-constructive.
Applied to the languages $L_1$, $L_2$, $L$ from the proof of Theorem \ref{thm:CFLIntClosed} it says that at least one of the following is true:
\begin{enumerate}
\item $L_1$ is context-free, but $\overline{L_1}$ is not;
\item $L_2$ is context-free, but $\overline{L_2}$ is not;
\item $\overline{L_1} \cup \overline{L_2}$ is context free, but its complement is not.
\end{enumerate}


\end{page}

%%%%%%%%%% output/1175--6-7-3-remark.tex
\begin{page}
\setcounter{section}{7}
\setcounter{subsection}{3}
\setcounter{dfn}{9}
\label{portion:1175}

\begin{rem}
The situation is similar to the following proof of the existence of two irrational numbers $\alpha$ and $\beta$ such that $\alpha^\beta$ is rational.
If $\sqrt{2}^{\sqrt{2}}$ is rational, then one may take $\alpha = \beta = \sqrt{2}$.
If $\sqrt{2}^{\sqrt{2}}$ is irrational, then one may take $\alpha = \sqrt{2}^{\sqrt{2}}$, $\beta = \sqrt{2}$ because of
\[
\left(\sqrt{2}^{\sqrt{2}}\right)^{\sqrt{2}} = \sqrt{2}^{\sqrt{2} \cdot \sqrt{2}} = \sqrt{2}^2 = 2.
\]
(In fact, $\sqrt{2}^{\sqrt{2}}$ is rational, which follows from a more general (and more complicated) theorem by Gelfond and Schneider from 1934.)
\end{rem}

\end{page}

%%%%%%%%%% output/1176--6-7-3-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{0}
\setcounter{dfn}{9}
\label{portion:1176}


It is possible to give a concrete example of a context-free language with a non-context-free complement.
Namely, according to Example \ref{exl:WW} the language $\{x \in \{0,1\}^* \mid x = ww \text{ for some } w \in \{0,1\}^*\}$
is not context-free. Its complement is context-free (exercise).












\end{page}

%%%%%%%%%% output/1177--6-8-0-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1177}

\section{Turing machines}

\end{page}

%%%%%%%%%% output/1178--6-8-1-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{0}
\label{portion:1178}

\subsection{Definition}
Similarly to all automata we considered before,
a Turing machine has a finite number of states and changes its states according to the input.
The input is a finite word on an infinite tape, and there are two new aspects in how the machine interacts with the input:
\begin{itemize}
\item
the machine can move along the tape;
\item
the machine can write on the tape.
\end{itemize}

Any sort of input: a number, a list, a table, a combinatorial structure such as a graph,
can be encoded as a sequence of symbols on a tape (one just needs to choose an encoding convention).
For any algorithm: multiplication of integers, search for a path between two vertices in a graph,
there is a Turing machine which applies this algorithm to any given input.
The last sentence is, in fact, a definition of the algorithm and a form of the Church thesis:
everything which ``can be computed'' can be done with some Turing machine.


\end{page}

%%%%%%%%%% output/1180--6-8-1-definition.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1180}

\begin{dfn}
A \emph{Turing machine} is
\[
M = (Q, \Sigma, \Gamma, \delta, q_0, B, F),
\]
where
\begin{itemize}
\item
$Q$ is the set of states, $q_0 \in Q$ is the initial state, and $F \subset Q$ is the set of final states;
\item
$\Gamma$ is the set of \emph{tape symbols}, $\Sigma \subset \Gamma$ is the set of \emph{input symbols},
and $B \in \Gamma \setminus \Sigma$ is the \emph{blank} symbol;
\item
$\delta \colon Q \times \Gamma \to Q \times \Gamma \times \{L, R\}$
is a partial map (that is, it can be undefined for some arguments), called the \emph{move function}.
\end{itemize}
\end{dfn}

\end{page}

%%%%%%%%%% output/1181--6-8-1-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{1}
\label{portion:1181}

At each moment of time, the machine is situated opposite to some cell of the input tape.
The input symbol $X$ and the current state $p$ of the machine determine its move $\delta(p, X) = (q, Y, D)$
which consists in:
\begin{itemize}
\item
changing the state to $q$,
\item
replacing the symbol $X$ in the current cell by $Y$,
\item
and moving in the direction $D$ (one cell to the left if $D = L$ or one cell to the right if $D = R$).
\end{itemize}
See Figure \ref{fig:TuringMachine}.

\begin{figure}[ht]
\begin{center}
\input{Fig/TuringMachine.pdf_t}
\end{center}
\caption{A Turing machine.}
\label{fig:TuringMachine}
\end{figure}


At the beginning, the machine is placed at the leftmost cell of the tape and is in the state $q_0$.
The \emph{language accepted by} $M$ is the set of all input words for which $M$ enters a final state at some moment of time.
After entering a final state, the machine \emph{halts}, that is $q(f,X)$ is undefined for all $f \in F$.
If the input word is not accepted, then the machine either halts in a non-final state or runs forever
(in an infinite loop or by increasing the data volume on the tape to infinity).


\end{page}

%%%%%%%%%% output/1183--6-8-1-example.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:1183}

\begin{exl}
Let us design a machine accepting the language $\{0^n1^n \mid n \ge 1\}$.
The algorithm is as follows:
check the first zero, move right until meet the first one and check it, move left to the first unchecked one and check it, move right... etc.
The machine will have several states according to the tasks: for example, a state for moving right until find the first (unchecked) one.
Checking the symbols will be done by replacing them: $0$ with $X$, and $1$ with $Y$.
By carefully working out the details one may arrive at a machine with
\[
Q = \{q_0, q_1, q_2, q_3, q_4\}, \quad \Gamma = \{0, 1, X, Y, B\}, \quad F = \{q_4\}
\]
and the move function described by the following table.
\begin{center}
\begin{tabular}{c|ccccc}
& 0 & 1 & $X$ & $Y$ & $B$\\\hline
$q_0$ & $(q_1, X, R)$ & $-$ & $-$ & $(q_3, Y, R)$ & $-$\\
$q_1$ & $(q_1, 0, R)$ & $(q_2, Y, L)$ & $-$ & $(q_1, Y, R)$ & $-$\\
$q_2$ & $(q_2, 0, L)$ & $-$ & $(q_0, X, R)$ & $(q_2, Y, L)$ & $-$\\
$q_3$ & $-$ & $-$ & $-$ & $(q_3, Y, R)$ & $(q_4, B, R)$\\
$q_4$ & $-$ & $-$ & $-$ & $-$ & $-$
\end{tabular}
\end{center}
\end{exl}

\end{page}

%%%%%%%%%% output/1184--6-8-1-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{2}
\label{portion:1184}


In a similar way one can construct a machine accepting the language $\{0^n 1^n 2^n \mid n \ge 1\}$
(recall that this language is not context-free).



\end{page}

%%%%%%%%%% output/1186--6-8-1-definition.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:1186}

\begin{dfn}
A language is called \emph{recursively enumerable} if it is accepted by some Turing machine.
A language is called \emph{recursive} if it is accepted by a Turing machine which halts on every input.
\end{dfn}

\end{page}

%%%%%%%%%% output/1187--6-8-1-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{3}
\label{portion:1187}

All context-free languages are recursively enumerable because every pushdown automaton can be simulated by a Turing machine.
One can show that context-free languages are also recursive.

If a language is recursively enumerable but not recursive,
then every Turing machine accepting it will run forever on some input word not belonging to the language.
Later we will give an example of such a language.


\end{page}

%%%%%%%%%% output/1189--6-8-1-lemma.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{dfn}{4}
\label{portion:1189}

\begin{lem}
A language $L$ is recursive if and only if both $L$ and its complement are recursively enumerable.
\end{lem}

\end{page}

%%%%%%%%%% output/1190--6-8-1-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{2}
\setcounter{dfn}{4}
\label{portion:1190}

\begin{proof}
A recursive language is recursively enumerable by definition.
Its complement is also recursively enumerable: take the same machine and replace $F$ by $Q \setminus F$.

If $L$ and $\Sigma^* \setminus L$ are recursively enumerable,
then one lets the corresponding machine run on the same input word $w$.
At least one of the machines will halt and tell us whether $w \in L$ or not.
\end{proof}

We do not go into details how one can combine two Turing machines into one,
but this can be done with the techniques similar to those we used to design new finite automata.



\end{page}

%%%%%%%%%% output/1191--6-8-2-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:1191}

\subsection{Modifications of Turing machines}
There are several variations of Turing machines, e.~g.:
\begin{itemize}
\item
multi-tape: several tapes, each with its own head for reading and writing;
\item
multidimensional: a square grid instead of a tape, the head can move not only left and right, but also up and down;
\item
non-deterministic.
\end{itemize}
Although they seem more powerful, each of them can be simulated by an ordinary Turing machine.




\end{page}

%%%%%%%%%% output/1192--6-8-3-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{3}
\setcounter{dfn}{4}
\label{portion:1192}

\subsection{Problems and languages}
Consider a problem like ``Is a given finite graph connected?''
An \emph{instance} of a problem is any finite graph.
Since graphs can be encoded as words in an alphabet, this problem determines a language:
the set of all words encoding connected graphs.
This holds for any question which depends on some countable parameter and has a yes/no answer:
encode the parameter values by words, and consider the set of all words for which the answer to the question is ``yes''.


\end{page}

%%%%%%%%%% output/1194--6-8-3-definition.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{3}
\setcounter{dfn}{5}
\label{portion:1194}

\begin{dfn}
A problem is called \emph{decidable} if the corresponding language is recursive,
that is if there is a Turing machine which halts on every input
and accepts exactly those words which correspond to problem instances with the positive answer.

A problem is called \emph{semi-decidable} if the corresponding language is recursively enumerable.
\end{dfn}

\end{page}

%%%%%%%%%% output/1196--6-8-4-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{4}
\setcounter{dfn}{5}
\label{portion:1196}

\subsection{The universal language}

\end{page}

%%%%%%%%%% output/1198--6-8-4-definition.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:1198}

\begin{dfn}
The \emph{universal language} is the set
\[
L_u = \{(M,w) \mid \text{Turing machine }M \text{ accepts word }w\}.
\]
\end{dfn}

\end{page}

%%%%%%%%%% output/1199--6-8-4-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{4}
\setcounter{dfn}{6}
\label{portion:1199}

A Turing machine can be encoded with a binary word (once an encoding convention is chosen).
Similarly, words in all finite alphabets can be encoded by binary words (once a binary encoding of the alphabet symbols is chosen).
Therefore the universal language can be viewed as a language over a binary alphabet.


\end{page}

%%%%%%%%%% output/1201--6-8-4-lemma.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{4}
\setcounter{dfn}{7}
\label{portion:1201}

\begin{lem}
\label{lem:UniLangRE}
The universal language is recursively enumerable.
\end{lem}

\end{page}

%%%%%%%%%% output/1202--6-8-4-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{5}
\setcounter{dfn}{7}
\label{portion:1202}

\begin{proof}
Receiving $(M, w)$ as input, let $M$ run on $w$.
As soon as $M$ halts accepting $w$, accept the pair $(M,w)$.

A Turing machine which implements the above algorithm can be described in a multi-track architecture:
the first tape holds the code of $M$, the second tape holds the word $w$, the third tape is used to store the state of $M$,
the fourth tape stores the position in the tape of $M$.
\end{proof}





\end{page}

%%%%%%%%%% output/1203--6-8-5-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{5}
\setcounter{dfn}{7}
\label{portion:1203}

\subsection{Undecidability of the halting problem}
The \emph{halting problem} is ``Does Turing machine $M$ accept input $w$?''
The corresponding language is the universal language $L_u$.


\end{page}

%%%%%%%%%% output/1205--6-8-5-theorem.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{5}
\setcounter{dfn}{8}
\label{portion:1205}

\begin{thm}
\label{thm:HaltingProblem}
The halting problem is undecidable, that is, the universal language is not recursive.
\end{thm}

\end{page}

%%%%%%%%%% output/1206--6-8-5-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{5}
\setcounter{dfn}{8}
\label{portion:1206}


Construct a language $L_d$:
\[
L_d = \{w_i \mid w_i \text{ is not accepted by }M_i\}.
\]


\end{page}

%%%%%%%%%% output/1208--6-8-5-lemma.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{5}
\setcounter{dfn}{9}
\label{portion:1208}

\begin{lem}
\label{lem:LdNotRE}
The language $L_d$ is not recursively enumerable.
\end{lem}

\end{page}

%%%%%%%%%% output/1209--6-8-5-other.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{5}
\setcounter{dfn}{9}
\label{portion:1209}

\begin{proof}
Assume the contrary.
Then there is a Turing machine $M_j$ which accepts the language $L_d$.
By inspection of the word $w_j$ we arrive to a contradiction:
\begin{itemize}
\item
if $w_j \in L_d$, then by definition of $L_d$ the word $w_j$ is not accepted by $M_j$, which by the choice of $M_j$ means that $w_j \notin L_d$;
\item
if $w_i \notin L_d$, then by definition of $L_d$ the word $w_j$ is accepted by $M_j$, which by the choice of $M_j$ means that $w_j \in L_d$.
\end{itemize}
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:HaltingProblem}]
If $L_u$ is recursive, then there is a Turing machine $A$ which always halts and accepts only pairs $(M, w)$ from $L_u$.
Let us show that then $L_d$ is recursive, which contradicts Lemma \ref{lem:LdNotRE}.
Given a word $w$ determine the integer $i$ such that $w_i = w$.
Then determine the machine $M_i$.
Feed $(M_i, w_i)$ into $A$ and accept $w$ if and only if $A$ \emph{does not} accept $(M_i,w_i)$.
This gives an algorithm which always stops and recognizes the language $L_d$.
\end{proof}


\end{page}

%%%%%%%%%% output/1211--6-8-5-remark.tex
\begin{page}
\setcounter{section}{8}
\setcounter{subsection}{5}
\setcounter{dfn}{10}
\label{portion:1211}

\begin{rem}
The above argument together with Lemma \ref{lem:UniLangRE} shows that
the complement of $L_d$ is recursively enumerable (but not recursive).
\end{rem}

\end{page}

\end{document}